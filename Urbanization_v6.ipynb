{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2016b3",
   "metadata": {},
   "source": [
    "# Spatiotemporal Trends in Urbanization: Cameroon\n",
    "*Using yearly estimates (2000-2015) of population, built-area, and economic indicators to track city-by-city growth and change over time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd14a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903293ba",
   "metadata": {},
   "source": [
    "### Research questions \n",
    "\n",
    "#### 1. How has the size of Settlement X changed over time? \n",
    "\n",
    "- Population size \n",
    "\n",
    "- Geographical extents \n",
    "\n",
    "- Population density \n",
    "\n",
    "#### 2. In what year did Settlement X become a new urban class?  \n",
    "\n",
    "- From semi-dense to high-density city \n",
    "\n",
    "- Small settlement area to built-up area \n",
    "\n",
    "- When a hamlet area or small settlement area first appeared\n",
    "\n",
    "#### 3. Is there a discernable pattern between the spatio-temporal distribution of economic density and population density? \n",
    "\n",
    "#### 4. How much of urban space attributable to City X is outside of the administrative limits of the city? \n",
    "\n",
    "- When did this fragment(s) appear? \n",
    "\n",
    "- Which district/municipality/authority has purview over the fragment(s)? \n",
    "\n",
    "#### 5. For the questions above, how does the answer change based on different understandings of urban limits? \n",
    "\n",
    "- Scenario A: where \"city\" is delimited by an official administrative boundary \n",
    "\n",
    "- Scenario B: where \"city\" includes all contiguous (and near-contiguous) built up area \n",
    "\n",
    "#### 6. Subnational and inter-national comparisons. Examples: \n",
    "\n",
    "- Compare the rates (pop, build-up, economicâ€¦) of the fastest growing settlement of each ADM1 region. \n",
    "\n",
    "- Which African metropoles experience the most vs. the least fragmentation? Is there a confluence between amount of urban fragmentation and rate of densification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55d1a9",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. Most up-to-date administrative boundaries: **ADM3.**\n",
    "2. Built-up area, yearly: **World Settlement Footprint Evolution.** Resolution: 30m.\n",
    "3. Settlement types: **GRID3 settlement extents.** Captured between 2009-2019.\n",
    "4. Population, yearly: **WorldPop.** UN-adjusted, unconstrained. Resolution: 100m.\n",
    "5. Nighttime lights, yearly: **Harmonization of DMSP and VIIRS.** Resolution: 1km.\n",
    "6. City names: **UCDB, Africapolis, and GeoNames.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5dafb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a61167",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276b447",
   "metadata": {},
   "source": [
    "## 1. PREPARE WORKSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0190f8",
   "metadata": {},
   "source": [
    "### 1.1 Off-script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9093fd6",
   "metadata": {},
   "source": [
    "##### Off-script: Create folders in working directory.\n",
    "> *ADM\n",
    "<br>Buildup\n",
    "<br>PlaceName\n",
    "<br>Population\n",
    "<br>Settlement\n",
    "<br>NTL*\n",
    "\n",
    "##### Off-script: Download datasets (as shapefile, GeoJSON, or tif where possible) and place or extract into corresponding folder:\n",
    "- ADM: *Sourced internally.*\n",
    "- Buildup: https://download.geoservice.dlr.de/WSF_EVO/files/\n",
    "- PlaceName: \n",
    "    - GeoNames: (file: cities500.zip) https://download.geonames.org/export/dump/\n",
    "    - Africapolis: https://africapolis.org/en/data\n",
    "    - Urban Centres Database: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php\n",
    "- Population: https://hub.worldpop.org/geodata/listing?id=69\n",
    "- Settlement: https://data.grid3.org/datasets/GRID3::grid3-cameroon-settlement-extents-version-01-01-/explore\n",
    "- NTL: https://figshare.com/articles/dataset/Harmonization_of_DMSP_and_VIIRS_nighttime_light_data_from_1992-2018_at_the_global_scale/9828827/2\n",
    "\n",
    "##### Other off-script:\n",
    "- Convert GeoNames from .txt file to shape (delimiter = tab, header rows = 0) and rename fields.\n",
    "- If necessary, mosaic WSFE rasters that cover the area of interest to create a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2548",
   "metadata": {},
   "source": [
    "### 1.2 Load all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c42795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Most but not all of these packages were used in final form. \n",
    "\n",
    "import os, sys, glob, re, time\n",
    "from os.path import exists\n",
    "from functools import reduce\n",
    "\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, Polygon, shape, MultiPoint\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.validation import make_valid, explain_validity\n",
    "import shapely.wkt\n",
    "import scipy\n",
    "\n",
    "#from xrspatial import zonal_stats \n",
    "#import xarray as xr \n",
    "import numpy as np \n",
    "import fiona, rioxarray\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio import features\n",
    "from rasterio.features import shapes\n",
    "from rasterio import mask\n",
    "from osgeo import gdal, osr, ogr, gdal_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f9d7",
   "metadata": {},
   "source": [
    "### 1.3 Set workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20dfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\n",
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Results\n"
     ]
    }
   ],
   "source": [
    "ProjectFolder = os.getcwd()\n",
    "ResultsFolder = os.path.join(ProjectFolder, 'Results')\n",
    "print(ProjectFolder)\n",
    "print(ResultsFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537c1ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08550",
   "metadata": {},
   "source": [
    "## 2. PREPARE BUILDUP, SETTLEMENT, AND ADMIN DATASETS\n",
    "Projection for all datasets: Africa Albers Equal Area Conic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d194cf",
   "metadata": {},
   "source": [
    "### 2.1 WSFE: Check contents and change NoData value as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa8859",
   "metadata": {},
   "source": [
    "##### Off-script: Run this block in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a191c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "# Change NoData value to zero, as this won't interfere with a possible value of 99999 in GRID3 and ADM.\n",
    "# Then make sure there are no values above 2015 (such as 99999) or below 1985 in the dataset by reclassifying them as NoData.\n",
    "# Was having trouble with rasterio & gdal here, so moved to QGIS.\n",
    "\n",
    "# processing.run(\"native:reclassifybytable\", {'INPUT_RASTER':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_CMN.tif','RASTER_BAND':1,'TABLE':['2016','','0','','1984','0'],'NO_DATA':0,'RANGE_BOUNDARIES':0,'NODATA_FOR_MISSING':False,'DATA_TYPE':5,'OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE.tif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ea5f",
   "metadata": {},
   "source": [
    "### 2.2 Prepare raster locations for GRID3 and Admin areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7946e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_vec = gpd.read_file(glob.glob('ADM/*.shp')[0])[['geometry']].to_crs(\"ESRI:102022\") # This glob() function pulls the first file ([0]) in the ADM folder which ended in '.shp'\n",
    "GRID3_vec = gpd.read_file(glob.glob('Settlement/*.shp')[0])[['type','geometry']].to_crs(\"ESRI:102022\")\n",
    "ADM_vec['ADM_ID'] = range(0,len(ADM_vec))\n",
    "GRID3_vec['G3_ID'] = range(0,len(GRID3_vec))\n",
    "ADM_vec.to_file(driver='GPKG', filename=r'ADM/ADM_warp.gpkg', layer='ADM')\n",
    "GRID3_vec.to_file(driver='GPKG', filename=r'Settlement/Settlement_warp.gpkg', layer='GRID3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44280f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_vec = gpd.read_file(r'ADM/ADM_warp.gpkg', layer='ADM')\n",
    "GRID3_vec = gpd.read_file(r'Settlement/Settlement_warp.gpkg', layer='GRID3')\n",
    "\n",
    "print(ADM_vec.info(), \"\\n\\n\", \n",
    "      ADM_vec.sample(5),\n",
    "      ADM_vec.crs, \"\\n\\n\", \n",
    "      len(str(ADM_vec['ADM_ID'].max()))) # We need to know how many digits need to be allocated to each dataset in the \"join\" serial.\n",
    "print(GRID3_vec.info(), \"\\n\\n\",\n",
    "      GRID3_vec.sample(5),\n",
    "      GRID3_vec.crs, \"\\n\\n\", \n",
    "      len(str(GRID3_vec['G3_ID'].max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44be4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440aa2a2",
   "metadata": {},
   "source": [
    "## 3. WSFE AND ADM; GRID3 AND ADM\n",
    "RASTERIZE: Bring ADM and GRID3 into raster space.\n",
    "\n",
    "RASTER MATH: \"Join\" ADM ID onto GRID3 and onto WSFE by creating unique concatenation string.\n",
    "\n",
    "VECTORIZE: Bring joined data into vector space.\n",
    "\n",
    "VECTOR MATH: Split unique ID from raster math step into separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca195d70",
   "metadata": {},
   "source": [
    "### 3.1 Reproject WSFE to project CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFE_in = glob.glob('Buildup/*.tif')[0]\n",
    "WSFE_warp = './Buildup/WSFE_warp.tif'\n",
    "ProjCRS = gdal.WarpOptions(dstSRS='ESRI:102022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Warp = gdal.Warp(WSFE_warp, # Where to store the warped raster\n",
    "                 WSFE_in, # Which raster to warp\n",
    "                 format='GTiff', \n",
    "                 options=ProjCRS) # Reproject to Africa Albers Equal Area Conic\n",
    "Warp = None\n",
    "print('Reprojected dataset. %s' % time.ctime())\n",
    "\n",
    "try:  \n",
    "    os.remove(os.path.join(ProjectFolder, WSFE_in))\n",
    "except OSError:\n",
    "    pass\n",
    "print('Removed (or skipped if error) intermediate file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFE = rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "print(WSFE) # WSFE values are all 4 digits long (1985-2015)\n",
    "print(dir(WSFE))\n",
    "print(WSFE.crs)\n",
    "print(WSFE.dtypes)\n",
    "NoDataValue = WSFE.nodatavals\n",
    "print(NoDataValue)\n",
    "print(WSFE.read(1).min(), WSFE.read(1).mean(), np.median(WSFE.read(1)), WSFE.read(1).max())\n",
    "\n",
    "# If NoDataValue != 0, change to 0. (See step 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9663bd1",
   "metadata": {},
   "source": [
    "### 3.2 Rasterize admin areas and GRID3 using WSFE specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af504c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and update the metadata from WSFE for the output\n",
    "meta = WSFE.meta.copy()\n",
    "meta.update(compress='lzw')\n",
    "WSFE.meta\n",
    "\n",
    "ADM_out = './ADM/ADM_rasterized.tif'\n",
    "GRID3_out = './Settlement/GRID3_rasterized.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f778fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rasterizing dataset. %s\" % time.ctime())\n",
    "with rasterio.open(ADM_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(ADM_vec.geometry, ADM_vec.ADM_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "    out.write_band(1, burned)\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rasterizing dataset. %s\" % time.ctime())\n",
    "with rasterio.open(GRID3_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(GRID3_vec.geometry, GRID3_vec.G3_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "    out.write_band(1, burned)\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbc740",
   "metadata": {},
   "source": [
    "*Validation: Check the dimensions, type, and basic stats of the three datasets. All should be the same dimension and NoData value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "RastersList = [gdal.Open(r\"ADM/ADM_rasterized.tif\"), \n",
    "               gdal.Open(r\"Settlement/GRID3_rasterized.tif\"),\n",
    "               gdal.Open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))]\n",
    "\n",
    "for item in RastersList:\n",
    "    print(gdal.GetDataTypeName(item.GetRasterBand(1).DataType), \n",
    "          item.GetRasterBand(1).GetNoDataValue(),\n",
    "         \"\\n\\n\")\n",
    "\n",
    "RastersList = None\n",
    "\n",
    "RastersList = [rasterio.open(r\"ADM/ADM_rasterized.tif\"), \n",
    "               rasterio.open(r\"Settlement/GRID3_rasterized.tif\"), \n",
    "               rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))]\n",
    "\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "\n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ceaa3",
   "metadata": {},
   "source": [
    "### 3.2 Raster math to \"join\" admin to GRID3 and to WSFE.\n",
    "Processing is more rapid when \"joining,\" i.e. creating serial codes out of two datasets, in raster rather than vector space.\n",
    "Here, we are concatenating the ID fields of the two datasets to create a serial number that we can then split in vector space later to create two ID fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a2406",
   "metadata": {},
   "source": [
    "*Adding together the values to create join IDs. This is in effect a concatenation of their ID strings, by way of summation. The number of zeros in the calc multiplication corresponds with number of digits of the maximum value in the \"B\" dataset. (e.g. Chad ADM codes go up 4 digits, so it's calc=(A*10000)+B).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e831cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN TERMINAL FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Gdal_calc.py # To see info.\n",
    "\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3_rasterized.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM_rasterized.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_warp.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM_rasterized.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "\n",
    "# # END TERMINAL-ONLY ASPECT. RETURN HERE FOR NEXT STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: check the basic statistics of the resulting datasets.\n",
    "RastersList = [rasterio.open(r\"Buildup/WSFE_ADM.tif\"), \n",
    "               rasterio.open(r\"Settlement/GRID3_ADM.tif\")]\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "    \n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea3639",
   "metadata": {},
   "source": [
    "### 3.3 Vectorize \"joined\" layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5d9d4",
   "metadata": {},
   "source": [
    "##### Off-script: Run this block in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Due to dtype errors with both gdal and rasterio here, I decided to run the raster to polygon function in QGIS instead.\n",
    "# It is possible to run QGIS functions within a Jupyter Notebook, but I ran it within the GUI. Arc or R are other options.\n",
    "# Command line code here.\n",
    "\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.tif','BAND':1,'FIELD':'gridcode','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.shp'})\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.tif','BAND':1,'FIELD':'gridcode','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.shp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03937b47",
   "metadata": {},
   "source": [
    "### 3.4 Vector math to split raster strings into admin area, GRID3, and WSFE year assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb900b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 208406 entries, 0 to 208405\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   gridcode  208406 non-null  int64   \n",
      " 1   geometry  208406 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 3.2 MB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 538320 entries, 0 to 538319\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   gridcode  538320 non-null  int64   \n",
      " 1   geometry  538320 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 8.2 MB\n",
      "None \n",
      "\n",
      "          gridcode                                           geometry\n",
      "26450   174255251  POLYGON ((-1017577.377 1197912.187, -1017485.5...\n",
      "96711   124381201  POLYGON ((-1080561.484 756319.533, -1080408.46...\n",
      "142194   69906138  POLYGON ((-1472819.649 631942.753, -1472727.83...\n",
      "196046    9382023  POLYGON ((-1584954.619 420251.271, -1584832.20...\n",
      "446     194955304  POLYGON ((-1091211.858 1492725.553, -1091120.0...\n",
      "104764   84305154  POLYGON ((-1510218.375 741537.548, -1510187.77...\n",
      "183133   10591024  POLYGON ((-1569193.290 468759.439, -1569132.08...\n",
      "179547   26834086  POLYGON ((-1392972.450 480480.972, -1392911.24...\n",
      "182945   19332276  POLYGON ((-1407264.762 470014.225, -1407234.15...\n",
      "24596   166086235  POLYGON ((-1137455.292 1204767.601, -1137394.0... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]] \n",
      "\n",
      " None \n",
      "\n",
      "         gridcode                                           geometry\n",
      "316916   2011171  POLYGON ((-1583301.975 596441.507, -1583271.37...\n",
      "56047    1997216  POLYGON ((-1179995.578 1067200.272, -1179964.9...\n",
      "511341   2010046  POLYGON ((-976200.063 385209.093, -976169.458 ...\n",
      "269405   1998282  POLYGON ((-1554350.097 638461.517, -1554288.88...\n",
      "411879   2008010  POLYGON ((-1640471.223 472738.027, -1640440.61...\n",
      "112467   2002150  POLYGON ((-1510340.793 703526.731, -1510310.18...\n",
      "292766   2005294  POLYGON ((-1526959.049 623067.442, -1526928.44...\n",
      "536479   2007083  POLYGON ((-1445244.974 257894.280, -1445183.76...\n",
      "496017   2008092  POLYGON ((-1397287.687 440664.488, -1397226.47...\n",
      "259021   2005138  POLYGON ((-1448152.404 642225.873, -1448121.79... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]]\n"
     ]
    }
   ],
   "source": [
    "# Load newly created vectorized datasets.\n",
    "GRID3_ADM = gpd.read_file(r\"Settlement/GRID3_ADM.shp\")\n",
    "WSFE_ADM = gpd.read_file(r\"Buildup/WSFE_ADM.shp\")\n",
    "print(GRID3_ADM.info(), \"\\n\\n\", GRID3_ADM.sample(10), \"\\n\\n\", GRID3_ADM.crs, \"\\n\\n\", \n",
    "      WSFE_ADM.info(), \"\\n\\n\", WSFE_ADM.sample(10), \"\\n\\n\", WSFE_ADM.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3085bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201820297 2015328\n"
     ]
    }
   ],
   "source": [
    "print(GRID3_ADM['gridcode'].max(), WSFE_ADM['gridcode'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f41b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         gridcode                                           geometry  \\\n",
      "196380  198774299  POLYGON ((-1415803.424 418261.977, -1415650.40...   \n",
      "139396   53574282  POLYGON ((-1561144.301 641215.924, -1561052.48...   \n",
      "77413   116042198  POLYGON ((-1286070.853 811713.718, -1285979.03...   \n",
      "163335   63096109  POLYGON ((-1459965.749 541108.531, -1459873.93...   \n",
      "48789   147626215  POLYGON ((-1231717.221 1068546.871, -1231625.4...   \n",
      "75180   118071321  POLYGON ((-1311258.375 823037.392, -1311166.56...   \n",
      "47391   150764002  POLYGON ((-1199276.427 1084614.245, -1199184.6...   \n",
      "185644   35593106  POLYGON ((-1305443.516 461169.518, -1305412.91...   \n",
      "164379   45688168  POLYGON ((-1636247.799 536426.039, -1636155.98...   \n",
      "24107   167436313  POLYGON ((-1061647.889 1206512.058, -1061556.0...   \n",
      "\n",
      "       gridstring  Sett_ID  ADM_ID  \n",
      "196380  198774299   198774     299  \n",
      "139396  053574282    53574     282  \n",
      "77413   116042198   116042     198  \n",
      "163335  063096109    63096     109  \n",
      "48789   147626215   147626     215  \n",
      "75180   118071321   118071     321  \n",
      "47391   150764002   150764       2  \n",
      "185644  035593106    35593     106  \n",
      "164379  045688168    45688     168  \n",
      "24107   167436313   167436     313           gridcode                                           geometry  \\\n",
      "380821   1999319  POLYGON ((-1637074.121 487703.639, -1637043.51...   \n",
      "202194   2014301  POLYGON ((-1561052.487 661261.886, -1560991.27...   \n",
      "437398   1997012  POLYGON ((-1400317.535 463740.298, -1400286.93...   \n",
      "237293   2004139  POLYGON ((-1494640.673 648346.777, -1494610.06...   \n",
      "134092   2007182  POLYGON ((-1600777.157 690703.436, -1600746.55...   \n",
      "477335   2000012  POLYGON ((-1404081.891 447397.483, -1404051.28...   \n",
      "388797   2013009  POLYGON ((-1594533.835 482256.034, -1594503.23...   \n",
      "144772   2002161  POLYGON ((-1571060.166 685194.622, -1571029.56...   \n",
      "471739   2008298  POLYGON ((-1406040.581 448713.477, -1405979.37...   \n",
      "166220   2010140  POLYGON ((-1437502.030 675492.989, -1437440.82...   \n",
      "\n",
      "       gridstring  year  ADM_ID  \n",
      "380821    1999319  1999     319  \n",
      "202194    2014301  2014     301  \n",
      "437398    1997012  1997      12  \n",
      "237293    2004139  2004     139  \n",
      "134092    2007182  2007     182  \n",
      "477335    2000012  2000      12  \n",
      "388797    2013009  2013       9  \n",
      "144772    2002161  2002     161  \n",
      "471739    2008298  2008     298  \n",
      "166220    2010140  2010     140  \n"
     ]
    }
   ],
   "source": [
    "# Split serial back into separate dataset fields.\n",
    "# For Burkina: WSFE and ADM: 4+3=7 digits. GRID3 and ADM: 6+3=9 digits.\n",
    "GRID3_ADM['gridstring'] = GRID3_ADM['gridcode'].astype(str).str.zfill(9)\n",
    "WSFE_ADM['gridstring'] = WSFE_ADM['gridcode'].astype(str).str.zfill(7)\n",
    "\n",
    "GRID3_ADM['Sett_ID'] = GRID3_ADM['gridstring'].str[:-3].astype(int) # Remove the last 4 digits to get the GRID3 portion.\n",
    "GRID3_ADM['ADM_ID'] = GRID3_ADM['gridstring'].str[-3:].astype(int) # Keep only the last 4 digits to get the ADM portion.\n",
    "WSFE_ADM['year'] = WSFE_ADM['gridstring'].str[:-3].astype(int)\n",
    "WSFE_ADM['ADM_ID'] = WSFE_ADM['gridstring'].str[-3:].astype(int)\n",
    "\n",
    "print(GRID3_ADM.sample(10), WSFE_ADM.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3127bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 204257 entries, 0 to 204256\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype   \n",
      "---  ------      --------------   -----   \n",
      " 0   Sett_ID     204257 non-null  int64   \n",
      " 1   ADM_ID      204257 non-null  int64   \n",
      " 2   geometry    204257 non-null  geometry\n",
      " 3   gridcode    204257 non-null  int64   \n",
      " 4   gridstring  204257 non-null  object  \n",
      "dtypes: geometry(1), int64(3), object(1)\n",
      "memory usage: 7.8+ MB\n",
      "None    Sett_ID  ADM_ID                                           geometry  \\\n",
      "0        1     323  MULTIPOLYGON (((-1572100.720 271452.083, -1572...   \n",
      "1        2     323  POLYGON ((-1572896.437 272094.778, -1572804.62...   \n",
      "2        3     323  POLYGON ((-1571274.398 273778.027, -1571213.18...   \n",
      "3        4     323  POLYGON ((-1575038.754 274298.304, -1574977.54...   \n",
      "4        5     323  POLYGON ((-1573416.714 277205.733, -1573233.08...   \n",
      "\n",
      "   gridcode gridstring  \n",
      "0      1323  000001323  \n",
      "1      2323  000002323  \n",
      "2      3323  000003323  \n",
      "3      4323  000004323  \n",
      "4      5323  000005323  \n"
     ]
    }
   ],
   "source": [
    "# Dissolve any features that have the same G3 and ADM values so that we have a single unique feature per settlement.\n",
    "# Note: we do NOT want to dissolve the WSFE features. Distinct features for noncontiguous builtup areas of the same year is necessary to separate them in the Near tool step.\n",
    "GRID3_ADM = GRID3_ADM.dissolve(by=['Sett_ID', 'ADM_ID'], as_index=False)\n",
    "print(GRID3_ADM.info(), GRID3_ADM.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f172cebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: WSFE (538320, 5) and GRID3 (204257, 5)\n",
      "\n",
      "After: WSFE (538320, 5) and GRID3 (204257, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove features where year, settlement, or admin area = 0.\n",
    "# This was supposed to be resolved earlier with the gdal_calc NoDataValue parameter.\n",
    "\n",
    "print(\"Before: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))\n",
    "WSFE_ADM = WSFE_ADM.loc[(WSFE_ADM[\"year\"] != 0) & (WSFE_ADM[\"ADM_ID\"] != 0)] # Since we change the datatype to integer, no need to include all digits. Otherwise, it would need to be: != '0000'\n",
    "GRID3_ADM = GRID3_ADM.loc[(GRID3_ADM[\"Sett_ID\"] != 0) & (GRID3_ADM[\"ADM_ID\"] != 0)]\n",
    "print(\"After: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bounded_ID is our new unique settlement identifier for subsequent matching steps.\n",
    "GRID3_ADM['Bounded_ID'] = GRID3_ADM.index\n",
    "WSFE_ADM['WSFE_ID'] = WSFE_ADM.index\n",
    "GRID3_ADM = GRID3_ADM[['Sett_ID', 'Bounded_ID', 'ADM_ID', 'geometry']]\n",
    "WSFE_ADM = WSFE_ADM[['WSFE_ID', 'year', 'ADM_ID', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c78f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204257 204257 538320 7691\n"
     ]
    }
   ],
   "source": [
    "# Validation: \n",
    "# The first two printed numbers should be the same. There shouldn't be any GRID3 rows with matching Sett_ID and ADM_IDs.\n",
    "# The latter two numbers should be different, and the first should be larger. We never dissolved WSFE by any column.\n",
    "\n",
    "print(len(GRID3_ADM[['Sett_ID', 'ADM_ID']]),\n",
    "      len(GRID3_ADM[['Sett_ID', 'ADM_ID']].drop_duplicates()),\n",
    "      len(WSFE_ADM[['year', 'ADM_ID']]),\n",
    "      len(WSFE_ADM[['year', 'ADM_ID']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2489b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID3_ADM.to_file(\n",
    "    driver='GPKG', filename='Settlement/GRID3_ADM.gpkg', layer='GRID3_ADM_cleaned')\n",
    "WSFE_ADM.to_file(\n",
    "    driver='GPKG', filename=r'Buildup/WSFE_ADM.gpkg', layer='WSFE_ADM_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd7f5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f0a8e",
   "metadata": {},
   "source": [
    "## 4. UNIQUE SETTLEMENTS FROM WSFE AND GRID3: TWO VERSIONS\n",
    "\n",
    "Note that there are 2 versions here, so that we can create a fragmentation index:\n",
    "1. **Boundless, aka boundary-agnostic settlements**: Unique settlements are linked to GRID3 settlement IDs. Administrative areas do not influence the extents of the settlement.\n",
    "2. **Bounded, aka politically-defined settlements**: Settlements in the Boundless dataset which spread across more than one administrative area are split into separate settlements in the Bounded dataset. The largest polygon after the split is considered the \"principal\" settlement, and polygons in other admin areas are considered \"fragments.\" By dividing the fragment area(s) of the Bounded settlement by the area of the Boundless settlement, we can acquire a fragmentation index for each locality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6624",
   "metadata": {},
   "source": [
    "### 4.1 BOUNDED SETTLEMENTS: Near Join by ADM group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d770122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of admin areas with GRID3 features: 327\n",
      "Number of admin areas with WSFE features: 326\n",
      "Number of admin areas where one dataset is observed but the other is not: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of admin areas with GRID3 features: %s\" % len(GRID3_ADM['ADM_ID'].unique().tolist()))\n",
    "print(\"Number of admin areas with WSFE features: %s\" % len(WSFE_ADM['ADM_ID'].unique().tolist()))\n",
    "print(\"Number of admin areas where one dataset is observed but the other is not: %s\" % (\n",
    "    len(GRID3_ADM['ADM_ID'].unique().tolist()) - len(WSFE_ADM['ADM_ID'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5760273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADM_IDs = sorted(GRID3_ADM['ADM_ID'].unique().tolist())\n",
    "ADM_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64998fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating this field to help in removing duplicates from the sjoin_nearest, next section.\n",
    "GRID3_ADM['G3_Area'] = GRID3_ADM['geometry'].area / 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263ce3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   Sett_ID     0 non-null      int64   \n",
      " 1   Bounded_ID  0 non-null      int64   \n",
      " 2   ADM_ID      0 non-null      int64   \n",
      " 3   geometry    0 non-null      geometry\n",
      " 4   G3_Area     0 non-null      float64 \n",
      " 5   year        0 non-null      int32   \n",
      "dtypes: float64(1), geometry(1), int32(1), int64(3)\n",
      "memory usage: 0.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create empty geodataframe to append onto using the dataframe whose geometry we want to retain.\n",
    "Bounded = GRID3_ADM[0:0]\n",
    "Bounded[\"year\"] = pd.Series(dtype='int')\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad271fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed near join in admin area 1. Tue Dec 27 13:54:34 2022 \n",
      "\n",
      "Completed near join in admin area 2. Tue Dec 27 13:54:35 2022 \n",
      "\n",
      "Completed near join in admin area 3. Tue Dec 27 13:54:35 2022 \n",
      "\n",
      "Completed near join in admin area 4. Tue Dec 27 13:54:35 2022 \n",
      "\n",
      "Completed near join in admin area 5. Tue Dec 27 13:54:36 2022 \n",
      "\n",
      "Completed near join in admin area 6. Tue Dec 27 13:54:37 2022 \n",
      "\n",
      "Completed near join in admin area 7. Tue Dec 27 13:54:37 2022 \n",
      "\n",
      "Completed near join in admin area 8. Tue Dec 27 13:54:38 2022 \n",
      "\n",
      "Completed near join in admin area 9. Tue Dec 27 13:54:40 2022 \n",
      "\n",
      "Completed near join in admin area 10. Tue Dec 27 13:54:41 2022 \n",
      "\n",
      "Completed near join in admin area 11. Tue Dec 27 13:54:41 2022 \n",
      "\n",
      "Completed near join in admin area 12. Tue Dec 27 13:54:44 2022 \n",
      "\n",
      "Completed near join in admin area 13. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 14. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 15. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 16. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 17. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 18. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 19. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 20. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 21. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 22. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 23. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 24. Tue Dec 27 13:54:45 2022 \n",
      "\n",
      "Completed near join in admin area 25. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 26. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 27. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 28. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 29. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 30. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 31. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 32. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 33. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 34. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 35. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 36. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 37. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 38. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 39. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 40. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 41. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 42. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 43. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 44. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 45. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 46. Tue Dec 27 13:54:46 2022 \n",
      "\n",
      "Completed near join in admin area 47. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 48. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 49. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 50. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 51. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 52. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 53. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 54. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 55. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 56. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 57. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 58. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 59. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 60. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 61. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 62. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 63. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 64. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 65. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 66. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 67. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 68. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 69. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 70. Tue Dec 27 13:54:47 2022 \n",
      "\n",
      "Completed near join in admin area 71. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 72. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 73. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 74. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 75. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 76. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 77. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 78. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 79. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 80. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 81. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 82. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 83. Tue Dec 27 13:54:48 2022 \n",
      "\n",
      "Completed near join in admin area 84. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 85. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 86. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 87. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 88. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 89. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 90. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 91. Tue Dec 27 13:54:49 2022 \n",
      "\n",
      "Completed near join in admin area 92. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 93. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 94. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 95. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 96. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 97. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 98. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 99. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 100. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 101. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 102. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 103. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 104. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 105. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 106. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 107. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 108. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 109. Tue Dec 27 13:54:50 2022 \n",
      "\n",
      "Completed near join in admin area 110. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 111. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 112. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 113. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 114. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 115. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 116. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 117. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 118. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 119. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 120. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 121. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 122. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 123. Tue Dec 27 13:54:51 2022 \n",
      "\n",
      "Completed near join in admin area 124. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 125. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 126. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 127. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 128. Tue Dec 27 13:54:52 2022 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed near join in admin area 129. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 130. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 131. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 132. Tue Dec 27 13:54:52 2022 \n",
      "\n",
      "Completed near join in admin area 133. Tue Dec 27 13:54:53 2022 \n",
      "\n",
      "Completed near join in admin area 134. Tue Dec 27 13:54:53 2022 \n",
      "\n",
      "Completed near join in admin area 135. Tue Dec 27 13:54:53 2022 \n",
      "\n",
      "Completed near join in admin area 136. Tue Dec 27 13:54:54 2022 \n",
      "\n",
      "Completed near join in admin area 137. Tue Dec 27 13:54:55 2022 \n",
      "\n",
      "Completed near join in admin area 138. Tue Dec 27 13:54:55 2022 \n",
      "\n",
      "Completed near join in admin area 139. Tue Dec 27 13:54:55 2022 \n",
      "\n",
      "Completed near join in admin area 140. Tue Dec 27 13:54:55 2022 \n",
      "\n",
      "Completed near join in admin area 141. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 142. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 143. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 144. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 145. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 146. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 147. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 148. Tue Dec 27 13:54:56 2022 \n",
      "\n",
      "Completed near join in admin area 149. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 150. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 151. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 152. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 153. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 154. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 155. Tue Dec 27 13:54:57 2022 \n",
      "\n",
      "Completed near join in admin area 156. Tue Dec 27 13:54:58 2022 \n",
      "\n",
      "Completed near join in admin area 157. Tue Dec 27 13:54:58 2022 \n",
      "\n",
      "Completed near join in admin area 158. Tue Dec 27 13:54:58 2022 \n",
      "\n",
      "Completed near join in admin area 159. Tue Dec 27 13:54:58 2022 \n",
      "\n",
      "Completed near join in admin area 160. Tue Dec 27 13:54:58 2022 \n",
      "\n",
      "Completed near join in admin area 161. Tue Dec 27 13:54:59 2022 \n",
      "\n",
      "Completed near join in admin area 162. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 163. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 164. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 165. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 166. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 167. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 168. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 169. Tue Dec 27 13:55:00 2022 \n",
      "\n",
      "Completed near join in admin area 170. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 171. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 172. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 173. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 174. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 175. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 176. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 177. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 178. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 179. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 180. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 181. Tue Dec 27 13:55:01 2022 \n",
      "\n",
      "Completed near join in admin area 182. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 183. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 184. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 185. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 186. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 187. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 188. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 189. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 190. Tue Dec 27 13:55:02 2022 \n",
      "\n",
      "Completed near join in admin area 191. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 192. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 193. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 194. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 195. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 196. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 197. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 198. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 199. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 200. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 201. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 202. Tue Dec 27 13:55:03 2022 \n",
      "\n",
      "Completed near join in admin area 203. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 204. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 205. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 206. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 207. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 208. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 209. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 210. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 211. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 213. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 214. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 215. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 216. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 217. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 218. Tue Dec 27 13:55:04 2022 \n",
      "\n",
      "Completed near join in admin area 219. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 220. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 221. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 222. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 223. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 224. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 225. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 226. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 227. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 228. Tue Dec 27 13:55:05 2022 \n",
      "\n",
      "Completed near join in admin area 229. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 230. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 231. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 232. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 233. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 234. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 235. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 236. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 237. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 238. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 239. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 240. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 241. Tue Dec 27 13:55:06 2022 \n",
      "\n",
      "Completed near join in admin area 242. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 243. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 244. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 245. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 246. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 247. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 248. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 249. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 250. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 251. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 252. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 253. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 254. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 255. Tue Dec 27 13:55:07 2022 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed near join in admin area 256. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 257. Tue Dec 27 13:55:07 2022 \n",
      "\n",
      "Completed near join in admin area 258. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 259. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 260. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 261. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 262. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 263. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 264. Tue Dec 27 13:55:08 2022 \n",
      "\n",
      "Completed near join in admin area 265. Tue Dec 27 13:55:09 2022 \n",
      "\n",
      "Completed near join in admin area 266. Tue Dec 27 13:55:09 2022 \n",
      "\n",
      "Completed near join in admin area 267. Tue Dec 27 13:55:09 2022 \n",
      "\n",
      "Completed near join in admin area 268. Tue Dec 27 13:55:11 2022 \n",
      "\n",
      "Completed near join in admin area 269. Tue Dec 27 13:55:12 2022 \n",
      "\n",
      "Completed near join in admin area 270. Tue Dec 27 13:55:12 2022 \n",
      "\n",
      "Completed near join in admin area 271. Tue Dec 27 13:55:12 2022 \n",
      "\n",
      "Completed near join in admin area 272. Tue Dec 27 13:55:12 2022 \n",
      "\n",
      "Completed near join in admin area 273. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 274. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 275. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 276. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 277. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 278. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 279. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 280. Tue Dec 27 13:55:13 2022 \n",
      "\n",
      "Completed near join in admin area 281. Tue Dec 27 13:55:14 2022 \n",
      "\n",
      "Completed near join in admin area 282. Tue Dec 27 13:55:14 2022 \n",
      "\n",
      "Completed near join in admin area 283. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 284. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 285. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 286. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 287. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 288. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 289. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 290. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 291. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 292. Tue Dec 27 13:55:17 2022 \n",
      "\n",
      "Completed near join in admin area 293. Tue Dec 27 13:55:18 2022 \n",
      "\n",
      "Completed near join in admin area 294. Tue Dec 27 13:55:18 2022 \n",
      "\n",
      "Completed near join in admin area 295. Tue Dec 27 13:55:18 2022 \n",
      "\n",
      "Completed near join in admin area 296. Tue Dec 27 13:55:18 2022 \n",
      "\n",
      "Completed near join in admin area 297. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 298. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 299. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 300. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 301. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 302. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 303. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 304. Tue Dec 27 13:55:19 2022 \n",
      "\n",
      "Completed near join in admin area 305. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 306. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 307. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 308. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 309. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 310. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 311. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 312. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 313. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 314. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 315. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 316. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 317. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 318. Tue Dec 27 13:55:20 2022 \n",
      "\n",
      "Completed near join in admin area 319. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 320. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 321. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 322. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 323. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 324. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 325. Tue Dec 27 13:55:21 2022 \n",
      "\n",
      "Completed near join in admin area 326. Tue Dec 27 13:55:22 2022 \n",
      "\n",
      "Completed near join in admin area 327. Tue Dec 27 13:55:22 2022 \n",
      "\n",
      "Completed near join in admin area 328. Tue Dec 27 13:55:22 2022 \n",
      "\n",
      "Completed near join for all ADMs. Tue Dec 27 13:55:22 2022 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ID in ADM_IDs:\n",
    "    WSFE_shard = WSFE_ADM.loc[WSFE_ADM['ADM_ID'] == ID]\n",
    "    GRID3_shard = GRID3_ADM.loc[GRID3_ADM['ADM_ID'] == ID]\n",
    "    WSFE_GRID3_shard = gpd.sjoin_nearest(WSFE_shard, \n",
    "                                         GRID3_shard, \n",
    "                                         how='inner',\n",
    "                                         max_distance=500)\n",
    "    Bounded = pd.concat([Bounded, WSFE_GRID3_shard])\n",
    "    print('Completed near join in admin area %s. %s \\n' % (ID, time.ctime()))\n",
    "print('Completed near join for all ADMs. %s \\n' % time.ctime())\n",
    "\n",
    "del WSFE_shard, GRID3_shard, WSFE_GRID3_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94d2026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sett_ID</th>\n",
       "      <th>Bounded_ID</th>\n",
       "      <th>ADM_ID</th>\n",
       "      <th>geometry</th>\n",
       "      <th>G3_Area</th>\n",
       "      <th>year</th>\n",
       "      <th>WSFE_ID</th>\n",
       "      <th>ADM_ID_left</th>\n",
       "      <th>index_right</th>\n",
       "      <th>ADM_ID_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>339569</th>\n",
       "      <td>43369</td>\n",
       "      <td>44716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1081755.061 552279.181, -1081724.45...</td>\n",
       "      <td>0.069311</td>\n",
       "      <td>1985</td>\n",
       "      <td>339569.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>44716.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413689</th>\n",
       "      <td>12521</td>\n",
       "      <td>13108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1581037.240 471972.914, -1580976.03...</td>\n",
       "      <td>167.851870</td>\n",
       "      <td>2002</td>\n",
       "      <td>413689.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13108.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442714</th>\n",
       "      <td>19120</td>\n",
       "      <td>19821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1401572.321 460955.286, -1401541.71...</td>\n",
       "      <td>209.284936</td>\n",
       "      <td>2003</td>\n",
       "      <td>442714.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19821.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>188152</td>\n",
       "      <td>190275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1128304.539 1318922.469, -1128273.9...</td>\n",
       "      <td>0.792395</td>\n",
       "      <td>1994</td>\n",
       "      <td>10555.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>190275.0</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>145497</td>\n",
       "      <td>148305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1207233.603 1101324.315, -1207202.9...</td>\n",
       "      <td>54.711765</td>\n",
       "      <td>1998</td>\n",
       "      <td>46173.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>148305.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27318</th>\n",
       "      <td>182922</td>\n",
       "      <td>186307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1120408.573 1240574.892, -1120377.9...</td>\n",
       "      <td>9.868405</td>\n",
       "      <td>1998</td>\n",
       "      <td>27318.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>186307.0</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402742</th>\n",
       "      <td>7747</td>\n",
       "      <td>8222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1639094.020 478155.028, -1639032.81...</td>\n",
       "      <td>31.207801</td>\n",
       "      <td>2014</td>\n",
       "      <td>402742.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8222.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324287</th>\n",
       "      <td>12893</td>\n",
       "      <td>13484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1566530.697 582700.077, -1566500.09...</td>\n",
       "      <td>15.348667</td>\n",
       "      <td>1995</td>\n",
       "      <td>324287.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13484.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261650</th>\n",
       "      <td>49133</td>\n",
       "      <td>50573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1518328.574 641307.737, -1518297.96...</td>\n",
       "      <td>117.446761</td>\n",
       "      <td>2008</td>\n",
       "      <td>261650.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50573.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117308</th>\n",
       "      <td>59177</td>\n",
       "      <td>60893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1543485.491 702027.110, -1543454.88...</td>\n",
       "      <td>90.981152</td>\n",
       "      <td>2008</td>\n",
       "      <td>117308.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60893.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116701</th>\n",
       "      <td>59254</td>\n",
       "      <td>60985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1551381.458 702241.341, -1551350.85...</td>\n",
       "      <td>2.097130</td>\n",
       "      <td>2015</td>\n",
       "      <td>116701.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60985.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379300</th>\n",
       "      <td>19118</td>\n",
       "      <td>19819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1399001.541 488989.029, -1398970.93...</td>\n",
       "      <td>12.084488</td>\n",
       "      <td>2001</td>\n",
       "      <td>379300.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>19819.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387906</th>\n",
       "      <td>7798</td>\n",
       "      <td>8280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1629484.200 482623.288, -1629422.99...</td>\n",
       "      <td>1.359060</td>\n",
       "      <td>2007</td>\n",
       "      <td>387906.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>8280.0</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440748</th>\n",
       "      <td>19120</td>\n",
       "      <td>19823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1394013.004 461934.631, -1393982.39...</td>\n",
       "      <td>32.579974</td>\n",
       "      <td>1985</td>\n",
       "      <td>440748.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>19823.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456442</th>\n",
       "      <td>19120</td>\n",
       "      <td>19821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1404112.496 454589.546, -1404081.89...</td>\n",
       "      <td>209.284936</td>\n",
       "      <td>1993</td>\n",
       "      <td>456442.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19821.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216464</th>\n",
       "      <td>56244</td>\n",
       "      <td>57885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1544036.373 655600.049, -1544005.76...</td>\n",
       "      <td>137.482359</td>\n",
       "      <td>1999</td>\n",
       "      <td>216464.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>57885.0</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441773</th>\n",
       "      <td>19120</td>\n",
       "      <td>19823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1389361.116 461383.750, -1389330.51...</td>\n",
       "      <td>32.579974</td>\n",
       "      <td>2013</td>\n",
       "      <td>441773.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>19823.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19084</th>\n",
       "      <td>183266</td>\n",
       "      <td>186674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1122459.076 1276167.951, -1122428.4...</td>\n",
       "      <td>0.511404</td>\n",
       "      <td>2003</td>\n",
       "      <td>19084.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>186674.0</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343274</th>\n",
       "      <td>45189</td>\n",
       "      <td>46550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1610662.418 544597.446, -1610601.20...</td>\n",
       "      <td>28.212437</td>\n",
       "      <td>2008</td>\n",
       "      <td>343274.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46550.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484837</th>\n",
       "      <td>33887</td>\n",
       "      <td>34932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLYGON ((-1325458.873 445836.652, -1325428.26...</td>\n",
       "      <td>5.685385</td>\n",
       "      <td>2008</td>\n",
       "      <td>484837.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>34932.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sett_ID  Bounded_ID  ADM_ID  \\\n",
       "339569    43369       44716     NaN   \n",
       "413689    12521       13108     NaN   \n",
       "442714    19120       19821     NaN   \n",
       "10555    188152      190275     NaN   \n",
       "46173    145497      148305     NaN   \n",
       "27318    182922      186307     NaN   \n",
       "402742     7747        8222     NaN   \n",
       "324287    12893       13484     NaN   \n",
       "261650    49133       50573     NaN   \n",
       "117308    59177       60893     NaN   \n",
       "116701    59254       60985     NaN   \n",
       "379300    19118       19819     NaN   \n",
       "387906     7798        8280     NaN   \n",
       "440748    19120       19823     NaN   \n",
       "456442    19120       19821     NaN   \n",
       "216464    56244       57885     NaN   \n",
       "441773    19120       19823     NaN   \n",
       "19084    183266      186674     NaN   \n",
       "343274    45189       46550     NaN   \n",
       "484837    33887       34932     NaN   \n",
       "\n",
       "                                                 geometry     G3_Area  year  \\\n",
       "339569  POLYGON ((-1081755.061 552279.181, -1081724.45...    0.069311  1985   \n",
       "413689  POLYGON ((-1581037.240 471972.914, -1580976.03...  167.851870  2002   \n",
       "442714  POLYGON ((-1401572.321 460955.286, -1401541.71...  209.284936  2003   \n",
       "10555   POLYGON ((-1128304.539 1318922.469, -1128273.9...    0.792395  1994   \n",
       "46173   POLYGON ((-1207233.603 1101324.315, -1207202.9...   54.711765  1998   \n",
       "27318   POLYGON ((-1120408.573 1240574.892, -1120377.9...    9.868405  1998   \n",
       "402742  POLYGON ((-1639094.020 478155.028, -1639032.81...   31.207801  2014   \n",
       "324287  POLYGON ((-1566530.697 582700.077, -1566500.09...   15.348667  1995   \n",
       "261650  POLYGON ((-1518328.574 641307.737, -1518297.96...  117.446761  2008   \n",
       "117308  POLYGON ((-1543485.491 702027.110, -1543454.88...   90.981152  2008   \n",
       "116701  POLYGON ((-1551381.458 702241.341, -1551350.85...    2.097130  2015   \n",
       "379300  POLYGON ((-1399001.541 488989.029, -1398970.93...   12.084488  2001   \n",
       "387906  POLYGON ((-1629484.200 482623.288, -1629422.99...    1.359060  2007   \n",
       "440748  POLYGON ((-1394013.004 461934.631, -1393982.39...   32.579974  1985   \n",
       "456442  POLYGON ((-1404112.496 454589.546, -1404081.89...  209.284936  1993   \n",
       "216464  POLYGON ((-1544036.373 655600.049, -1544005.76...  137.482359  1999   \n",
       "441773  POLYGON ((-1389361.116 461383.750, -1389330.51...   32.579974  2013   \n",
       "19084   POLYGON ((-1122459.076 1276167.951, -1122428.4...    0.511404  2003   \n",
       "343274  POLYGON ((-1610662.418 544597.446, -1610601.20...   28.212437  2008   \n",
       "484837  POLYGON ((-1325458.873 445836.652, -1325428.26...    5.685385  2008   \n",
       "\n",
       "         WSFE_ID  ADM_ID_left  index_right  ADM_ID_right  \n",
       "339569  339569.0         72.0      44716.0          72.0  \n",
       "413689  413689.0          9.0      13108.0           9.0  \n",
       "442714  442714.0         12.0      19821.0          12.0  \n",
       "10555    10555.0        261.0     190275.0         261.0  \n",
       "46173    46173.0          2.0     148305.0           2.0  \n",
       "27318    27318.0        241.0     186307.0         241.0  \n",
       "402742  402742.0         10.0       8222.0          10.0  \n",
       "324287  324287.0          3.0      13484.0           3.0  \n",
       "261650  261650.0          6.0      50573.0           6.0  \n",
       "117308  117308.0          1.0      60893.0           1.0  \n",
       "116701  116701.0          1.0      60985.0           1.0  \n",
       "379300  379300.0        278.0      19819.0         278.0  \n",
       "387906  387906.0        319.0       8280.0         319.0  \n",
       "440748  440748.0         86.0      19823.0          86.0  \n",
       "456442  456442.0         12.0      19821.0          12.0  \n",
       "216464  216464.0        268.0      57885.0         268.0  \n",
       "441773  441773.0         86.0      19823.0          86.0  \n",
       "19084    19084.0        242.0     186674.0         242.0  \n",
       "343274  343274.0          4.0      46550.0           4.0  \n",
       "484837  484837.0        106.0      34932.0         106.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bounded.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d60ac21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 536223 entries, 104264 to 3660\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   Sett_ID       536223 non-null  int64   \n",
      " 1   Bounded_ID    536223 non-null  int64   \n",
      " 2   ADM_ID        0 non-null       float64 \n",
      " 3   geometry      536223 non-null  geometry\n",
      " 4   G3_Area       536223 non-null  float64 \n",
      " 5   year          536223 non-null  int32   \n",
      " 6   WSFE_ID       536223 non-null  float64 \n",
      " 7   ADM_ID_left   536223 non-null  float64 \n",
      " 8   index_right   536223 non-null  float64 \n",
      " 9   ADM_ID_right  536223 non-null  float64 \n",
      "dtypes: float64(6), geometry(1), int32(1), int64(2)\n",
      "memory usage: 43.0 MB\n"
     ]
    }
   ],
   "source": [
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "268855f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 536223 entries, 104264 to 3660\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   Sett_ID       536223 non-null  int64   \n",
      " 1   Bounded_ID    536223 non-null  int64   \n",
      " 2   ADM_ID        0 non-null       float64 \n",
      " 3   geometry      536223 non-null  geometry\n",
      " 4   G3_Area       536223 non-null  float64 \n",
      " 5   year          536223 non-null  int32   \n",
      " 6   WSFE_ID       536223 non-null  float64 \n",
      " 7   ADM_ID_left   536223 non-null  float64 \n",
      " 8   index_right   536223 non-null  float64 \n",
      " 9   ADM_ID_right  536223 non-null  float64 \n",
      "dtypes: float64(6), geometry(1), int32(1), int64(2)\n",
      "memory usage: 43.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Remove WSFE features that did not match any GRID3 settlements.\n",
    "Bounded = Bounded.loc[~Bounded['Sett_ID'].isna()]\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12cc76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del GRID3_ADM, ADM_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31d8d9",
   "metadata": {},
   "source": [
    "### 4.2 Remove duplicates: where buildup polygons intersected with more than one GRID3 settlement extent.\n",
    "This happens when the first dataset (WSFE) intersects (distance = 0) with more than one feature of the second dataset (GRID3). More common for large cities. For example, YaoundÃ©, CMN has a large contiguous 1985 WSFE polygon which overlaps several small GRID3 features that are not YaoundÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b9400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1864\n"
     ]
    }
   ],
   "source": [
    "# The first number should always be zero. \n",
    "# The second tells us whether/how many WSFE polygons were duplicated by the Near join.\n",
    "\n",
    "print(len(WSFE_ADM[WSFE_ADM.duplicated('WSFE_ID')]), len(Bounded[Bounded.duplicated('WSFE_ID')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f837ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 534359 entries, 480176 to 3912\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   Sett_ID       534359 non-null  int64   \n",
      " 1   Bounded_ID    534359 non-null  int64   \n",
      " 2   ADM_ID        0 non-null       float64 \n",
      " 3   geometry      534359 non-null  geometry\n",
      " 4   G3_Area       534359 non-null  float64 \n",
      " 5   year          534359 non-null  int32   \n",
      " 6   WSFE_ID       534359 non-null  float64 \n",
      " 7   ADM_ID_left   534359 non-null  float64 \n",
      " 8   index_right   534359 non-null  float64 \n",
      " 9   ADM_ID_right  534359 non-null  float64 \n",
      "dtypes: float64(6), geometry(1), int32(1), int64(2)\n",
      "memory usage: 42.8 MB\n"
     ]
    }
   ],
   "source": [
    "# If there are duplicate WSFE_IDs, then we need to choose between them.\n",
    "# We'll pick the one that joined with the largest GRID3 polygon.\n",
    "# To do that, we can just sort the dataframe by GRID3 areas, then drop_duplicates. \n",
    "# It will retain the first row of each WSFE_ID group.\n",
    "Bounded = Bounded.sort_values('G3_Area', ascending=False).drop_duplicates(['WSFE_ID'])\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ee7da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(Bounded[Bounded.duplicated('WSFE_ID')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d22854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 82254 entries, 0 to 82253\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   year          82254 non-null  int64   \n",
      " 1   Bounded_ID    82254 non-null  int64   \n",
      " 2   geometry      82254 non-null  geometry\n",
      " 3   Sett_ID       82254 non-null  int64   \n",
      " 4   ADM_ID        0 non-null      float64 \n",
      " 5   G3_Area       82254 non-null  float64 \n",
      " 6   WSFE_ID       82254 non-null  float64 \n",
      " 7   ADM_ID_left   82254 non-null  float64 \n",
      " 8   index_right   82254 non-null  float64 \n",
      " 9   ADM_ID_right  82254 non-null  float64 \n",
      "dtypes: float64(6), geometry(1), int64(3)\n",
      "memory usage: 6.3 MB\n",
      "None        year  Bounded_ID                                           geometry  \\\n",
      "32316  2001      198530  MULTIPOLYGON (((-1671167.559 582669.472, -1671...   \n",
      "12821  1989       70751  MULTIPOLYGON (((-1471901.513 666005.587, -1471...   \n",
      "40280  2004       74875  MULTIPOLYGON (((-1444143.211 663832.666, -1444...   \n",
      "49027  2007       47864  MULTIPOLYGON (((-1633126.138 687887.820, -1633...   \n",
      "49566  2007       69959  POLYGON ((-1498619.261 604612.915, -1498588.65...   \n",
      "66793  2012       49863  POLYGON ((-1581343.286 595921.230, -1581312.68...   \n",
      "6725   1985       53283  MULTIPOLYGON (((-1552330.198 643694.890, -1552...   \n",
      "66529  2012       43054  MULTIPOLYGON (((-1099352.661 524949.343, -1099...   \n",
      "53330  2008       81863  POLYGON ((-1571733.466 680022.458, -1571702.86...   \n",
      "53363  2008       84597  POLYGON ((-1507739.409 742119.034, -1507708.80...   \n",
      "\n",
      "       Sett_ID  ADM_ID   G3_Area   WSFE_ID  ADM_ID_left  index_right  \\\n",
      "32316   196248     NaN  2.027819  323861.0        175.0     198530.0   \n",
      "12821    68775     NaN  0.434599  189884.0        325.0      70751.0   \n",
      "40280    72867     NaN  0.077741  194587.0        140.0      74875.0   \n",
      "49027    46460     NaN  0.172341  137626.0        182.0      47864.0   \n",
      "49566    68002     NaN  0.147052  309693.0        133.0      69959.0   \n",
      "66793    48438     NaN  0.161102  317407.0        171.0      49863.0   \n",
      "6725     51728     NaN  0.888868  252334.0        282.0      53283.0   \n",
      "66529    41715     NaN  0.020606  361407.0         71.0      43054.0   \n",
      "53330    79806     NaN  0.031846  154801.0        161.0      81863.0   \n",
      "53363    82497     NaN  1.335644   95683.0        149.0      84597.0   \n",
      "\n",
      "       ADM_ID_right  \n",
      "32316         175.0  \n",
      "12821         325.0  \n",
      "40280         140.0  \n",
      "49027         182.0  \n",
      "49566         133.0  \n",
      "66793         171.0  \n",
      "6725          282.0  \n",
      "66529          71.0  \n",
      "53330         161.0  \n",
      "53363         149.0  \n"
     ]
    }
   ],
   "source": [
    "# Now we can dissolve with the WSFE years, now that we can group them by their administratively split ID.\n",
    "Bounded = Bounded.dissolve(by=['year', 'Bounded_ID'], as_index=False)\n",
    "print(Bounded.info(), Bounded.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db8f3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Bounded = Bounded[['ADM_ID_left', 'year', 'Bounded_ID', 'Sett_ID', 'geometry']].rename(columns={\"ADM_ID_left\": \"ADM_ID\"})\n",
    "Bounded = Bounded.astype({\"ADM_ID\":'int', \"Bounded_ID\":'int', \"Sett_ID\":'int', \"year\":'int'})\n",
    "Bounded.to_file(\n",
    "    driver='GPKG', filename=r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Bounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e43acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del WSFE_ADM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eadab",
   "metadata": {},
   "source": [
    "### 4.3 BOUNDLESS SETTLEMENTS: Dissolve features that were split by an ADM boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2638d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 79285 entries, 0 to 79284\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   year        79285 non-null  int64   \n",
      " 1   Sett_ID     79285 non-null  int64   \n",
      " 2   geometry    79285 non-null  geometry\n",
      " 3   ADM_ID      79285 non-null  int32   \n",
      " 4   Bounded_ID  79285 non-null  int32   \n",
      "dtypes: geometry(1), int32(2), int64(2)\n",
      "memory usage: 2.4 MB\n",
      "None        year  Sett_ID                                           geometry  \\\n",
      "58779  2010   196240  MULTIPOLYGON (((-1636186.590 575140.760, -1636...   \n",
      "73127  2014    68708  MULTIPOLYGON (((-1492314.730 653733.173, -1492...   \n",
      "70306  2013   197876  POLYGON ((-1421954.933 413365.254, -1421924.32...   \n",
      "51167  2008    75034  MULTIPOLYGON (((-1117501.143 575232.573, -1117...   \n",
      "57844  2010    71907  POLYGON ((-1494365.233 679287.950, -1494334.62...   \n",
      "51929  2008   191729  POLYGON ((-1107279.232 1473842.562, -1107248.6...   \n",
      "13314  1991    79684  MULTIPOLYGON (((-1566561.301 704077.613, -1566...   \n",
      "1205   1985     3700  MULTIPOLYGON (((-1379261.624 325346.647, -1379...   \n",
      "20500  1998   126045  MULTIPOLYGON (((-1099260.848 831606.658, -1099...   \n",
      "77515  2015    66533  POLYGON ((-1504923.793 602960.270, -1504893.18...   \n",
      "\n",
      "       ADM_ID  Bounded_ID  \n",
      "58779     169      198522  \n",
      "73127     144       70679  \n",
      "70306     300      200201  \n",
      "51167      70       77063  \n",
      "57844     144       73913  \n",
      "51929     328      193932  \n",
      "13314     160       81740  \n",
      "1205       19        3854  \n",
      "20500     208      128641  \n",
      "77515     133       68472  \n"
     ]
    }
   ],
   "source": [
    "# Fragments of any bounded settlement will be combined into a single \"boundless\" settlement in this version.\n",
    "# It is based on their \"Sett_ID\", which is a direct loan from the GRID3 settlement features.\n",
    "Boundless = Bounded.dissolve(by=['year', 'Sett_ID'], as_index=False)\n",
    "print(Boundless.info(), Boundless.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b76e2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Boundless.to_file(driver='GPKG', filename=r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Boundless')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba12eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027eba",
   "metadata": {},
   "source": [
    "## 5. CUMULATIVE ANNUALIZED SETTLEMENT EXTENTS\n",
    "DISSOLVE BY YEAR SETS: Create separate feature layers of each cumulative year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d70322",
   "metadata": {},
   "source": [
    "### 5.1 Define study years for each for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60c3aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015] \n",
      "\n",
      " [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
      "\n",
      "\n",
      " [2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999]\n"
     ]
    }
   ],
   "source": [
    "# Boundless = gpd.read_file(r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Boundless')\n",
    "\n",
    "def CreateList(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]\n",
    "\n",
    "CuStart, CuEnd = Boundless['year'].min(), Boundless['year'].max()\n",
    "StudyStart, StudyEnd = 1999, Boundless['year'].max()\n",
    "\n",
    "AllCuYears = CreateList(CuStart, CuEnd) # All years in the WSFE dataset\n",
    "AllStudyYears = CreateList(StudyStart, StudyEnd) # All years for which there will be growth stats in the present study.\n",
    "print(AllCuYears, '\\n\\n', AllStudyYears)\n",
    "\n",
    "ReversedStudyYears = []\n",
    "for i in AllStudyYears:\n",
    "    ReversedStudyYears.insert(0,i)\n",
    "ReversedStudyYears.remove(StudyEnd)\n",
    "print('\\n\\n', ReversedStudyYears)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de5331",
   "metadata": {},
   "source": [
    "### 5.2 Starting with main Boundless dataset, create a cumulative area feature layer for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3c1be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to cumulative area for year: 1999. Tue Dec 27 14:01:10 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:01:10 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:01:34 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2000. Tue Dec 27 14:01:41 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:01:41 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:02:07 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2001. Tue Dec 27 14:02:14 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:02:14 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:02:43 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2002. Tue Dec 27 14:02:49 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:02:49 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:03:18 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2003. Tue Dec 27 14:03:25 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:03:25 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:03:56 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2004. Tue Dec 27 14:04:02 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:04:02 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:04:34 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2005. Tue Dec 27 14:04:41 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:04:41 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:05:15 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2006. Tue Dec 27 14:05:21 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:05:21 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:05:56 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2007. Tue Dec 27 14:06:02 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:06:02 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:06:39 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2008. Tue Dec 27 14:06:45 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:06:45 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:07:23 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2009. Tue Dec 27 14:07:30 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:07:30 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:08:09 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2010. Tue Dec 27 14:08:17 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:08:17 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:09:00 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2011. Tue Dec 27 14:09:07 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:09:07 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:09:52 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2012. Tue Dec 27 14:09:59 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:09:59 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:10:46 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2013. Tue Dec 27 14:10:53 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:10:53 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:11:42 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2014. Tue Dec 27 14:11:48 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:11:48 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:12:37 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2015. Tue Dec 27 14:12:43 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Tue Dec 27 14:12:43 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_22820\\457725344.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Boundless[Boundless['year'].between(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Tue Dec 27 14:13:35 2022\n",
      "\n",
      "Done with all years in set. Tue Dec 27 14:13:41 2022\n"
     ]
    }
   ],
   "source": [
    "# For each year in the growth stats study, we are taking features from all years prior to and including that year, \n",
    "# dissolving those features, and exporting as its own file.\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYearSet = Boundless[Boundless['year'].between(\n",
    "        CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    CuYearDissolve = CuYearSet.dissolve(by='Sett_ID', \n",
    "                                        as_index=False)[['Sett_ID', 'geometry']]\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    CuYearName = ''.join(['Cu', str(item), '_Boundless'])\n",
    "    CuYearDissolve.to_file(driver='GPKG', filename=r'Results/CumulativeSettlements2.gpkg', layer=CuYearName)\n",
    "    del CuYearSet, CuYearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c4746",
   "metadata": {},
   "source": [
    "##### Join area information from each cumulative layer onto the latest year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b33dc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "Results/CumulativeSettlements.gpkg: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mfiona\\_shim.pyx:83\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: Results/CumulativeSettlements.gpkg: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The latest year in the study contains all settlements. Merge all other years' areas onto this dataset.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m SettAreas \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResults/CumulativeSettlements.gpkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStudyEnd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_Boundless\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m SettAreas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArea2015\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SettAreas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39marea \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m      5\u001b[0m SettAreas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(SettAreas)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# We have settlement IDs, so no need to join spatially!\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\geopandas\\io\\file.py:253\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m     path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_fiona(\n\u001b[0;32m    254\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[0;32m    258\u001b[0m         path_or_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    259\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\geopandas\\io\\file.py:294\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m reader(path_or_bytes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# In a future Fiona release the crs attribute of features will\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# no longer be a dict, but will behave like a dict. So this should\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;66;03m# be forwards compatible\u001b[39;00m\n\u001b[0;32m    299\u001b[0m         crs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    300\u001b[0m             features\u001b[38;5;241m.\u001b[39mcrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[0;32m    303\u001b[0m         )\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# handle loading the bounding box\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\fiona\\env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local\u001b[38;5;241m.\u001b[39m_env:\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\fiona\\__init__.py:264\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 264\u001b[0m     c \u001b[38;5;241m=\u001b[39m Collection(path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    265\u001b[0m                    layer\u001b[38;5;241m=\u001b[39mlayer, enabled_drivers\u001b[38;5;241m=\u001b[39menabled_drivers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;66;03m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\fiona\\collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mstart(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\ogrext.pyx:540\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\_shim.pyx:90\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: Results/CumulativeSettlements.gpkg: No such file or directory"
     ]
    }
   ],
   "source": [
    "# The latest year in the study contains all settlements. Merge all other years' areas onto this dataset.\n",
    "SettAreas = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=\n",
    "                          ''.join(['Cu', str(StudyEnd), '_Boundless'])) \n",
    "SettAreas['Area2015'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry') # We have settlement IDs, so no need to join spatially!\n",
    "\n",
    "\n",
    "for item in ReversedStudyYears:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=''.join(['Cu', str(item), '_Boundless']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['Area', str(item)])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Sett_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, StudyEnd, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Sett_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(ResultsFolder, 'Areas%sto%s.csv' % (StudyStart, StudyEnd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0879f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cafcd",
   "metadata": {},
   "source": [
    "### 5.3 Repeat for Bounded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dcf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded = gpd.read_file(r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Bounded')\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYearSet = Bounded[Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    CuYearDissolve = CuYearSet.dissolve(by='Bounded_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\", \"Sett_ID\":\"min\"}, # Though ADM_ID and Sett_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    CuYearName = ''.join(['Cu', str(item), '_Bounded'])\n",
    "    CuYearDissolve.to_file(driver='GPKG', filename=r'Results/CumulativeSettlements.gpkg', layer=CuYearName)\n",
    "    del CuYearSet, CuYearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaeaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SettAreas = gpd.read_file(r'Results/CumulativeSettlements.gpkg', \n",
    "                          layer=''.join(['Cu', str(StudyEnd), '_Bounded']))\n",
    "SettAreas['Area2015'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry')\n",
    "\n",
    "\n",
    "for item in ReversedStudyYears:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=''.join(['Cu', str(item), '_Bounded']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['Area', str(item)])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Bounded_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, StudyEnd, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Bounded_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(ResultsFolder, 'Areas%sto%s_%s.csv' % (StudyStart, StudyEnd, 'Bounded')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7574cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ecaf8",
   "metadata": {},
   "source": [
    "### 5.4 One settlement geofile to rule them all. ...and in the Sett_ID bind them.\n",
    "The annualized values can be stored as distinct non-spatial dataframes. Their Sett_IDs will be used to join onto this geoversion with place names for the summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settlements = gpd.read_file(r'Results/CumulativeSettlements.gpkg', \n",
    "                           layer=''.join(['Cu', str(StudyEnd), '_Boundless']))[['Sett_ID', 'ADM_ID', 'geometry']]\n",
    "print(Settlements.info())\n",
    "Settlements.to_file(driver='GPKG', \n",
    "                       filename=r'Results/SETTLEMENTS.gpkg', \n",
    "                       layer='SETTLEMENTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8856d7",
   "metadata": {},
   "source": [
    "### 5.5 Buffer the area of the Boundless dataset's latest year to mask raster data in later sections.\n",
    "The Bounded dataset would also be fine for our purposes here. The buffer is dissolved to a single feature to be used for its total extents, which are identical between Bounded & Boundless datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buffer layer(s) to use as maximum distance for Near joins.\n",
    "\n",
    "# Population buffer: 2km\n",
    "Distance = 2000\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS')\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(\n",
    "    make_valid).buffer(Distance) # make_valid is a workaround for any null geometries.\n",
    "print('Finished buffer layer creation. %s' % time.ctime())\n",
    "BufferFileName1 = ''.join(['Buff', str(Distance), 'm_', str(StudyEnd)])\n",
    "BufferLayer.to_file(driver='GPKG', filename=r'Results/Catchment.gpkg', layer=BufferFileName1)\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1556d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTL buffer: 250m\n",
    "Distance = 250\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS')\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(\n",
    "    make_valid).buffer(Distance) # make_valid is a workaround for any null geometries.\n",
    "print('Finished buffer layer creation. %s' % time.ctime())\n",
    "BufferFileName2 = ''.join(['Buff', str(Distance), 'm_', str(StudyEnd)])\n",
    "BufferLayer.to_file(driver='GPKG', filename=r'Results/Catchment.gpkg', layer=BufferFileName2)\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef5b7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ce84a",
   "metadata": {},
   "source": [
    "## 6. PLACE NAMES\n",
    "Join urban place names from UCDB, Africapolis, and GeoNames onto the settlement vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456bf69",
   "metadata": {},
   "source": [
    "### 6.1 Load placename datasets, filter, and project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f815f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If restarting here:\n",
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS')\n",
    "Settlements['Area2015'] = Settlements['geometry'].area / 10**6\n",
    "\n",
    "# Load, pull name field, rename, and reproject to match the catchments CRS.\n",
    "UCDB = gpd.read_file('PlaceName/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_2.gpkg', \n",
    "                     layer=0)[['UC_NM_MN', 'geometry']].rename(\n",
    "    columns={\"UC_NM_MN\": \"UCDB_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "Africapolis = gpd.read_file('PlaceName/AFRICAPOLIS2020.shp')[['agglosName', 'geometry']].rename(\n",
    "    columns={\"agglosName\": \"Afpl_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "GeoNames = gpd.read_file('PlaceName/GeoNames.gpkg', \n",
    "                         layer=0)[['GeoName', 'geometry']].to_crs(\"ESRI:102022\")\n",
    "\n",
    "print(Settlements.info(), UCDB.info(), Africapolis.info(), GeoNames.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e934567",
   "metadata": {},
   "source": [
    "### 6.2 Join placenames onto settlements geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap it in pd.DataFrame() since the sjoin() is the last time we need the geometry.\n",
    "\n",
    "GeoNames = pd.DataFrame(gpd.sjoin_nearest(GeoNames, Settlements, \n",
    "                             how='left', distance_col=\"distGN\", max_distance=250, \n",
    "                             lsuffix=\"G3\", rsuffix=\"GN\")).drop(columns='geometry')\n",
    "Africapolis = pd.DataFrame(gpd.sjoin_nearest(Africapolis, Settlements, \n",
    "                             how='left', distance_col=\"distAF\", max_distance=250,\n",
    "                             lsuffix=\"G3\", rsuffix=\"Af\")).drop(columns='geometry')\n",
    "UCDB = pd.DataFrame(gpd.sjoin_nearest(UCDB, Settlements, \n",
    "                             how='left', distance_col=\"distUC\", max_distance=250,\n",
    "                             lsuffix=\"G3\", rsuffix=\"UC\")).drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f0ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(GeoNames.info())\n",
    "print(Africapolis.info())\n",
    "print(UCDB.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fed8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldatasets = [pd.DataFrame(Settlements).drop(columns='geometry'),\n",
    "               Africapolis[['Sett_ID', 'Afpl_Name', 'distAF']], \n",
    "               GeoNames[['Sett_ID', 'GeoName', 'distGN']],\n",
    "               UCDB[['Sett_ID', 'UCDB_Name', 'distUC']]]\n",
    "\n",
    "SettlementsNamed = reduce(lambda left,right: pd.merge(left,right,on=['Sett_ID'], how='left'), alldatasets)\n",
    "SettlementsNamed[['Afpl_Name', 'GeoName', 'UCDB_Name']] = SettlementsNamed[['Afpl_Name', 'GeoName', 'UCDB_Name']].fillna('UNK')\n",
    "SettlementsNamed[['distAF', 'distGN', 'distUC']] = SettlementsNamed[['distAF', 'distGN', 'distUC']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af429c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(SettlementsNamed.info())\n",
    "print(SettlementsNamed.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del UCDB, Africapolis, GeoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fc7cd",
   "metadata": {},
   "source": [
    "### 6.3 Reduce to single name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0c215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fbfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a5022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7030548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdac073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b606e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The near joins should have prevented duplication of rows, but sometimes they creep in.\n",
    "SettlementsNamed.drop_duplicates(subset=['Sett_ID'], inplace=True, keep='first')\n",
    "SettlementsNamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88954a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which source has a name geometrically closest to the settlement.\n",
    "# Since we switched NaN values to -1 earlier, we also resolved what happens in the event of a tie, \n",
    "# i.e. when more than one source is 0.0 meters from the settlement. It will take the value from the first column.\n",
    "SettlementsNamed['SettName'] = \"UNK\"\n",
    "SettlementsNamed['closest'] = SettlementsNamed[['distAF', 'distGN', 'distUC']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ead5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single name column where non-named settlements are \"UNK\" but all others use one of the three name sources.\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distAF\", \n",
    "    'SettName'] = SettlementsNamed['Afpl_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distUC\", \n",
    "    'SettName'] = SettlementsNamed['UCDB_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distGN\", \n",
    "    'SettName'] = SettlementsNamed['GeoName']\n",
    "\n",
    "SettlementsNamed.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c748e23",
   "metadata": {},
   "source": [
    "### 6.4 Make sure place name is unique by stripping smaller localities of duplicated names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first number should always be zero. \n",
    "# The second tells us whether/how many settlement polygons were duplicated by the Near join.\n",
    "\n",
    "print(len(Settlements[Settlements.duplicated('Sett_ID')]), \n",
    "      len(SettlementsNamed[SettlementsNamed.duplicated('Sett_ID')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bounded = Bounded.sort_values('Area2015', ascending=False).drop_duplicates(['WSFE_ID'], keep='first')\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba8cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833aef54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8e886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07979ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4024d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dupes = SettlementsNamed[ \n",
    "    (SettlementsNamed['SettName'] != 'UNK') & \n",
    "    (SettlementsNamed.duplicated('SettName')) ]\n",
    "\n",
    "Largest = Dupes.loc[Dupes.groupby([\"SettName\"])[\"Area2015\"].idxmax()]\n",
    "print(Largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73867e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed['SettName'].str.contains('UNK').value_counts()[False] # Count number of non-UNK settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to settlements which have a duplicated name and are not the largest of those with that name, then replace with UNK.\n",
    "SettlementsNamed.loc[(~SettlementsNamed.Sett_ID.isin(Largest.Sett_ID)) \n",
    "                     & (SettlementsNamed.Sett_ID.isin(Dupes.Sett_ID)), \n",
    "                     'SettName'] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bddc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed['SettName'].str.contains('UNK').value_counts()[False] # Count number of non-UNK settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SettlementsNamed.info(), SettlementsNamed[SettlementsNamed['SettName'] != \"UNK\"].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d202603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns and save to file.\n",
    "SettlementsNamed = SettlementsNamed[['Sett_ID', 'SettName']]\n",
    "SettlementsNamed.to_csv(r'Results/PlaceNames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettlementsNamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c3d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d7b6d",
   "metadata": {},
   "source": [
    "## 7. CREATE FRAGMENTATION INDEX\n",
    "We are determining what percentage of a settlement's area lies outside of its administrative zone each year.\n",
    "The index is a range of 0 to 100, i.e. the percent of the settlement area which is fragmented.\n",
    "\n",
    "For each Sett_ID:\n",
    "((Area of Boundless settlement - Area of largest Bounded settlement feature) / Area of Boundless settlement) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7537f",
   "metadata": {},
   "source": [
    "### 7.1 Load boundless and bounded cumulative settlements and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bdf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BoundlessAreas = pd.read_csv(os.path.join(ResultsFolder, ('Areas%sto%s.csv' % (StudyStart, StudyEnd))))\n",
    "print('Loaded Boundless dataset, whose settlements will be used as the index of the Fragmentation Index dataset. %s' \n",
    "      % time.ctime())\n",
    "print(BoundlessAreas.info())\n",
    "\n",
    "BoundedAreas = pd.read_csv(os.path.join(ResultsFolder, ('Areas%sto%s_%s.csv' % (StudyStart, StudyEnd, 'Bounded'))))\n",
    "print('Loaded Bounded dataset, which will factor into the fragmentation calculation. %s' % time.ctime())\n",
    "print(BoundedAreas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261495a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LargestFragments = BoundedAreas.loc[BoundedAreas.groupby([\"Sett_ID\"])[\"Area2015\"].idxmax()] \n",
    "print(LargestFragments.info())\n",
    "print(\"Filtered the Bounded dataset to only rows where latest year's area is largest for each Sett_ID. %s\" % time.ctime())\n",
    "LargestFragments.columns = LargestFragments.columns.str.replace('Area', 'Largest')\n",
    "LargestFragments = LargestFragments.drop(columns=['year', 'ADM_ID'])\n",
    "print(\"Renamed columns to avoid duplication during merge, and dropped unnecessary columns. %s\" % time.ctime())\n",
    "FragIndices = BoundlessAreas.merge(LargestFragments, how='left', on='Sett_ID')\n",
    "print(FragIndices.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a828bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del BoundlessAreas, BoundedAreas, LargestFragments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d4c7b",
   "metadata": {},
   "source": [
    "### 7.2 Merge and run fragmentation calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bcaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in AllStudyYears:\n",
    "    YY = str(item) # 4-digit year\n",
    "    AreaYY = ''.join([\"Area\", YY]) # The Boundless area variable name\n",
    "    LargestYY = ''.join(['Largest', YY]) # The Bounded largest area variable name\n",
    "    FragYY = ''.join([\"Frag\", YY]) # Name for the fragmentation index variable\n",
    "    print(\"Created names for Year %s's variables and temporary objects. %s\" % (item, time.ctime()))\n",
    "    \n",
    "    FragIndices[FragYY] = ((FragIndices[AreaYY] - FragIndices[LargestYY]) / FragIndices[AreaYY]) * 100\n",
    "    FragIndices[FragYY] = (FragIndices[FragYY].fillna(0).replace([np.inf, -np.inf], 0)).astype('int')\n",
    "    print(\"Calculated fragmentation index for year %s. %s\" % (item, time.ctime()))\n",
    "\n",
    "# Remove unnecessary columns.\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('Largest')]\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('Area')]\n",
    "\n",
    "print('Completed fragmentation index calculations for all years. %s' % time.ctime())\n",
    "print(FragIndices.info())\n",
    "print(FragIndices.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FragIndices = FragIndices.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y', 'year', 'ADM_ID'])\n",
    "FragIndices.to_csv(os.path.join(ResultsFolder, 'FragIndex%sto%s.csv' % (StudyStart, StudyEnd)))\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del FragIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36f039",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db18c",
   "metadata": {},
   "source": [
    "## 8. PREPARE YEARLY DATASETS: POPULATION\n",
    "Can use this as a template for other annualized rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd172",
   "metadata": {},
   "source": [
    "### 8.1 Reproject and reclassify with settlement buffer mask.\n",
    "Reclassify so that we only need to work with cells within X distance of settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189136b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjCRS = gdal.WarpOptions(dstSRS='ESRI:102022')\n",
    "AnnualizedSourceFiles = [i for i in os.listdir('Population/') if i.endswith('.tif')]\n",
    "\n",
    "with fiona.open(r'Results/Catchment.gpkg', mode=\"r\", layer=\"Buff2000m_2015\") as shapefile:\n",
    "    MaskGeom = [feature[\"geometry\"] for feature in shapefile] # Identify the bounding areas of the mask.\n",
    "# Mask_out = './LatestYearBuffer.tif'\n",
    "AnnualizedSourceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This codeblock changes each annual population raster's projection (gdal.Warp()), \n",
    "# then masks it to within a specified distance of the settlements (rasterio.mask.mask()).\n",
    "\n",
    "for YearFile in AnnualizedSourceFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    TempOutputName = \"Temp_\" + Year + \"_albers.tif\"\n",
    "    TempOutputPath = os.path.join(ProjectFolder, \"Population\", TempOutputName)\n",
    "    if exists(TempOutputPath):\n",
    "        pass\n",
    "    else:\n",
    "        # Reproject to same CRS as settlements.\n",
    "        Warp = gdal.Warp(TempOutputPath, # Where to store the warped raster\n",
    "                     InputRasterObject, # Which raster to warp\n",
    "                     format='GTiff', \n",
    "                     options=ProjCRS) # Reproject to Africa Albers Equal Area Conic\n",
    "        print('Finished gdal.Warp() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "        \n",
    "        Warp = None # Close the files\n",
    "        InputRasterObject = None\n",
    "\n",
    "        # Reclassify as nodata if outside settlement buffer zones.\n",
    "        with rasterio.open(TempOutputPath) as InputRasterObject:\n",
    "            MaskedOutputRaster, OutTransform = rasterio.mask.mask(\n",
    "                InputRasterObject, MaskGeom, crop=True) # Anything outside the mask is reclassed to the raster's NoData value.\n",
    "            OutMetaData = InputRasterObject.meta.copy()\n",
    "        print('Finished rasterio.mask.mask() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "        OutMetaData.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": MaskedOutputRaster.shape[1],\n",
    "                         \"width\": MaskedOutputRaster.shape[2],\n",
    "                         \"transform\": OutTransform})\n",
    "        FinalOutputPath = os.path.join(ProjectFolder, \"Population\", ''.join(['Masked_', Year, '.tif'])) # ''.join([r'Population/', 'Masked_', Year, '.tif']\n",
    "        with rasterio.open(FinalOutputPath, \"w\", **OutMetaData) as dest:\n",
    "            dest.write(MaskedOutputRaster)\n",
    "        print('Written to file. %s \\n' % time.ctime())\n",
    "    InputRasterObject = None\n",
    "    \n",
    "    try:  # Finally, remove the intermediate file from disk\n",
    "        os.remove(TempOutputPath)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed intermediate file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished all years in list. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('Population/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba45368",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnualizedSourceFiles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624d801",
   "metadata": {},
   "source": [
    "### 8.2 Raster values summarized by settlement.\n",
    "1. Convert each annualized raster to .xyz, \n",
    "2. then bring them to vector space and assign their Sett_ID,\n",
    "3. and finally, aggregate the value as appropriate to the settlement level and save table to file.\n",
    "\n",
    "XYZ is similar to .csv. Raster cell centers are stored as x and y, and their value is stored as z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddceeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoDataVal = -99999 \n",
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS')\n",
    "AllSummaries = pd.DataFrame(Settlements).drop(columns='geometry')\n",
    "\n",
    "AnnualizedMaskedFiles = [i for i in os.listdir('Population/') if i.startswith('Masked') and i.endswith('.tif')]\n",
    "AnnualizedMaskedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for YearFile in AnnualizedMaskedFiles:\n",
    "    \n",
    "### STEP 1: TIF TO XYZ ###\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    print('Loading data for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    XYZOutputPath = r'Population/{}'.format(\n",
    "        YearFile.replace('.tif', '.xyz')) # New file path will be the same as original, but .tif is replaced with .xyz\n",
    "    \n",
    "    # Create an .xyz version of the .tif\n",
    "    XYZ = gdal.Translate(XYZOutputPath, # Specify a destination path\n",
    "                         InputRasterObject, # Input is the masked .tif file\n",
    "                         format='XYZ', \n",
    "                         creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "    print('Finished gdal.Translate() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "#     # Remove the temporary masked tif file.\n",
    "#     try:  \n",
    "#         os.remove(InputRasterName)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     print('Removed (or skipped if error) intermediate tif file. %s \\n' % time.ctime())\n",
    "    \n",
    "    InputRasterObject = None\n",
    "    XYZ = None # Reload XYZ as a point geodataframe\n",
    "\n",
    "    \n",
    "### STEP 2: GENERATE GEODATAFRAME WITH SETT_ID FIELD ###\n",
    "    InputXYZName = ''.join(['Masked_', Year, '.xyz'])\n",
    "    InputXYZ = pd.read_table(os.path.join(ProjectFolder, 'Population', InputXYZName), delim_whitespace=True)\n",
    "    InputXYZ = InputXYZ.loc[InputXYZ['Z'] != NoDataVal] # Subset to only the features that have a raster value.\n",
    "    print('Loaded XYZ file as a pandas dataframe, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    ValObject = gpd.GeoDataFrame(InputXYZ,\n",
    "                                 geometry = gpd.points_from_xy(InputXYZ['X'], InputXYZ['Y']),\n",
    "                                 crs = 'ESRI:102022')\n",
    "    print('Created geodataframe from non-NoData points, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    del InputXYZ\n",
    "    \n",
    "    # Sjoin_nearest: No need to group by ADM this time. \n",
    "    ValObject_withID = gpd.sjoin_nearest(ValObject, \n",
    "                                    Settlements, \n",
    "                                    how='left') # No need for max_distance parameter this time. We've already narrowed down to nearby raster cells.\n",
    "    \n",
    "    print('\\nJoined settlement ID onto vectorized raster cells for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValObject_withID.sample(10))\n",
    "    del ValObject\n",
    "    \n",
    "    # We no longer need the spatial information of the raster values because we have their unique settlement ID.\n",
    "    ValObject_withID = pd.DataFrame(ValObject_withID).drop(columns='geometry')\n",
    "    \n",
    "    ValObject_withID.to_csv(''.join([r'Population/', 'Masked_', Year, '.csv']))\n",
    "    print('\\nExported as table, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    # Remove the temporary xyz file.\n",
    "    try:  \n",
    "        os.remove(os.path.join(ProjectFolder, 'Population', InputXYZName))\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed (or skipped if error) intermediate xyz file. %s \\n' % time.ctime())\n",
    "\n",
    "    \n",
    "\n",
    "### STEP 3: AGGREGATE BY SETTLEMENT AND MERGE ONTO SUMMARIES TABLE ###\n",
    "    VariableName = ''.join(['PopSum', Year])\n",
    "    \n",
    "    ValAggregated = ValObject_withID.groupby('Sett_ID', \n",
    "                                      as_index=False)['Z'].sum().rename(columns={\"Z\": VariableName})\n",
    "    print('\\nValues aggregated to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValAggregated.sample(10))\n",
    "    \n",
    "    AllSummaries = AllSummaries.merge(ValAggregated, how='left', on='Sett_ID')\n",
    "    print('\\nMerged year %s onto latest year settlement feature layer. %s \\n' % (Year, time.ctime()))\n",
    "    print(AllSummaries.sample(10))\n",
    "    \n",
    "    del ValObject_withID, ValAggregated\n",
    "    print('\\n\\n')\n",
    "    \n",
    "\n",
    "print('\\n\\nFinished. All years masked and assigned their nearest settlement. %s' % time.ctime())\n",
    "\n",
    "AllSummaries.to_csv(os.path.join(ResultsFolder, 'Pop%sto%s.csv' % (2000, 2015)))\n",
    "print('Saved to file. %s \\n' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllSummaries.sort_values('PopSum2010', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a37640",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49e4cf",
   "metadata": {},
   "source": [
    "## 9. PREPARE YEARLY DATASETS: NIGHTTIME LIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667dd97",
   "metadata": {},
   "source": [
    "### 9.1 Reproject and reclassify with settlement buffer mask.\n",
    "Reclassify so that we only need to work with cells within X distance of settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjCRS = gdal.WarpOptions(dstSRS='ESRI:102022')\n",
    "AnnualizedSourceFiles = [i for i in os.listdir('NTL/') if i.endswith('.tif')]\n",
    "\n",
    "with fiona.open(r'Results/Catchment.gpkg', mode=\"r\", layer=\"Buff250m_2015\") as shapefile:\n",
    "    MaskGeom = [feature[\"geometry\"] for feature in shapefile] # Identify the bounding areas of the mask.\n",
    "# Mask_out = './LatestYearBuffer.tif'\n",
    "AnnualizedSourceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ValStart = 1999\n",
    "ValEnd = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This codeblock changes each annual population raster's projection (gdal.Warp()), \n",
    "# then masks it to within a specified distance of the settlements (rasterio.mask.mask()).\n",
    "\n",
    "for YearFile in AnnualizedSourceFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"NTL\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    TempOutputName = \"Temp_\" + Year + \"_albers.tif\"\n",
    "    TempOutputPath = os.path.join(ProjectFolder, \"NTL\", TempOutputName)\n",
    "    if exists(TempOutputPath):\n",
    "        pass\n",
    "    else:\n",
    "        # Reproject to same CRS as settlements.\n",
    "        Warp = gdal.Warp(TempOutputPath, # Where to store the warped raster\n",
    "                     InputRasterObject, # Which raster to warp\n",
    "                     format='GTiff', \n",
    "                     options=ProjCRS) # Reproject to Africa Albers Equal Area Conic\n",
    "        print('Finished gdal.Warp() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "        \n",
    "        Warp = None # Close the files\n",
    "        InputRasterObject = None\n",
    "\n",
    "        # Reclassify as nodata if outside settlement buffer zones.\n",
    "        with rasterio.open(TempOutputPath) as InputRasterObject:\n",
    "            MaskedOutputRaster, OutTransform = rasterio.mask.mask(\n",
    "                InputRasterObject, MaskGeom, crop=True) # Anything outside the mask is reclassed to the raster's NoData value.\n",
    "            OutMetaData = InputRasterObject.meta.copy()\n",
    "        print('Finished rasterio.mask.mask() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "        OutMetaData.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": MaskedOutputRaster.shape[1],\n",
    "                         \"width\": MaskedOutputRaster.shape[2],\n",
    "                         \"transform\": OutTransform})\n",
    "        FinalOutputPath = os.path.join(ProjectFolder, \"NTL\", ''.join(['Masked_', Year, '.tif']))\n",
    "        with rasterio.open(FinalOutputPath, \"w\", **OutMetaData) as dest:\n",
    "            dest.write(MaskedOutputRaster)\n",
    "        print('Written to file. %s \\n' % time.ctime())\n",
    "    InputRasterObject = None\n",
    "    \n",
    "    try:  # Finally, remove the intermediate file from disk\n",
    "        os.remove(TempOutputPath)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed intermediate file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished all years in list. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('NTL/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnualizedSourceFiles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52640a6",
   "metadata": {},
   "source": [
    "### 9.2 Raster values summarized by settlement.\n",
    "1. Convert each annualized raster to .xyz, \n",
    "2. then bring them to vector space and assign their Sett_ID,\n",
    "3. and finally, aggregate the value as appropriate to the settlement level and save table to file.\n",
    "\n",
    "XYZ is similar to .csv. Raster cell centers are stored as x and y, and their value is stored as z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e920861",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoDataVal = 0\n",
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS')\n",
    "AllSummaries = pd.DataFrame(Settlements).drop(columns='geometry')\n",
    "\n",
    "AnnualizedMaskedFiles = [i for i in os.listdir('NTL/') if i.startswith('Masked') and i.endswith('.tif')]\n",
    "AnnualizedMaskedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fceb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for YearFile in AnnualizedMaskedFiles:\n",
    "    \n",
    "### STEP 1: TIF TO XYZ ###\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"NTL\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    print('Loading data for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    XYZOutputPath = r'NTL/{}'.format(\n",
    "        YearFile.replace('.tif', '.xyz')) # New file path will be the same as original, but .tif is replaced with .xyz\n",
    "    \n",
    "    # Create an .xyz version of the .tif\n",
    "    XYZ = gdal.Translate(XYZOutputPath, # Specify a destination path\n",
    "                         InputRasterObject, # Input is the masked .tif file\n",
    "                         format='XYZ', \n",
    "                         creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "    print('Finished gdal.Translate() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "#     # Remove the temporary masked tif file.\n",
    "#     try:  \n",
    "#         os.remove(InputRasterName)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     print('Removed (or skipped if error) intermediate tif file. %s \\n' % time.ctime())\n",
    "    \n",
    "    InputRasterObject = None\n",
    "    XYZ = None # Reload XYZ as a point geodataframe\n",
    "\n",
    "    \n",
    "### STEP 2: GENERATE GEODATAFRAME WITH SETT_ID FIELD ###\n",
    "    InputXYZName = ''.join(['Masked_', Year, '.xyz'])\n",
    "    InputXYZ = pd.read_table(os.path.join(ProjectFolder, 'NTL', InputXYZName), delim_whitespace=True)\n",
    "    InputXYZ = InputXYZ.loc[InputXYZ['Z'] >= 10] # Subset to only the cells that contained values of 10 or higher.\n",
    "    print('Loaded XYZ file as a pandas dataframe, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    ValObject = gpd.GeoDataFrame(InputXYZ,\n",
    "                                 geometry = gpd.points_from_xy(InputXYZ['X'], InputXYZ['Y']),\n",
    "                                 crs = 'ESRI:102022')\n",
    "    print('Created geodataframe from non-NoData points, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    del InputXYZ\n",
    "    \n",
    "    # Sjoin_nearest: No need to group by ADM this time. \n",
    "    ValObject_withID = gpd.sjoin_nearest(ValObject, \n",
    "                                    Settlements, \n",
    "                                    how='left') # No need for max_distance parameter this time. We've already narrowed down to nearby raster cells.\n",
    "    \n",
    "    print('\\nJoined settlement ID onto vectorized raster cells for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValObject_withID.sample(10))\n",
    "    del ValObject\n",
    "    \n",
    "    # We no longer need the spatial information of the raster values because we have their unique settlement ID.\n",
    "    ValObject_withID = pd.DataFrame(ValObject_withID).drop(columns='geometry')\n",
    "    \n",
    "    ValObject_withID.to_csv(''.join([r'NTL/', 'Masked_', Year, '.csv']))\n",
    "    print('\\nExported as table, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    # Remove the temporary xyz file.\n",
    "    try:  \n",
    "        os.remove(os.path.join(ProjectFolder, 'NTL', InputXYZName))\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed (or skipped if error) intermediate xyz file. %s \\n' % time.ctime())\n",
    "\n",
    "    \n",
    "\n",
    "### STEP 3: AGGREGATE BY SETTLEMENT AND MERGE ONTO SUMMARIES TABLE ###\n",
    "    \n",
    "    # Cell count\n",
    "    VariableName = ''.join(['NTLct', Year])\n",
    "    ValAggregated = ValObject_withID[\n",
    "        ValObject_withID['Z'].notna()].groupby(\n",
    "        'Sett_ID', as_index=False)['Z'].count().rename(columns={\"Z\": VariableName})\n",
    "    print('\\nCells per settlement counted, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValAggregated.sample(10))\n",
    "    AllSummaries = AllSummaries.merge(ValAggregated, how='left', on='Sett_ID')\n",
    "    \n",
    "    # Sum\n",
    "    VariableName = ''.join(['NTLsum', Year])\n",
    "    ValAggregated = ValObject_withID[\n",
    "        ValObject_withID['Z'].notna()].groupby(\n",
    "        'Sett_ID', as_index=False)['Z'].sum().rename(columns={\"Z\": VariableName})\n",
    "    print('\\nValues summed to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValAggregated.sample(10))\n",
    "    AllSummaries = AllSummaries.merge(ValAggregated, how='left', on='Sett_ID')\n",
    "    \n",
    "    # Average\n",
    "    VariableName = ''.join(['NTLavg', Year])\n",
    "    ValAggregated = ValObject_withID[\n",
    "        ValObject_withID['Z'].notna()].groupby(\n",
    "        'Sett_ID', as_index=False)['Z'].mean().rename(columns={\"Z\": VariableName})\n",
    "    print('\\nValues averaged to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValAggregated.sample(10))\n",
    "    AllSummaries = AllSummaries.merge(ValAggregated, how='left', on='Sett_ID')\n",
    "    print('\\nMerged year %s onto latest year settlement feature layer. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    \n",
    "    print(AllSummaries.sample(10))\n",
    "    del ValObject_withID, ValAggregated\n",
    "    \n",
    "    \n",
    "\n",
    "print('\\n\\nFinished. All years masked and assigned their nearest settlement. %s' % time.ctime())\n",
    "\n",
    "AllSummaries.to_csv(os.path.join(ResultsFolder, 'NTL%sto%s.csv' % (ValStart, ValEnd)))\n",
    "print('Saved to file. %s \\n' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19996d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllSummaries.sort_values('NTLsum2010', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3343240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df1869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
