{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2016b3",
   "metadata": {},
   "source": [
    "# Spatiotemporal Trends in Urbanization: Cameroon\n",
    "*Using yearly estimates (2000-2015) of population, built-area, and economic indicators to track city-by-city growth and change over time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd14a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903293ba",
   "metadata": {},
   "source": [
    "### Research questions \n",
    "\n",
    "#### 1. How has the size of Settlement X changed over time? \n",
    "\n",
    "- Population size \n",
    "\n",
    "- Geographical extents \n",
    "\n",
    "- Population density \n",
    "\n",
    "#### 2. In what year did Settlement X become a new urban class?  \n",
    "\n",
    "- From semi-dense to high-density city \n",
    "\n",
    "- Small settlement area to built-up area \n",
    "\n",
    "- When a hamlet area or small settlement area first appeared\n",
    "\n",
    "#### 3. Is there a discernable pattern between the spatio-temporal distribution of economic density and population density? \n",
    "\n",
    "#### 4. How much of urban space attributable to City X is outside of the administrative limits of the city? \n",
    "\n",
    "- When did this fragment(s) appear? \n",
    "\n",
    "- Which district/municipality/authority has purview over the fragment(s)? \n",
    "\n",
    "#### 5. For the questions above, how does the answer change based on different understandings of urban limits? \n",
    "\n",
    "- Scenario A: where \"city\" is delimited by an official administrative boundary \n",
    "\n",
    "- Scenario B: where \"city\" includes all contiguous (and near-contiguous) built up area \n",
    "\n",
    "#### 6. Subnational and inter-national comparisons. Examples: \n",
    "\n",
    "- Compare the rates (pop, build-up, economicâ€¦) of the fastest growing settlement of each ADM1 region. \n",
    "\n",
    "- Which African metropoles experience the most vs. the least fragmentation? Is there a confluence between amount of urban fragmentation and rate of densification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55d1a9",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. Most up-to-date administrative boundaries: **ADM3.**\n",
    "2. Built-up area, yearly: **World Settlement Footprint Evolution.** Resolution: 30m.\n",
    "3. Settlement types: **GRID3 settlement extents.** Captured between 2009-2019.\n",
    "4. Population, yearly: **WorldPop.** UN-adjusted, unconstrained. Resolution: 100m.\n",
    "5. Nighttime lights, yearly: **Harmonization of DMSP and VIIRS.** Resolution: 1km.\n",
    "6. City names: **UCDB, Africapolis, and GeoNames.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5dafb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a61167",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276b447",
   "metadata": {},
   "source": [
    "## 1. PREPARE WORKSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0190f8",
   "metadata": {},
   "source": [
    "### 1.1 Off-script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9093fd6",
   "metadata": {},
   "source": [
    "##### Off-script: Create folders in working directory.\n",
    "> *ADM\n",
    "<br>Buildup\n",
    "<br>PlaceName\n",
    "<br>Population\n",
    "<br>Settlement\n",
    "<br>NTL*\n",
    "\n",
    "##### Off-script: Download datasets (as shapefile, GeoJSON, or tif where possible) and place or extract into corresponding folder:\n",
    "- ADM: *Sourced internally.*\n",
    "- Buildup: https://download.geoservice.dlr.de/WSF_EVO/files/\n",
    "- PlaceName: \n",
    "    - GeoNames: (file: cities500.zip) https://download.geonames.org/export/dump/\n",
    "    - Africapolis: https://africapolis.org/en/data\n",
    "    - Urban Centres Database: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php\n",
    "- Population: https://hub.worldpop.org/geodata/listing?id=69\n",
    "- Settlement: https://data.grid3.org/datasets/GRID3::grid3-cameroon-settlement-extents-version-01-01-/explore\n",
    "- NTL: https://figshare.com/articles/dataset/Harmonization_of_DMSP_and_VIIRS_nighttime_light_data_from_1992-2018_at_the_global_scale/9828827/2\n",
    "\n",
    "##### Other off-script:\n",
    "- Convert GeoNames from .txt file to shape (delimiter = tab, header rows = 0) and rename fields.\n",
    "- If necessary, mosaic WSFE rasters that cover the area of interest to create a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2548",
   "metadata": {},
   "source": [
    "### 1.2 Load all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c42795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Most but not all of these packages were used in final form. \n",
    "\n",
    "import os, sys, glob, re, time\n",
    "from os.path import exists\n",
    "\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, Polygon, shape, MultiPoint\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.validation import make_valid, explain_validity\n",
    "import shapely.wkt\n",
    "import scipy\n",
    "\n",
    "#from xrspatial import zonal_stats \n",
    "#import xarray as xr \n",
    "import numpy as np \n",
    "import fiona, rioxarray\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio import features\n",
    "from rasterio.features import shapes\n",
    "from rasterio import mask\n",
    "from osgeo import gdal, osr, ogr, gdal_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f9d7",
   "metadata": {},
   "source": [
    "### 1.3 Set workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a20dfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\n",
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Results\n"
     ]
    }
   ],
   "source": [
    "ProjectFolder = os.getcwd()\n",
    "ResultsFolder = os.path.join(ProjectFolder, 'Results')\n",
    "print(ProjectFolder)\n",
    "print(ResultsFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537c1ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08550",
   "metadata": {},
   "source": [
    "## 2. PREPARE BUILDUP, SETTLEMENT, AND ADMIN DATASETS\n",
    "Projection for all datasets: Africa Albers Equal Area Conic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d194cf",
   "metadata": {},
   "source": [
    "### 2.1 WSFE: Check contents and change NoData value as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFE = rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "print(WSFE) # WSFE values are all 4 digits long (1985-2015)\n",
    "print(dir(WSFE))\n",
    "print(WSFE.crs)\n",
    "print(WSFE.dtypes)\n",
    "NoDataValue = WSFE.nodatavals\n",
    "print(NoDataValue)\n",
    "print(WSFE.read(1).min(), WSFE.read(1).mean(), np.median(WSFE.read(1)), WSFE.read(1).max())\n",
    "\n",
    "# If NoDataValue != 0, change to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa8859",
   "metadata": {},
   "source": [
    "##### Off-script: Run this block in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a191c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "# Change NoData value to zero, as this won't interfere with a possible value of 99999 in GRID3 and ADM.\n",
    "# Then make sure there are no values above 2015 (such as 99999) or below 1985 in the dataset by reclassifying them as NoData.\n",
    "# Was having trouble with rasterio & gdal here, so moved to QGIS.\n",
    "\n",
    "# processing.run(\"native:reclassifybytable\", {'INPUT_RASTER':'C:/Users/grace/GIS/povertyequity/urban_growth/WSFE_TCD.tif','RASTER_BAND':1,'TABLE':['2016','','0','','1984','0'],'NO_DATA':0,'RANGE_BOUNDARIES':0,'NODATA_FOR_MISSING':False,'DATA_TYPE':5,'OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/WSFE.tif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ea5f",
   "metadata": {},
   "source": [
    "### 2.2 Prepare raster locations for GRID3 and Admin areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7946e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_vec = gpd.read_file(glob.glob('ADM/*.shp')[0])[['geometry']].to_crs(\"ESRI:102022\") # This glob() function pulls the first file ([0]) in the ADM folder which ended in '.shp'\n",
    "GRID3_vec = gpd.read_file(glob.glob('Settlement/*.shp')[0])[['type','geometry']].to_crs(\"ESRI:102022\")\n",
    "ADM_vec['ADM_ID'] = ADM_vec.index\n",
    "GRID3_vec['G3_ID'] = GRID3_vec.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44280f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_out = './ADM/ADM.tif'\n",
    "GRID3_out = './Settlement/GRID3.tif'\n",
    "\n",
    "print(ADM_vec.info(), \"\\n\\n\", \n",
    "      ADM_vec.crs, \"\\n\\n\", \n",
    "      len(str(ADM_vec['ADM_ID'].max()))) # We need to know how many digits need to be allocated to each dataset in the \"join\" serial.\n",
    "print(GRID3_vec.info(), \"\\n\\n\", \n",
    "      GRID3_vec.crs, \"\\n\\n\", \n",
    "      len(str(GRID3_vec['G3_ID'].max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44be4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440aa2a2",
   "metadata": {},
   "source": [
    "## 3. WSFE AND ADM; GRID3 AND ADM\n",
    "RASTERIZE: Bring ADM and GRID3 into raster space.\n",
    "\n",
    "RASTER MATH: \"Join\" ADM ID onto GRID3 and onto WSFE by creating unique concatenation string.\n",
    "\n",
    "VECTORIZE: Bring joined data into vector space.\n",
    "\n",
    "VECTOR MATH: Split unique ID from raster math step into separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca195d70",
   "metadata": {},
   "source": [
    "### 3.1 Rasterize admin areas and GRID3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af504c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and update the metadata from WSFE for the output\n",
    "meta = WSFE.meta.copy()\n",
    "meta.update(compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f778fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ADM_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(ADM_vec.geometry, ADM_vec.ADM_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, all_touched=False)\n",
    "    out.write_band(1, burned)\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(GRID3_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(GRID3_vec.geometry, GRID3_vec.G3_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, all_touched=False)\n",
    "    out.write_band(1, burned)\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbc740",
   "metadata": {},
   "source": [
    "*Validation: Check the dimensions, type, and basic stats of the three datasets. All should be the same dimension and NoData value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckContents = gdal.Open(r\"ADM/ADM.tif\")\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "CheckContents =  gdal.Open(r\"Settlement/GRID3.tif\")\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "CheckContents = gdal.Open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "CheckContents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RastersList = [rasterio.open(r\"ADM/ADM.tif\"), \n",
    "               rasterio.open(r\"Settlement/GRID3.tif\"), \n",
    "               rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))]\n",
    "\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "\n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ceaa3",
   "metadata": {},
   "source": [
    "### 3.2 Raster math to \"join\" admin to GRID3 and to WSFE.\n",
    "Processing is more rapid when \"joining,\" i.e. creating serial codes out of two datasets, in raster rather than vector space.\n",
    "Here, we are concatenating the ID fields of the two datasets to create a serial number that we can then split in vector space later to create two ID fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a2406",
   "metadata": {},
   "source": [
    "*Adding together the values to create join IDs. This is in effect a concatenation of their ID strings, by way of summation. The number of zeros in the calc multiplication corresponds with number of digits of the maximum value in the \"B\" dataset. (e.g. Chad ADM codes go up 4 digits, so it's calc=(A*10000)+B).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e831cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN TERMINAL FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Gdal_calc.py # To see info.\n",
    "\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_CMN.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "\n",
    "# # END TERMINAL-ONLY ASPECT. RETURN HERE FOR NEXT STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: check the basic statistics of the resulting datasets.\n",
    "RastersList = [rasterio.open(r\"Buildup/WSFE_ADM.tif\"), \n",
    "               rasterio.open(r\"Settlement/GRID3_ADM.tif\")]\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "    \n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea3639",
   "metadata": {},
   "source": [
    "### 3.3 Vectorize \"joined\" layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5d9d4",
   "metadata": {},
   "source": [
    "##### Off-script: Run this block in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Due to dtype errors with both gdal and rasterio here, I decided to run the raster to polygon function in QGIS instead.\n",
    "# It is possible to run QGIS functions within a Jupyter Notebook, but I ran it within the GUI. Arc or R are other options.\n",
    "# Command line code here.\n",
    "\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.tif','BAND':1,'FIELD':'DN','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.shp'})\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.tif','BAND':1,'FIELD':'DN','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.shp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03937b47",
   "metadata": {},
   "source": [
    "### 3.4 Vector math to split raster strings into admin area, GRID3, and WSFE year assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb900b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 210151 entries, 0 to 210150\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   DN        210151 non-null  int64   \n",
      " 1   geometry  210151 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 3.2 MB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 584286 entries, 0 to 584285\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   DN        584286 non-null  int64   \n",
      " 1   geometry  584286 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 8.9 MB\n",
      "None \n",
      "\n",
      "                DN                                           geometry\n",
      "9868    183794257  POLYGON ((-1098165.459 1276979.817, -1098137.2...\n",
      "130798   71719143  POLYGON ((-1494360.099 672422.887, -1494331.85...\n",
      "69733   117481203  POLYGON ((-1349141.459 851390.454, -1349056.71...\n",
      "202997    1400005  POLYGON ((-1468146.736 382426.511, -1468033.74...\n",
      "151812   67710131  POLYGON ((-1487157.073 601480.789, -1487100.57...\n",
      "160164   74936062  POLYGON ((-1159348.803 556904.871, -1159292.30...\n",
      "123809   60751156  POLYGON ((-1556023.645 695121.830, -1555995.39...\n",
      "32860   171531267  POLYGON ((-986024.241 1184034.287, -985939.500...\n",
      "71508   136936211  POLYGON ((-1069240.369 846047.667, -1069183.87...\n",
      "134710   71154143  POLYGON ((-1501930.337 659840.465, -1501873.84... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]] \n",
      "\n",
      " None \n",
      "\n",
      "              DN                                           geometry\n",
      "271193  1985136  POLYGON ((-1516703.601 646752.217, -1516675.35...\n",
      "142577  2001299  POLYGON ((-1513878.885 698409.699, -1513850.63...\n",
      "242415  2003289  POLYGON ((-1532183.043 655382.874, -1532154.79...\n",
      "384451  2014174  POLYGON ((-1615031.958 539864.858, -1614975.46...\n",
      "299657  1998134  POLYGON ((-1541278.628 638058.333, -1541250.38...\n",
      "574074  2005034  POLYGON ((-1209374.520 321569.318, -1209346.27...\n",
      "195519  1991288  POLYGON ((-1540120.495 673244.855, -1540092.24...\n",
      "148212  2012058  POLYGON ((-1086810.101 694616.004, -1086781.85...\n",
      "515384  2010324  POLYGON ((-1397218.122 448468.420, -1397189.87...\n",
      "422280  1993347  POLYGON ((-1638251.122 485109.192, -1638222.87... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]]\n"
     ]
    }
   ],
   "source": [
    "# Load newly created vectorized datasets.\n",
    "GRID3_ADM = gpd.read_file(r\"Settlement/GRID3_ADM.shp\")\n",
    "WSFE_ADM = gpd.read_file(r\"Buildup/WSFE_ADM.shp\")\n",
    "print(GRID3_ADM.info(), \"\\n\\n\", GRID3_ADM.sample(10), \"\\n\\n\", GRID3_ADM.crs, \"\\n\\n\", \n",
    "      WSFE_ADM.info(), \"\\n\\n\", WSFE_ADM.sample(10), \"\\n\\n\", WSFE_ADM.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f41b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DN                                           geometry  \\\n",
      "8410    181666243  POLYGON ((-1169997.982 1283365.870, -1169913.2...   \n",
      "151305   66537131  POLYGON ((-1501478.382 604673.816, -1501393.64...   \n",
      "16445   179978243  POLYGON ((-1158868.602 1253427.293, -1158783.8...   \n",
      "181164   35849335  POLYGON ((-1276122.555 481030.969, -1276009.56...   \n",
      "14020   169809271  POLYGON ((-1032321.333 1263164.444, -1032151.8...   \n",
      "39433   158862239  POLYGON ((-1150648.679 1164243.844, -1150535.6...   \n",
      "209234    2132075  POLYGON ((-1431114.712 275191.396, -1431058.21...   \n",
      "124427   73925140  POLYGON ((-1466338.918 692877.227, -1466282.42...   \n",
      "2889    193386274  POLYGON ((-1039015.910 1406281.592, -1038959.4...   \n",
      "75777   118213206  POLYGON ((-1266377.285 827806.316, -1266349.03...   \n",
      "\n",
      "       gridstring  Sett_ID  ADM_ID  \n",
      "8410    181666243   181666     243  \n",
      "151305  066537131    66537     131  \n",
      "16445   179978243   179978     243  \n",
      "181164  035849335    35849     335  \n",
      "14020   169809271   169809     271  \n",
      "39433   158862239   158862     239  \n",
      "209234  002132075     2132      75  \n",
      "124427  073925140    73925     140  \n",
      "2889    193386274   193386     274  \n",
      "75777   118213206   118213     206                DN                                           geometry gridstring  \\\n",
      "583694  2007078  POLYGON ((-1417866.794 253883.475, -1417838.54...    2007078   \n",
      "548081  2005031  POLYGON ((-1450238.038 416443.310, -1450209.79...    2005031   \n",
      "272950  1985138  POLYGON ((-1493879.897 646341.234, -1493823.40...    1985138   \n",
      "51358   1985267  POLYGON ((-998735.462 1177363.706, -998707.215...    1985267   \n",
      "226417  2010289  POLYGON ((-1528708.643 661326.329, -1528680.39...    2010289   \n",
      "13262   2008277  POLYGON ((-1128389.918 1304262.807, -1128361.6...    2008277   \n",
      "397486  2009173  POLYGON ((-1641951.500 523615.197, -1641923.25...    2009173   \n",
      "170358  2009163  POLYGON ((-1574101.826 682476.179, -1574073.57...    2009163   \n",
      "238580  2000287  POLYGON ((-1542662.739 656742.281, -1542634.49...    2000287   \n",
      "213721  1985143  POLYGON ((-1504472.581 666194.905, -1504387.84...    1985143   \n",
      "\n",
      "        year  ADM_ID  \n",
      "583694  2007      78  \n",
      "548081  2005      31  \n",
      "272950  1985     138  \n",
      "51358   1985     267  \n",
      "226417  2010     289  \n",
      "13262   2008     277  \n",
      "397486  2009     173  \n",
      "170358  2009     163  \n",
      "238580  2000     287  \n",
      "213721  1985     143  \n"
     ]
    }
   ],
   "source": [
    "# Split serial back into separate dataset fields.\n",
    "# For Cameroon: WSFE and ADM: 4+3=7 digits. GRID3 and ADM: 6+3=9 digits.\n",
    "GRID3_ADM['gridstring'] = GRID3_ADM['DN'].astype(str).str.zfill(9)\n",
    "WSFE_ADM['gridstring'] = WSFE_ADM['DN'].astype(str).str.zfill(7)\n",
    "\n",
    "GRID3_ADM['Sett_ID'] = GRID3_ADM['gridstring'].str[:-3].astype(int) # Remove the last 4 digits to get the GRID3 portion.\n",
    "GRID3_ADM['ADM_ID'] = GRID3_ADM['gridstring'].str[-3:].astype(int) # Keep only the last 4 digits to get the ADM portion.\n",
    "WSFE_ADM['year'] = WSFE_ADM['gridstring'].str[:-3].astype(int)\n",
    "WSFE_ADM['ADM_ID'] = WSFE_ADM['gridstring'].str[-3:].astype(int)\n",
    "\n",
    "print(GRID3_ADM.sample(10), WSFE_ADM.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3127bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 205788 entries, 0 to 205787\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype   \n",
      "---  ------      --------------   -----   \n",
      " 0   Sett_ID     205788 non-null  int64   \n",
      " 1   ADM_ID      205788 non-null  int64   \n",
      " 2   geometry    205788 non-null  geometry\n",
      " 3   DN          205788 non-null  int64   \n",
      " 4   gridstring  205788 non-null  object  \n",
      "dtypes: geometry(1), int64(3), object(1)\n",
      "memory usage: 7.9+ MB\n",
      "None    Sett_ID  ADM_ID                                           geometry    DN  \\\n",
      "0        1     354  POLYGON ((-1571983.289 271555.771, -1571870.30...  1354   \n",
      "1        2     354  POLYGON ((-1572915.445 272093.211, -1572802.45...  2354   \n",
      "2        3     354  POLYGON ((-1571390.099 273768.760, -1571361.85...  3354   \n",
      "3        4     354  POLYGON ((-1575062.229 274274.586, -1574949.24...  4354   \n",
      "4        5     354  POLYGON ((-1573395.647 277214.700, -1573226.16...  5354   \n",
      "\n",
      "  gridstring  \n",
      "0  000001354  \n",
      "1  000002354  \n",
      "2  000003354  \n",
      "3  000004354  \n",
      "4  000005354  \n"
     ]
    }
   ],
   "source": [
    "# Dissolve any features that have the same G3 and ADM values so that we have a single unique feature per settlement.\n",
    "# Note: we do NOT want to dissolve the WSFE features. Distinct features for noncontiguous builtup areas of the same year is necessary to separate them in the Near tool step.\n",
    "GRID3_ADM = GRID3_ADM.dissolve(by=['Sett_ID', 'ADM_ID'], as_index=False)\n",
    "print(GRID3_ADM.info(), GRID3_ADM.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f172cebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: WSFE (584286, 5) and GRID3 (205788, 5)\n",
      "\n",
      "After: WSFE (584286, 5) and GRID3 (205788, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove features where year, settlement, or admin area = 0.\n",
    "# This was supposed to be resolved earlier with the gdal_calc NoDataValue parameter.\n",
    "\n",
    "print(\"Before: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))\n",
    "WSFE_ADM = WSFE_ADM.loc[(WSFE_ADM[\"year\"] != 0) & (WSFE_ADM[\"ADM_ID\"] != 0)] # Since we change the datatype to integer, no need to include all digits. Otherwise, it would need to be: != '0000'\n",
    "GRID3_ADM = GRID3_ADM.loc[(GRID3_ADM[\"Sett_ID\"] != 0) & (GRID3_ADM[\"ADM_ID\"] != 0)]\n",
    "print(\"After: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bounded_ID is our new unique settlement identifier for subsequent matching steps.\n",
    "GRID3_ADM['Bounded_ID'] = GRID3_ADM.index\n",
    "WSFE_ADM['WSFE_ID'] = WSFE_ADM.index\n",
    "GRID3_ADM = GRID3_ADM[['Sett_ID', 'Bounded_ID', 'ADM_ID', 'geometry']]\n",
    "WSFE_ADM = WSFE_ADM[['year', 'ADM_ID', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd7f5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f0a8e",
   "metadata": {},
   "source": [
    "## 4. UNIQUE SETTLEMENTS FROM WSFE AND GRID3: TWO VERSIONS\n",
    "\n",
    "Note that there are 2 versions here, so that we can create a fragmentation index:\n",
    "1. **Boundless, aka boundary-agnostic settlements**: Unique settlements are linked to GRID3 settlement IDs. Administrative areas do not influence the extents of the settlement.\n",
    "2. **Bounded, aka politically-defined settlements**: Settlements in the Boundless dataset which spread across more than one administrative area are split into separate settlements in the Bounded dataset. The largest polygon after the split is considered the \"principal\" settlement, and polygons in other admin areas are considered \"fragments.\" By dividing the fragment area(s) of the Bounded settlement by the area of the Boundless settlement, we can acquire a fragmentation index for each locality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6624",
   "metadata": {},
   "source": [
    "### 4.1 BOUNDED SETTLEMENTS: Near Join by ADM group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b111165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# The sharding step below doesn't work if any ADM group contains features from only one of the two datasets.\n",
    "WSFE_u = sorted(WSFE_ADM.ADM_ID.unique().tolist())\n",
    "GRID3_u = sorted(GRID3_ADM.ADM_ID.unique().tolist())\n",
    "\n",
    "not_matching = list(set(GRID3_u).symmetric_difference(set(WSFE_u)))\n",
    "print(not_matching) # Validate: If there are many ADM_IDs in this list, investigate why GRID3 or WSFE is missing in so many areas.\n",
    "\n",
    "# Take only the features that share an ADM with at least one GRID3 feature.\n",
    "WSFE_matching = WSFE_ADM[~WSFE_ADM[\"ADM_ID\"].isin(not_matching)] \n",
    "GRID3_matching = GRID3_ADM[~GRID3_ADM[\"ADM_ID\"].isin(not_matching)]\n",
    "\n",
    "WSFE_u = sorted(WSFE_matching.ADM_ID.unique().tolist())\n",
    "GRID3_u = sorted(GRID3_matching.ADM_ID.unique().tolist())\n",
    "\n",
    "not_matching = list(set(GRID3_u).symmetric_difference(set(WSFE_u)))\n",
    "print(not_matching) # This should now be empty.\n",
    "\n",
    "del WSFE_u, GRID3_u, not_matching, WSFE_ADM, GRID3_ADM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0979d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "MultiIndex: 586113 entries, (0, 560868) to (356, 3948)\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   year          586113 non-null  int32   \n",
      " 1   ADM_ID_left   586113 non-null  int32   \n",
      " 2   geometry      586113 non-null  geometry\n",
      " 3   index_right   581895 non-null  float64 \n",
      " 4   Sett_ID       581895 non-null  float64 \n",
      " 5   Bounded_ID    581895 non-null  float64 \n",
      " 6   ADM_ID_right  581895 non-null  float64 \n",
      "dtypes: float64(4), geometry(1), int32(2)\n",
      "memory usage: 50.8 MB\n",
      "None\n",
      "            year  ADM_ID_left  \\\n",
      "338 493162  1995          341   \n",
      "286 229845  2006          289   \n",
      "344 418376  1999          347   \n",
      "118 352670  2015          120   \n",
      "311 305584  1985          314   \n",
      "177 344591  2007          179   \n",
      "348 460159  2005          351   \n",
      "76  582967  2012           78   \n",
      "169 394489  2006          171   \n",
      "174 369227  2010          176   \n",
      "\n",
      "                                                     geometry  index_right  \\\n",
      "338 493162  POLYGON ((-1395410.304 456656.478, -1395382.05...      19750.0   \n",
      "286 229845  POLYGON ((-1526138.152 660219.835, -1526109.90...      57834.0   \n",
      "344 418376  POLYGON ((-1634240.026 487037.653, -1634211.77...       8126.0   \n",
      "118 352670  POLYGON ((-1350327.839 590795.214, -1350299.59...      29356.0   \n",
      "311 305584  POLYGON ((-1527183.296 634233.023, -1527126.80...      50512.0   \n",
      "177 344591  POLYGON ((-1593281.646 600974.963, -1593253.39...      50269.0   \n",
      "348 460159  POLYGON ((-1577434.991 470124.096, -1577378.49...      13021.0   \n",
      "76  582967  POLYGON ((-1417584.323 255906.779, -1417556.07...       1605.0   \n",
      "169 394489  POLYGON ((-1633110.139 527535.348, -1633053.64...      46510.0   \n",
      "174 369227  POLYGON ((-1611642.299 557221.013, -1611614.05...      46611.0   \n",
      "\n",
      "            Sett_ID  Bounded_ID  ADM_ID_right  \n",
      "338 493162  19120.0     19750.0         341.0  \n",
      "286 229845  56244.0     57834.0         289.0  \n",
      "344 418376   7750.0      8126.0         347.0  \n",
      "118 352670  28421.0     29356.0         120.0  \n",
      "311 305584  49133.0     50512.0         314.0  \n",
      "177 344591  48906.0     50269.0         179.0  \n",
      "348 460159  12521.0     13021.0         351.0  \n",
      "76  582967   1707.0      1605.0          78.0  \n",
      "169 394489  45220.0     46510.0         171.0  \n",
      "174 369227  45306.0     46611.0         176.0  \n"
     ]
    }
   ],
   "source": [
    "# Shard the dataframe whose variables we want to join into a dict\n",
    "shards = {k:d for k, d in GRID3_matching.groupby('ADM_ID', as_index=False)}\n",
    "\n",
    "# Take the dataframe whose geometry we want to retain.\n",
    "# Group by ADM, then sjoin_nearest among the smaller dataframe's matching ADM shard\n",
    "Bounded = WSFE_matching.groupby('ADM_ID', as_index=False).apply(\n",
    "    lambda d: gpd.sjoin_nearest(\n",
    "    d, shards[d['ADM_ID'].values[0]], \n",
    "        how='left', \n",
    "        max_distance=500))\n",
    "\n",
    "print(Bounded.info())\n",
    "print(Bounded.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d22854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 86839 entries, 0 to 86838\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   year          86839 non-null  int64   \n",
      " 1   Bounded_ID    86839 non-null  float64 \n",
      " 2   geometry      86839 non-null  geometry\n",
      " 3   ADM_ID_left   86839 non-null  int32   \n",
      " 4   index_right   86839 non-null  float64 \n",
      " 5   Sett_ID       86839 non-null  float64 \n",
      " 6   ADM_ID_right  86839 non-null  float64 \n",
      "dtypes: float64(4), geometry(1), int32(1), int64(1)\n",
      "memory usage: 4.3 MB\n",
      "None        year  Bounded_ID                                           geometry  \\\n",
      "11200  1985    191769.0  MULTIPOLYGON (((-1138897.861 1310016.578, -113...   \n",
      "7834   1985     61487.0  POLYGON ((-1540628.944 682539.408, -1540600.69...   \n",
      "19188  1996    159211.0  POLYGON ((-1217735.679 1185867.906, -1217707.4...   \n",
      "81407  2014    179149.0  MULTIPOLYGON (((-1152851.957 1282006.463, -115...   \n",
      "78018  2014      5782.0  POLYGON ((-1337899.090 379897.381, -1337870.84...   \n",
      "31195  2001      8146.0  MULTIPOLYGON (((-1626698.034 473854.563, -1626...   \n",
      "61418  2010      8127.0  MULTIPOLYGON (((-1627545.449 495320.554, -1627...   \n",
      "58496  2009     33201.0  MULTIPOLYGON (((-1241406.798 564270.963, -1241...   \n",
      "27970  2000     13553.0  MULTIPOLYGON (((-1574836.252 597054.811, -1574...   \n",
      "82035  2015        93.0  MULTIPOLYGON (((-1487947.994 378443.131, -1487...   \n",
      "\n",
      "       ADM_ID_left  index_right   Sett_ID  ADM_ID_right  \n",
      "11200          278     191769.0  188127.0         278.0  \n",
      "7834           155      61487.0   59797.0         155.0  \n",
      "19188          240     159211.0  156296.0         240.0  \n",
      "81407          245     179149.0  175813.0         245.0  \n",
      "78018           72       5782.0    5621.0          72.0  \n",
      "31195          172       8146.0    7769.0         172.0  \n",
      "61418          171       8127.0    7751.0         171.0  \n",
      "58496           90      33201.0   32254.0          90.0  \n",
      "27970          125      13553.0   13034.0         125.0  \n",
      "82035            5         93.0     106.0           5.0  \n"
     ]
    }
   ],
   "source": [
    "# Now we can dissolve with the WSFE years, now that we can group them by their settlement ID.\n",
    "Bounded = Bounded.dissolve(by=['year', 'Bounded_ID'], as_index=False)\n",
    "print(Bounded.info(), Bounded.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02684aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Bounded = Bounded.rename(\n",
    "    columns={\"ADM_ID_left\": \"ADM_ID\"})[['ADM_ID', 'year', 'Bounded_ID', 'Sett_ID', 'geometry']]\n",
    "Bounded.to_file(\n",
    "    driver='GPKG', filename=r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Bounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b17381",
   "metadata": {},
   "outputs": [],
   "source": [
    "del WSFE_matching, GRID3_matching, shards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eadab",
   "metadata": {},
   "source": [
    "### 4.2 BOUNDLESS SETTLEMENTS: Simple near join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2638d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 83061 entries, 0 to 83060\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   year        83061 non-null  int64   \n",
      " 1   Sett_ID     83061 non-null  float64 \n",
      " 2   geometry    83061 non-null  geometry\n",
      " 3   ADM_ID      83061 non-null  int32   \n",
      " 4   Bounded_ID  83061 non-null  float64 \n",
      "dtypes: float64(2), geometry(1), int32(1), int64(1)\n",
      "memory usage: 2.9 MB\n",
      "None        year   Sett_ID                                           geometry  \\\n",
      "80183  2015   33901.0  MULTIPOLYGON (((-1338944.235 411511.506, -1338...   \n",
      "26068  2000    6321.0  POLYGON ((-1001955.638 252207.926, -1001927.39...   \n",
      "67007  2012   37880.0  POLYGON ((-1135310.472 389729.374, -1135282.22...   \n",
      "37216  2003   63066.0  MULTIPOLYGON (((-1422216.857 535027.896, -1422...   \n",
      "29526  2001    7283.0  POLYGON ((-1083392.195 404366.714, -1083335.70...   \n",
      "82500  2015  175813.0  MULTIPOLYGON (((-1152908.451 1282069.691, -115...   \n",
      "49836  2007   68735.0  MULTIPOLYGON (((-1477214.074 660346.291, -1477...   \n",
      "8621   1985   71536.0  POLYGON ((-1498258.206 667269.785, -1498229.95...   \n",
      "45444  2006   21060.0  MULTIPOLYGON (((-1405099.079 466298.787, -1405...   \n",
      "48358  2007    9318.0  POLYGON ((-1553735.625 407211.985, -1553707.37...   \n",
      "\n",
      "       ADM_ID  Bounded_ID  \n",
      "80183      73     34863.0  \n",
      "26068      36      6541.0  \n",
      "67007      42     39113.0  \n",
      "37216     105     64836.0  \n",
      "29526      38      7537.0  \n",
      "82500     245    179149.0  \n",
      "49836     356     70666.0  \n",
      "8621      143     73500.0  \n",
      "45444     295     21802.0  \n",
      "48358      13      9754.0  \n"
     ]
    }
   ],
   "source": [
    "# Fragments of any bounded settlement will be combined into a single \"boundless\" settlement in this version.\n",
    "# It is based on their \"Sett_ID\", which is a direct loan from the GRID3 settlement features.\n",
    "Boundless = Bounded.dissolve(by=['year', 'Sett_ID'], as_index=False)\n",
    "print(Boundless.info(), Boundless.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76e2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Boundless.to_file(driver='GPKG', filename=r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Boundless')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba12eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027eba",
   "metadata": {},
   "source": [
    "## 5. CUMULATIVE ANNUALIZED SETTLEMENT EXTENTS\n",
    "DISSOLVE BY YEAR SETS: Create separate feature layers of each cumulative year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d70322",
   "metadata": {},
   "source": [
    "### 5.1 Define study years for each for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c3aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015] [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n"
     ]
    }
   ],
   "source": [
    "# Boundless = gpd.read_file(r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Boundless')\n",
    "\n",
    "def CreateList(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]\n",
    "\n",
    "CuStart, CuEnd = Boundless['year'].min(), Boundless['year'].max()\n",
    "StudyStart, StudyEnd = 1999, Boundless['year'].max()\n",
    "\n",
    "AllCuYears = CreateList(CuStart, CuEnd) # All years in the WSFE dataset\n",
    "AllStudyYears = CreateList(StudyStart, StudyEnd) # All years for which there will be growth stats in the present study.\n",
    "print(AllCuYears, '\\n\\n', AllStudyYears)\n",
    "\n",
    "ReversedStudyYears = []\n",
    "for i in AllStudyYears:\n",
    "    ReversedStudyYears.insert(0,i)\n",
    "ReversedStudyYears.remove(StudyEnd)\n",
    "print('\\n\\n', ReversedStudyYears)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de5331",
   "metadata": {},
   "source": [
    "### 5.2 Starting with main Boundless dataset, create a cumulative area feature layer for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c1be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to cumulative area for year: 1999. Thu Nov 17 14:10:45 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:10:45 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:11:10 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2000. Thu Nov 17 14:11:17 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:11:17 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:11:44 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2001. Thu Nov 17 14:11:51 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:11:51 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:12:20 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2002. Thu Nov 17 14:12:27 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:12:27 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:12:56 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2003. Thu Nov 17 14:13:03 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:13:03 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:13:35 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2004. Thu Nov 17 14:13:41 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:13:41 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:14:13 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2005. Thu Nov 17 14:14:20 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:14:20 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:14:55 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2006. Thu Nov 17 14:15:02 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:15:02 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:15:38 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2007. Thu Nov 17 14:15:44 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:15:44 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:16:21 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2008. Thu Nov 17 14:16:28 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:16:28 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:17:05 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2009. Thu Nov 17 14:17:12 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:17:12 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:17:50 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2010. Thu Nov 17 14:17:56 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:17:56 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:18:36 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2011. Thu Nov 17 14:18:42 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:18:43 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:19:25 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2012. Thu Nov 17 14:19:31 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:19:31 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:20:14 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2013. Thu Nov 17 14:20:21 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:20:21 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:21:03 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2014. Thu Nov 17 14:21:09 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:21:09 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:21:55 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2015. Thu Nov 17 14:22:01 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Thu Nov 17 14:22:01 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_48528\\4228306981.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements[Settlements['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Thu Nov 17 14:22:48 2022\n",
      "\n",
      "Done with all years in set. Thu Nov 17 14:22:54 2022\n"
     ]
    }
   ],
   "source": [
    "# For each year in the growth stats study, we are taking features from all years prior to and including that year, \n",
    "# dissolving those features, and exporting as its own file.\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYearSet = Boundless[Boundless['year'].between(\n",
    "        CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    CuYearDissolve = CuYearSet.dissolve(by='Sett_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\"}, # Though ADM_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    CuYearName = ''.join(['Cu', str(item), '_Boundless'])\n",
    "    CuYearDissolve.to_file(driver='GPKG', filename=r'Results/CumulativeSettlements.gpkg', layer=CuYearName)\n",
    "    del CuYearSet, CuYearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9568709a",
   "metadata": {},
   "source": [
    "##### Join area information from each cumulative layer onto the latest year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b33dc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cumulative layer for year 2014. Thu Nov 17 14:22:55 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:22:57 2022\n",
      "\n",
      "Merging variables from 2014 onto our latest year (2015) via table join. Thu Nov 17 14:22:57 2022\n",
      "\n",
      "Loading cumulative layer for year 2013. Thu Nov 17 14:22:57 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:22:58 2022\n",
      "\n",
      "Merging variables from 2013 onto our latest year (2015) via table join. Thu Nov 17 14:22:58 2022\n",
      "\n",
      "Loading cumulative layer for year 2012. Thu Nov 17 14:22:58 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:22:59 2022\n",
      "\n",
      "Merging variables from 2012 onto our latest year (2015) via table join. Thu Nov 17 14:22:59 2022\n",
      "\n",
      "Loading cumulative layer for year 2011. Thu Nov 17 14:22:59 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:00 2022\n",
      "\n",
      "Merging variables from 2011 onto our latest year (2015) via table join. Thu Nov 17 14:23:00 2022\n",
      "\n",
      "Loading cumulative layer for year 2010. Thu Nov 17 14:23:00 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:02 2022\n",
      "\n",
      "Merging variables from 2010 onto our latest year (2015) via table join. Thu Nov 17 14:23:02 2022\n",
      "\n",
      "Loading cumulative layer for year 2009. Thu Nov 17 14:23:02 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:03 2022\n",
      "\n",
      "Merging variables from 2009 onto our latest year (2015) via table join. Thu Nov 17 14:23:03 2022\n",
      "\n",
      "Loading cumulative layer for year 2008. Thu Nov 17 14:23:03 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:04 2022\n",
      "\n",
      "Merging variables from 2008 onto our latest year (2015) via table join. Thu Nov 17 14:23:04 2022\n",
      "\n",
      "Loading cumulative layer for year 2007. Thu Nov 17 14:23:04 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:06 2022\n",
      "\n",
      "Merging variables from 2007 onto our latest year (2015) via table join. Thu Nov 17 14:23:06 2022\n",
      "\n",
      "Loading cumulative layer for year 2006. Thu Nov 17 14:23:06 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:07 2022\n",
      "\n",
      "Merging variables from 2006 onto our latest year (2015) via table join. Thu Nov 17 14:23:07 2022\n",
      "\n",
      "Loading cumulative layer for year 2005. Thu Nov 17 14:23:07 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:08 2022\n",
      "\n",
      "Merging variables from 2005 onto our latest year (2015) via table join. Thu Nov 17 14:23:08 2022\n",
      "\n",
      "Loading cumulative layer for year 2004. Thu Nov 17 14:23:08 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:10 2022\n",
      "\n",
      "Merging variables from 2004 onto our latest year (2015) via table join. Thu Nov 17 14:23:10 2022\n",
      "\n",
      "Loading cumulative layer for year 2003. Thu Nov 17 14:23:10 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:11 2022\n",
      "\n",
      "Merging variables from 2003 onto our latest year (2015) via table join. Thu Nov 17 14:23:11 2022\n",
      "\n",
      "Loading cumulative layer for year 2002. Thu Nov 17 14:23:11 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:12 2022\n",
      "\n",
      "Merging variables from 2002 onto our latest year (2015) via table join. Thu Nov 17 14:23:12 2022\n",
      "\n",
      "Loading cumulative layer for year 2001. Thu Nov 17 14:23:12 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:14 2022\n",
      "\n",
      "Merging variables from 2001 onto our latest year (2015) via table join. Thu Nov 17 14:23:14 2022\n",
      "\n",
      "Loading cumulative layer for year 2000. Thu Nov 17 14:23:14 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:15 2022\n",
      "\n",
      "Merging variables from 2000 onto our latest year (2015) via table join. Thu Nov 17 14:23:15 2022\n",
      "\n",
      "Loading cumulative layer for year 1999. Thu Nov 17 14:23:15 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Thu Nov 17 14:23:16 2022\n",
      "\n",
      "Merging variables from 1999 onto our latest year (2015) via table join. Thu Nov 17 14:23:16 2022\n",
      "\n",
      "Done merging annualized areas onto latest year geometries. Saving to file. Thu Nov 17 14:23:16 2022\n",
      "\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 13753 entries, 0 to 13752\n",
      "Data columns (total 21 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Sett_ID   13753 non-null  float64 \n",
      " 1   year      13753 non-null  int64   \n",
      " 2   ADM_ID    13753 non-null  int64   \n",
      " 3   geometry  13753 non-null  geometry\n",
      " 4   Area15    13753 non-null  float64 \n",
      " 5   Area14    13649 non-null  float64 \n",
      " 6   Area13    13511 non-null  float64 \n",
      " 7   Area12    13388 non-null  float64 \n",
      " 8   Area11    13262 non-null  float64 \n",
      " 9   Area10    13164 non-null  float64 \n",
      " 10  Area09    13059 non-null  float64 \n",
      " 11  Area08    12992 non-null  float64 \n",
      " 12  Area07    12893 non-null  float64 \n",
      " 13  Area06    12801 non-null  float64 \n",
      " 14  Area05    12714 non-null  float64 \n",
      " 15  Area04    12632 non-null  float64 \n",
      " 16  Area03    12569 non-null  float64 \n",
      " 17  Area02    12494 non-null  float64 \n",
      " 18  Area01    12415 non-null  float64 \n",
      " 19  Area00    12320 non-null  float64 \n",
      " 20  Area99    12173 non-null  float64 \n",
      "dtypes: float64(18), geometry(1), int64(2)\n",
      "memory usage: 2.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The latest year in the study contains all settlements. Merge all other years' areas onto this dataset.\n",
    "SettAreas = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=\n",
    "                          ''.join(['Cu', str(StudyEnd), '_Boundless'])) \n",
    "SettAreas['Area15'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry') # We have settlement IDs, so no need to join spatially!\n",
    "\n",
    "\n",
    "for item in ReversedStudyYears:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=''.join(['Cu', str(item), '_Boundless']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['Area', str(item)[2:]])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Sett_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, StudyEnd, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Sett_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(ResultsFolder, 'Areas%sto%s.csv' % (StudyStart, StudyEnd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c522c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cafcd",
   "metadata": {},
   "source": [
    "### 5.3 Repeat for Bounded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1dcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to cumulative area for year: 1999. Sat Nov 19 15:42:00 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:42:00 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:42:53 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2000. Sat Nov 19 15:43:13 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:43:13 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:44:07 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2001. Sat Nov 19 15:44:25 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:44:25 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:45:25 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2002. Sat Nov 19 15:45:43 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:45:43 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:46:50 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2003. Sat Nov 19 15:47:09 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:47:09 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:48:11 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2004. Sat Nov 19 15:48:25 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:48:25 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:49:32 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2005. Sat Nov 19 15:49:50 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:49:50 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:51:01 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2006. Sat Nov 19 15:51:20 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:51:20 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:52:28 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2007. Sat Nov 19 15:52:44 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:52:44 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:54:02 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2008. Sat Nov 19 15:54:20 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:54:20 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:55:35 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2009. Sat Nov 19 15:55:49 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:55:49 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:57:03 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2010. Sat Nov 19 15:57:18 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:57:18 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 15:58:42 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2011. Sat Nov 19 15:58:57 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 15:58:57 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 16:00:23 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2012. Sat Nov 19 16:00:41 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 16:00:41 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 16:02:15 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2013. Sat Nov 19 16:02:30 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 16:02:30 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 16:03:51 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2014. Sat Nov 19 16:04:06 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 16:04:06 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 16:05:29 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2015. Sat Nov 19 16:05:44 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. Sat Nov 19 16:05:44 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_12396\\3209638953.py:5: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = Settlements_Bounded[Settlements_Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sat Nov 19 16:08:00 2022\n",
      "\n",
      "Done with all years in set. Sat Nov 19 16:08:24 2022\n"
     ]
    }
   ],
   "source": [
    "# Bounded = gpd.read_file(r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Bounded')\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYearSet = Bounded[Bounded['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    CuYearDissolve = CuYearSet.dissolve(by='Bounded_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\", \"Sett_ID\":\"min\"}, # Though ADM_ID and Sett_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    CuYearName = ''.join(['Cu', str(item), '_Bounded'])\n",
    "    CuYearDissolve.to_file(driver='GPKG', filename=r'Results/CumulativeSettlements.gpkg', layer=CuYearName)\n",
    "    del CuYearSet, CuYearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49eaeaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cumulative layer for year 2014. Sat Nov 19 16:08:30 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:08:35 2022\n",
      "\n",
      "Merging variables from 2014 onto our latest year (2015) via table join. Sat Nov 19 16:08:36 2022\n",
      "\n",
      "Loading cumulative layer for year 2013. Sat Nov 19 16:08:36 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:08:41 2022\n",
      "\n",
      "Merging variables from 2013 onto our latest year (2015) via table join. Sat Nov 19 16:08:41 2022\n",
      "\n",
      "Loading cumulative layer for year 2012. Sat Nov 19 16:08:41 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:08:46 2022\n",
      "\n",
      "Merging variables from 2012 onto our latest year (2015) via table join. Sat Nov 19 16:08:46 2022\n",
      "\n",
      "Loading cumulative layer for year 2011. Sat Nov 19 16:08:46 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:08:51 2022\n",
      "\n",
      "Merging variables from 2011 onto our latest year (2015) via table join. Sat Nov 19 16:08:51 2022\n",
      "\n",
      "Loading cumulative layer for year 2010. Sat Nov 19 16:08:51 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:08:56 2022\n",
      "\n",
      "Merging variables from 2010 onto our latest year (2015) via table join. Sat Nov 19 16:08:56 2022\n",
      "\n",
      "Loading cumulative layer for year 2009. Sat Nov 19 16:08:56 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:01 2022\n",
      "\n",
      "Merging variables from 2009 onto our latest year (2015) via table join. Sat Nov 19 16:09:01 2022\n",
      "\n",
      "Loading cumulative layer for year 2008. Sat Nov 19 16:09:01 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:06 2022\n",
      "\n",
      "Merging variables from 2008 onto our latest year (2015) via table join. Sat Nov 19 16:09:06 2022\n",
      "\n",
      "Loading cumulative layer for year 2007. Sat Nov 19 16:09:06 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:12 2022\n",
      "\n",
      "Merging variables from 2007 onto our latest year (2015) via table join. Sat Nov 19 16:09:12 2022\n",
      "\n",
      "Loading cumulative layer for year 2006. Sat Nov 19 16:09:12 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:17 2022\n",
      "\n",
      "Merging variables from 2006 onto our latest year (2015) via table join. Sat Nov 19 16:09:17 2022\n",
      "\n",
      "Loading cumulative layer for year 2005. Sat Nov 19 16:09:17 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:23 2022\n",
      "\n",
      "Merging variables from 2005 onto our latest year (2015) via table join. Sat Nov 19 16:09:23 2022\n",
      "\n",
      "Loading cumulative layer for year 2004. Sat Nov 19 16:09:23 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:29 2022\n",
      "\n",
      "Merging variables from 2004 onto our latest year (2015) via table join. Sat Nov 19 16:09:29 2022\n",
      "\n",
      "Loading cumulative layer for year 2003. Sat Nov 19 16:09:29 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:35 2022\n",
      "\n",
      "Merging variables from 2003 onto our latest year (2015) via table join. Sat Nov 19 16:09:35 2022\n",
      "\n",
      "Loading cumulative layer for year 2002. Sat Nov 19 16:09:35 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:41 2022\n",
      "\n",
      "Merging variables from 2002 onto our latest year (2015) via table join. Sat Nov 19 16:09:41 2022\n",
      "\n",
      "Loading cumulative layer for year 2001. Sat Nov 19 16:09:41 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:47 2022\n",
      "\n",
      "Merging variables from 2001 onto our latest year (2015) via table join. Sat Nov 19 16:09:47 2022\n",
      "\n",
      "Loading cumulative layer for year 2000. Sat Nov 19 16:09:47 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:09:53 2022\n",
      "\n",
      "Merging variables from 2000 onto our latest year (2015) via table join. Sat Nov 19 16:09:53 2022\n",
      "\n",
      "Loading cumulative layer for year 1999. Sat Nov 19 16:09:53 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sat Nov 19 16:10:00 2022\n",
      "\n",
      "Merging variables from 1999 onto our latest year (2015) via table join. Sat Nov 19 16:10:00 2022\n",
      "\n",
      "Done merging annualized areas onto latest year geometries. Saving to file. Sat Nov 19 16:10:00 2022\n",
      "\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 14468 entries, 0 to 14467\n",
      "Data columns (total 22 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   Bounded_ID  14468 non-null  float64 \n",
      " 1   year        14468 non-null  int64   \n",
      " 2   ADM_ID      14468 non-null  int64   \n",
      " 3   Sett_ID     14468 non-null  float64 \n",
      " 4   geometry    14468 non-null  geometry\n",
      " 5   Area15      14468 non-null  float64 \n",
      " 6   Area14      14362 non-null  float64 \n",
      " 7   Area13      14219 non-null  float64 \n",
      " 8   Area12      14086 non-null  float64 \n",
      " 9   Area11      13957 non-null  float64 \n",
      " 10  Area10      13853 non-null  float64 \n",
      " 11  Area09      13741 non-null  float64 \n",
      " 12  Area08      13671 non-null  float64 \n",
      " 13  Area07      13564 non-null  float64 \n",
      " 14  Area06      13471 non-null  float64 \n",
      " 15  Area05      13379 non-null  float64 \n",
      " 16  Area04      13296 non-null  float64 \n",
      " 17  Area03      13231 non-null  float64 \n",
      " 18  Area02      13153 non-null  float64 \n",
      " 19  Area01      13069 non-null  float64 \n",
      " 20  Area00      12972 non-null  float64 \n",
      " 21  Area99      12810 non-null  float64 \n",
      "dtypes: float64(19), geometry(1), int64(2)\n",
      "memory usage: 2.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "SettAreas = gpd.read_file(r'Results/CumulativeSettlements.gpkg', \n",
    "                          layer=''.join(['Cu', str(StudyEnd), '_Bounded']))\n",
    "SettAreas['Area15'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry')\n",
    "\n",
    "\n",
    "for item in ReversedStudyYears:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(r'Results/CumulativeSettlements.gpkg', layer=''.join(['Cu', str(item), '_Bounded']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['Area', str(item)[2:]])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Bounded_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, StudyEnd, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Bounded_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(ResultsFolder, 'Areas%sto%s_%s.csv' % (StudyStart, StudyEnd, 'Bounded')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d83c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9717437",
   "metadata": {},
   "source": [
    "### 5.4 One settlement geofile to rule them all. ...and in the Sett_ID bind them.\n",
    "The annualized values can be stored as distinct non-spatial dataframes. Their Sett_IDs will be used to join onto this geoversion with place names for the summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settlements = gpd.read_file('AnnualSettlements.gpkg', \n",
    "                           layer=''.join(['Cu', str(StudyEnd), '_Boundless']))[['Sett_ID', 'ADM_ID', 'geometry']]\n",
    "print(Settlements.info())\n",
    "Settlements.to_file(driver='GPKG', \n",
    "                       filename=r'Results/SETTLEMENTS.gpkg', \n",
    "                       layer='SETTLEMENTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744902e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbef5b7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ce84a",
   "metadata": {},
   "source": [
    "## 6. PLACE NAMES\n",
    "Join urban place names from UCDB, Africapolis, and GeoNames onto the settlement vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456bf69",
   "metadata": {},
   "source": [
    "### 6.1 Load placename datasets, filter, and project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "612f815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 13135 entries, 0 to 13134\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   UCDB_Name  13135 non-null  object  \n",
      " 1   geometry   13135 non-null  geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 7720 entries, 0 to 7719\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   Afpl_Name  7720 non-null   object  \n",
      " 1   geometry   7720 non-null   geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 120.8+ KB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 199390 entries, 0 to 199389\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   GeoName   199390 non-null  object  \n",
      " 1   geometry  199390 non-null  geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None \n",
      "\n",
      "\n",
      " None \n",
      "\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# Load, pull name field, rename, and reproject to match the catchments CRS.\n",
    "UCDB = gpd.read_file('PlaceName/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_2.gpkg', \n",
    "                     layer=0)[['UC_NM_MN', 'geometry']].rename(\n",
    "    columns={\"UC_NM_MN\": \"UCDB_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "Africapolis = gpd.read_file('PlaceName/AFRICAPOLIS2020.shp')[['agglosName', 'geometry']].rename(\n",
    "    columns={\"agglosName\": \"Afpl_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "GeoNames = gpd.read_file('PlaceName/GeoNames.gpkg', \n",
    "                         layer=0)[['GeoName', 'geometry']].to_crs(\"ESRI:102022\")\n",
    "\n",
    "print(UCDB.info(), Africapolis.info(), GeoNames.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2ae91",
   "metadata": {},
   "source": [
    "### 6.2 Join placenames onto settlements geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed = gpd.sjoin(Settlements, GeoNames, \n",
    "                             how='left', predicate='contains', # Name file is point type, so we can do contain.\n",
    "                             lsuffix=\"G3\", rsuffix=\"GN\") \n",
    "SettlementsNamed = gpd.sjoin(SettlementsNamed, Africapolis, \n",
    "                             how='left', predicate='intersects', # Name file is polygon type.\n",
    "                             lsuffix=\"G3\", rsuffix=\"Af\") \n",
    "SettlementsNamed = gpd.sjoin(SettlementsNamed, UCDB, \n",
    "                             how='left', predicate='intersects', # Name file is polygon type.\n",
    "                             lsuffix=\"G3\", rsuffix=\"UC\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SettlementsNamed.info())\n",
    "print(SettlementsNamed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SettlementsNamed['GeoName'].count(), \n",
    "      SettlementsNamed['Afpl_Name'].count(), \n",
    "      SettlementsNamed['UCDB_Name'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed = SettlementsNamed[['year', 'Sett_ID', 'GeoName', 'Afpl_Name', 'UCDB_Name', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d77c84",
   "metadata": {},
   "source": [
    "### 6.3 Reduce to single name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single name column where non-named settlements are \"UNK\" but all others use one of the three name sources.\n",
    "SettlementsNamed['SettName'] = \"UNK\"\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['Afpl_Name'].isnan == False, \n",
    "    'SettName'] = SettlementsNamed['Afpl_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['SettName'] == \"UNK\" & SettlementsNamed['UCDB_Name'].isnan == False, \n",
    "    'SettName'] = SettlementsNamed['UCDB_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['SettName'] == \"UNK\" & SettlementsNamed['GeoName'].isnan == False, \n",
    "    'SettName'] = SettlementsNamed['GeoName']\n",
    "\n",
    "SettlementsNamed.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d202603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop geometry and save to file.\n",
    "SettlementsNamed = pd.DataFrame(SettlementsNamed).drop(columns='geometry')\n",
    "SettlementsNamed.to_csv(r'Results/PlaceNames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57be76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d7b6d",
   "metadata": {},
   "source": [
    "## 6. CREATE FRAGMENTATION INDEX\n",
    "We are determining what percentage of a settlement's area lies outside of its administrative zone each year.\n",
    "The index is a range of 0 to 100, i.e. the percent of the settlement area which is fragmented.\n",
    "\n",
    "For each Sett_ID:\n",
    "((Area of Boundless settlement - Area of largest Bounded settlement feature) / Area of Boundless settlement) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17a7248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Boundless dataset, whose settlements will be used as the index of the Fragmentation Index dataset. Sun Nov 20 09:37:43 2022\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13753 entries, 0 to 13752\n",
      "Data columns (total 20 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Sett_ID  13753 non-null  float64\n",
      " 1   year     13753 non-null  int64  \n",
      " 2   ADM_ID   13753 non-null  int64  \n",
      " 3   Area15   13753 non-null  float64\n",
      " 4   Area14   13649 non-null  float64\n",
      " 5   Area13   13511 non-null  float64\n",
      " 6   Area12   13388 non-null  float64\n",
      " 7   Area11   13262 non-null  float64\n",
      " 8   Area10   13164 non-null  float64\n",
      " 9   Area09   13059 non-null  float64\n",
      " 10  Area08   12992 non-null  float64\n",
      " 11  Area07   12893 non-null  float64\n",
      " 12  Area06   12801 non-null  float64\n",
      " 13  Area05   12714 non-null  float64\n",
      " 14  Area04   12632 non-null  float64\n",
      " 15  Area03   12569 non-null  float64\n",
      " 16  Area02   12494 non-null  float64\n",
      " 17  Area01   12415 non-null  float64\n",
      " 18  Area00   12320 non-null  float64\n",
      " 19  Area99   12173 non-null  float64\n",
      "dtypes: float64(18), int64(2)\n",
      "memory usage: 2.1 MB\n",
      "None\n",
      "Loaded Bounded dataset, which will factor into the fragmentation calculation. Sun Nov 20 09:37:45 2022\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14468 entries, 0 to 14467\n",
      "Data columns (total 21 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Bounded_ID  14468 non-null  float64\n",
      " 1   year        14468 non-null  int64  \n",
      " 2   ADM_ID      14468 non-null  int64  \n",
      " 3   Sett_ID     14468 non-null  float64\n",
      " 4   Area15      14468 non-null  float64\n",
      " 5   Area14      14362 non-null  float64\n",
      " 6   Area13      14219 non-null  float64\n",
      " 7   Area12      14086 non-null  float64\n",
      " 8   Area11      13957 non-null  float64\n",
      " 9   Area10      13853 non-null  float64\n",
      " 10  Area09      13741 non-null  float64\n",
      " 11  Area08      13671 non-null  float64\n",
      " 12  Area07      13564 non-null  float64\n",
      " 13  Area06      13471 non-null  float64\n",
      " 14  Area05      13379 non-null  float64\n",
      " 15  Area04      13296 non-null  float64\n",
      " 16  Area03      13231 non-null  float64\n",
      " 17  Area02      13153 non-null  float64\n",
      " 18  Area01      13069 non-null  float64\n",
      " 19  Area00      12972 non-null  float64\n",
      " 20  Area99      12810 non-null  float64\n",
      "dtypes: float64(19), int64(2)\n",
      "memory usage: 2.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "FragIndices = pd.DataFrame(\n",
    "    gpd.read_file('AnnualizedOntoLatestYear.gpkg', \n",
    "                  layer=('Areas%sto%s_%s' % (StudyStart, StudyEnd, 'Boundless')))).drop(\n",
    "    columns='geometry') # We can assign geometries back to the settlements after calculating fragmentation.\n",
    "print('Loaded Boundless dataset, whose settlements will be used as the index of the Fragmentation Index dataset. %s' % time.ctime())\n",
    "print(FragIndices.info())\n",
    "Bounded = pd.DataFrame(\n",
    "    gpd.read_file('AnnualizedOntoLatestYear.gpkg', \n",
    "                  layer=('Areas%sto%s_%s' % (StudyStart, StudyEnd, 'Bounded')))).drop(\n",
    "    columns='geometry')\n",
    "print('Loaded Bounded dataset, which will factor into the fragmentation calculation. %s' % time.ctime())\n",
    "print(Bounded.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27235433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13753 entries, 0 to 14467\n",
      "Data columns (total 21 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Bounded_ID  13753 non-null  float64\n",
      " 1   year        13753 non-null  int64  \n",
      " 2   ADM_ID      13753 non-null  int64  \n",
      " 3   Sett_ID     13753 non-null  float64\n",
      " 4   Area15      13753 non-null  float64\n",
      " 5   Area14      13649 non-null  float64\n",
      " 6   Area13      13510 non-null  float64\n",
      " 7   Area12      13386 non-null  float64\n",
      " 8   Area11      13260 non-null  float64\n",
      " 9   Area10      13162 non-null  float64\n",
      " 10  Area09      13057 non-null  float64\n",
      " 11  Area08      12989 non-null  float64\n",
      " 12  Area07      12889 non-null  float64\n",
      " 13  Area06      12799 non-null  float64\n",
      " 14  Area05      12712 non-null  float64\n",
      " 15  Area04      12630 non-null  float64\n",
      " 16  Area03      12566 non-null  float64\n",
      " 17  Area02      12491 non-null  float64\n",
      " 18  Area01      12408 non-null  float64\n",
      " 19  Area00      12314 non-null  float64\n",
      " 20  Area99      12167 non-null  float64\n",
      "dtypes: float64(19), int64(2)\n",
      "memory usage: 2.3 MB\n",
      "None\n",
      "Filtered the Bounded dataset to only rows where latest year's area is largest for each Sett_ID. Sun Nov 20 09:37:48 2022\n",
      "Renamed columns to avoid duplication during merge, and dropped unnecessary columns. Sun Nov 20 09:37:48 2022\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13753 entries, 0 to 13752\n",
      "Data columns (total 38 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Sett_ID     13753 non-null  float64\n",
      " 1   year        13753 non-null  int64  \n",
      " 2   ADM_ID      13753 non-null  int64  \n",
      " 3   Area15      13753 non-null  float64\n",
      " 4   Area14      13649 non-null  float64\n",
      " 5   Area13      13511 non-null  float64\n",
      " 6   Area12      13388 non-null  float64\n",
      " 7   Area11      13262 non-null  float64\n",
      " 8   Area10      13164 non-null  float64\n",
      " 9   Area09      13059 non-null  float64\n",
      " 10  Area08      12992 non-null  float64\n",
      " 11  Area07      12893 non-null  float64\n",
      " 12  Area06      12801 non-null  float64\n",
      " 13  Area05      12714 non-null  float64\n",
      " 14  Area04      12632 non-null  float64\n",
      " 15  Area03      12569 non-null  float64\n",
      " 16  Area02      12494 non-null  float64\n",
      " 17  Area01      12415 non-null  float64\n",
      " 18  Area00      12320 non-null  float64\n",
      " 19  Area99      12173 non-null  float64\n",
      " 20  Bounded_ID  13753 non-null  float64\n",
      " 21  Largest15   13753 non-null  float64\n",
      " 22  Largest14   13649 non-null  float64\n",
      " 23  Largest13   13510 non-null  float64\n",
      " 24  Largest12   13386 non-null  float64\n",
      " 25  Largest11   13260 non-null  float64\n",
      " 26  Largest10   13162 non-null  float64\n",
      " 27  Largest09   13057 non-null  float64\n",
      " 28  Largest08   12989 non-null  float64\n",
      " 29  Largest07   12889 non-null  float64\n",
      " 30  Largest06   12799 non-null  float64\n",
      " 31  Largest05   12712 non-null  float64\n",
      " 32  Largest04   12630 non-null  float64\n",
      " 33  Largest03   12566 non-null  float64\n",
      " 34  Largest02   12491 non-null  float64\n",
      " 35  Largest01   12408 non-null  float64\n",
      " 36  Largest00   12314 non-null  float64\n",
      " 37  Largest99   12167 non-null  float64\n",
      "dtypes: float64(36), int64(2)\n",
      "memory usage: 4.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LargestFragments = Bounded.loc[Bounded.groupby([\"Sett_ID\"])[\"Area15\"].idxmax()] \n",
    "print(LargestFragments.info())\n",
    "print(\"Filtered the Bounded dataset to only rows where latest year's area is largest for each Sett_ID. %s\" % time.ctime())\n",
    "LargestFragments.columns = LargestFragments.columns.str.replace('Area', 'Largest')\n",
    "LargestFragments = LargestFragments.drop(columns=['year', 'ADM_ID'])\n",
    "print(\"Renamed columns to avoid duplication during merge, and dropped unnecessary columns. %s\" % time.ctime())\n",
    "FragIndices = FragIndices.merge(LargestFragments, how='left', on='Sett_ID')\n",
    "print(FragIndices.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "826b712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created names for Year 1999's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 1999. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2000's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2000. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2001's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2001. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2002's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2002. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2003's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2003. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2004's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2004. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2005's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2005. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2006's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2006. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2007's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2007. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2008's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2008. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2009's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2009. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2010's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2010. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2011's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2011. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2012's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2012. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2013's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2013. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2014's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2014. Sun Nov 20 09:41:24 2022\n",
      "Created names for Year 2015's variables and temporary objects. Sun Nov 20 09:41:24 2022\n",
      "Calculated fragmentation index for year 2015. Sun Nov 20 09:41:24 2022\n",
      "Completed fragmentation index calculations for all years and saved to file. Sun Nov 20 09:41:24 2022\n"
     ]
    }
   ],
   "source": [
    "for item in AllStudyYears:\n",
    "    YY = str(item)[2:] # 2-digit year\n",
    "    AreaYY = ''.join([\"Area\", YY]) # The Boundless area variable name\n",
    "    LargestYY = ''.join(['Largest', YY]) # The Bounded largest area variable name\n",
    "    FragYY = ''.join([\"Frag\", YY]) # Name for the fragmentation index variable\n",
    "    print(\"Created names for Year %s's variables and temporary objects. %s\" % (item, time.ctime()))\n",
    "    \n",
    "    FragIndices[FragYY] = ((FragIndices[AreaYY] - FragIndices[LargestYY]) / FragIndices[AreaYY]) * 100\n",
    "    FragIndices[FragYY] = (FragIndices[FragYY].fillna(0).replace([np.inf, -np.inf], 0)).astype('int')\n",
    "    print(\"Calculated fragmentation index for year %s. %s\" % (item, time.ctime()))\n",
    "\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('Largest')]\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('Area')]\n",
    "\n",
    "print('Completed fragmentation index calculations for all years and saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f309be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13753 entries, 0 to 13752\n",
      "Data columns (total 21 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Sett_ID     13753 non-null  float64\n",
      " 1   year        13753 non-null  int64  \n",
      " 2   ADM_ID      13753 non-null  int64  \n",
      " 3   Bounded_ID  13753 non-null  float64\n",
      " 4   Frag99      13753 non-null  int32  \n",
      " 5   Frag00      13753 non-null  int32  \n",
      " 6   Frag01      13753 non-null  int32  \n",
      " 7   Frag02      13753 non-null  int32  \n",
      " 8   Frag03      13753 non-null  int32  \n",
      " 9   Frag04      13753 non-null  int32  \n",
      " 10  Frag05      13753 non-null  int32  \n",
      " 11  Frag06      13753 non-null  int32  \n",
      " 12  Frag07      13753 non-null  int32  \n",
      " 13  Frag08      13753 non-null  int32  \n",
      " 14  Frag09      13753 non-null  int32  \n",
      " 15  Frag10      13753 non-null  int32  \n",
      " 16  Frag11      13753 non-null  int32  \n",
      " 17  Frag12      13753 non-null  int32  \n",
      " 18  Frag13      13753 non-null  int32  \n",
      " 19  Frag14      13753 non-null  int32  \n",
      " 20  Frag15      13753 non-null  int32  \n",
      "dtypes: float64(2), int32(17), int64(2)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "        Sett_ID  year  ADM_ID  Bounded_ID  Frag99  Frag00  Frag01  Frag02  \\\n",
      "11027  122151.0  2015     209    124622.0       0       0       0       0   \n",
      "11815  156462.0  2015     240    159389.0       0       0       0       0   \n",
      "8039    56278.0  2015     288     57877.0       0       0       0       0   \n",
      "12406  191121.0  2007     275    194826.0       0       0       0       0   \n",
      "11415  137717.0  2015     211    140405.0       0       0       0       0   \n",
      "13708  201189.0  1999     322    205141.0       0       0       0       0   \n",
      "6128    34692.0  2006     101     35690.0       0       0       0       0   \n",
      "6793    46409.0  2015     189     47742.0      22      23      26      27   \n",
      "7553    51706.0  2015     301     53195.0       0       0       0       0   \n",
      "7428    51333.0  2000     307     52799.0       0       0       0       0   \n",
      "8012    56249.0  2010     287     57844.0       0       0       0       0   \n",
      "10426   80721.0  2015     162     82749.0       0       0       0       0   \n",
      "11579  145880.0  2015     227    148660.0       0       0       0       0   \n",
      "6662    45207.0  2015     173     46494.0       0       0       0       0   \n",
      "3592    15436.0  2013      31     15991.0       0       0       0       0   \n",
      "\n",
      "       Frag03  Frag04  ...  Frag06  Frag07  Frag08  Frag09  Frag10  Frag11  \\\n",
      "11027       0       0  ...       0       0       0       0       0       0   \n",
      "11815       0       0  ...       0       0       0       0       0       0   \n",
      "8039        0       0  ...       0       0       0       0       0       0   \n",
      "12406       0       0  ...       0       0       0       0       0       0   \n",
      "11415       0       0  ...       0       0       0       0       0       0   \n",
      "13708       0       0  ...       0       0       0       0       0       0   \n",
      "6128        0       0  ...       0       0       0       0       0       0   \n",
      "6793       27      28  ...      27      27      26      27      26      26   \n",
      "7553        0       0  ...       0       0       0       0       0       0   \n",
      "7428        0       0  ...       0       0       0       0       0       0   \n",
      "8012        0       0  ...       0       0       0       0       0       0   \n",
      "10426       0       0  ...       0       0       0       0       0       0   \n",
      "11579       0       0  ...       0       0       0       0       0       0   \n",
      "6662        0       0  ...       0       0       0       0       0       0   \n",
      "3592        0       0  ...       0       0       0       0       0       0   \n",
      "\n",
      "       Frag12  Frag13  Frag14  Frag15  \n",
      "11027       0       0       0       0  \n",
      "11815       0       0       0       0  \n",
      "8039        0       0       0       0  \n",
      "12406       0       0       0       0  \n",
      "11415       0       0       0       0  \n",
      "13708       0       0       0       0  \n",
      "6128        0       0       0       0  \n",
      "6793       26      29      29      28  \n",
      "7553        0       0       0       0  \n",
      "7428        0       0       0       0  \n",
      "8012        0       0       0       0  \n",
      "10426       0       0       0       0  \n",
      "11579       0       0       0       0  \n",
      "6662        0       0       0       0  \n",
      "3592        0       0       0       0  \n",
      "\n",
      "[15 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(FragIndices.info())\n",
    "print(FragIndices.sample(15))\n",
    "\n",
    "Settlements = gpd.read_file('AnnualizedOntoLatestYear.gpkg', \n",
    "                  layer=('Areas%sto%s_%s' % (StudyStart, StudyEnd, 'Boundless')))[['Sett_ID', 'geometry']]\n",
    "Settlements = Settlements.merge(FragIndices, how='left', on='Sett_ID')\n",
    "Settlements.to_file(driver='GPKG', \n",
    "                    filename='AnnualizedOntoLatestYear.gpkg', \n",
    "                    layer=('FragIndex%sto%s' % (StudyStart, StudyEnd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae635c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sett_ID</th>\n",
       "      <th>geometry</th>\n",
       "      <th>year</th>\n",
       "      <th>ADM_ID</th>\n",
       "      <th>Bounded_ID</th>\n",
       "      <th>Frag99</th>\n",
       "      <th>Frag00</th>\n",
       "      <th>Frag01</th>\n",
       "      <th>Frag02</th>\n",
       "      <th>Frag03</th>\n",
       "      <th>...</th>\n",
       "      <th>Frag06</th>\n",
       "      <th>Frag07</th>\n",
       "      <th>Frag08</th>\n",
       "      <th>Frag09</th>\n",
       "      <th>Frag10</th>\n",
       "      <th>Frag11</th>\n",
       "      <th>Frag12</th>\n",
       "      <th>Frag13</th>\n",
       "      <th>Frag14</th>\n",
       "      <th>Frag15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>766.0</td>\n",
       "      <td>MULTIPOLYGON (((-1515714.950 357166.824, -1515...</td>\n",
       "      <td>1985</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12485</th>\n",
       "      <td>191351.0</td>\n",
       "      <td>MULTIPOLYGON (((-1061754.872 1428537.936, -106...</td>\n",
       "      <td>2015</td>\n",
       "      <td>279</td>\n",
       "      <td>195070.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>832.0</td>\n",
       "      <td>POLYGON ((-1517212.050 367694.328, -1517183.80...</td>\n",
       "      <td>1985</td>\n",
       "      <td>4</td>\n",
       "      <td>728.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>12922.0</td>\n",
       "      <td>MULTIPOLYGON (((-1599213.550 492380.440, -1599...</td>\n",
       "      <td>2015</td>\n",
       "      <td>113</td>\n",
       "      <td>13434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>79589.0</td>\n",
       "      <td>MULTIPOLYGON (((-1578734.360 682001.968, -1578...</td>\n",
       "      <td>2015</td>\n",
       "      <td>164</td>\n",
       "      <td>81596.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sett_ID                                           geometry  year  \\\n",
       "221       766.0  MULTIPOLYGON (((-1515714.950 357166.824, -1515...  1985   \n",
       "12485  191351.0  MULTIPOLYGON (((-1061754.872 1428537.936, -106...  2015   \n",
       "244       832.0  POLYGON ((-1517212.050 367694.328, -1517183.80...  1985   \n",
       "3176    12922.0  MULTIPOLYGON (((-1599213.550 492380.440, -1599...  2015   \n",
       "10243   79589.0  MULTIPOLYGON (((-1578734.360 682001.968, -1578...  2015   \n",
       "\n",
       "       ADM_ID  Bounded_ID  Frag99  Frag00  Frag01  Frag02  Frag03  ...  \\\n",
       "221         4       655.0       0       0       0       0       0  ...   \n",
       "12485     279    195070.0       3       3       3       3       2  ...   \n",
       "244         4       728.0       0       0       0       0       0  ...   \n",
       "3176      113     13434.0       0       0       0       0       0  ...   \n",
       "10243     164     81596.0       0       0       0       0       0  ...   \n",
       "\n",
       "       Frag06  Frag07  Frag08  Frag09  Frag10  Frag11  Frag12  Frag13  Frag14  \\\n",
       "221         0       0       0       0       0       0       0       0       0   \n",
       "12485       3       3       3       4       4       4       5       7       7   \n",
       "244         0       0       0       0       0       0       0       0       0   \n",
       "3176        0       0       0       0       0       0       0       0       0   \n",
       "10243       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       Frag15  \n",
       "221         0  \n",
       "12485       7  \n",
       "244         0  \n",
       "3176        0  \n",
       "10243       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settlements.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36f039",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db18c",
   "metadata": {},
   "source": [
    "## 7. PREPARE YEARLY DATASETS\n",
    "Population, nighttime lights, and any other annualized rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8856d7",
   "metadata": {},
   "source": [
    "### 7.1 Buffer the area of the Boundless dataset's latest year to mask raster data in next section.\n",
    "The Bounded dataset would also be fine for our purposes here. The buffer is dissolved to a single feature to be used for its total extents, which are identical between Bounded & Boundless datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bb2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating buffer layer. Thu Nov 17 16:42:08 2022\n",
      "Finished buffer layer creation. Thu Nov 17 18:05:13 2022\n",
      "Saved to file. Thu Nov 17 18:05:18 2022\n"
     ]
    }
   ],
   "source": [
    "# Create buffer layer(s) to use as maximum distance for Near joins.\n",
    "\n",
    "Distance = 2000\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = gpd.read_file('AnnualSettlements.gpkg', layer=''.join(['Cu', str(StudyEnd), '_Boundless']))\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(\n",
    "    make_valid).buffer(Distance) #.dissolve() # make_valid is a workaround for any null geometries.\n",
    "print('Finished buffer layer creation. %s' % time.ctime())\n",
    "BufferFileName = ''.join(['Buff', str(Distance/1000), 'km_', str(StudyEnd)])\n",
    "BufferLayer.to_file(driver='GPKG', filename='Catchment.gpkg', layer=BufferFileName)\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd172",
   "metadata": {},
   "source": [
    "### 7.2 Population: Reproject and reclassify with settlement buffer mask.\n",
    "Reclassify so that we only need to work with cells within X distance of settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189136b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProjCRS = gdal.WarpOptions(dstSRS='ESRI:102022')\n",
    "AnnualizedSourceFiles = [i for i in os.listdir('Population/') if i.endswith('.tif')]\n",
    "\n",
    "with fiona.open(\"Catchment.gpkg\", mode=\"r\", layer=BufferFileName) as shapefile:\n",
    "    MaskGeom = [feature[\"geometry\"] for feature in shapefile] # Identify the bounding areas of the mask.\n",
    "# Mask_out = './LatestYearBuffer.tif'\n",
    "AnnualizedSourceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block changes each annual population raster's projection (gdal.Warp()), \n",
    "# then masks it to within a specified distance of the settlements (rasterio.mask.mask()).\n",
    "\n",
    "for YearFile in AnnualizedSourceFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    TempOutputName = \"Wpop_\" + Year + \"_albers.tif\"\n",
    "    TempOutputPath = os.path.join(ProjectFolder, \"Population\", TempOutputName)\n",
    "    if exists(TempOutputPath):\n",
    "        pass\n",
    "    else:\n",
    "        # Reproject to same CRS as settlements.\n",
    "        Warp = gdal.Warp(TempOutputPath, # Where to store the warped raster\n",
    "                     InputRasterObject, # Which raster to warp\n",
    "                     format='GTiff', \n",
    "                     options=ProjCRS) # Reproject to Africa Albers Equal Area Conic\n",
    "        print('Finished gdal.Warp() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "        \n",
    "        Warp = None # Close the files\n",
    "        InputRasterObject = None\n",
    "\n",
    "        # Reclassify as nodata if outside settlement buffer zones.\n",
    "        with rasterio.open(TempOutputPath) as InputRasterObject:\n",
    "            MaskedOutputRaster, OutTransform = rasterio.mask.mask(InputRasterObject, MaskGeom) # Anything outside the mask is reclassed to the raster's NoData value.\n",
    "            OutMetaData = InputRasterObject.meta.copy()\n",
    "        print('Finished rasterio.mask.mask() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "        OutMetaData.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": MaskedOutputRaster.shape[1],\n",
    "                         \"width\": MaskedOutputRaster.shape[2],\n",
    "                         \"transform\": OutTransform})\n",
    "        FinalOutputPath = os.path.join(ProjectFolder, \"Population\", ''.join(['Masked_', Year, '.tif'])) # ''.join([r'Population/', 'Masked_', Year, '.tif']\n",
    "        with rasterio.open(FinalOutputPath), \"w\", **OutMetaData) as dest:\n",
    "            dest.write(MaskedOutputRaster)\n",
    "        print('Written to file. %s \\n' % time.ctime())\n",
    "    InputRasterObject = None\n",
    "    \n",
    "    try:  # Finally, remove the intermediate file from disk\n",
    "        os.remove(TempOutputPath)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed intermediate file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished all years in list. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('Population/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624d801",
   "metadata": {},
   "source": [
    "### 7.3 Convert each annualized raster to .xyz where cell centers are stored as x and y. \n",
    "Similar to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddceeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnualizedMaskedFiles = [i for i in os.listdir('Population/') if i.startswith('Masked') and i.endswith('.tif')]\n",
    "AnnualizedMaskedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for YearFile in AnnualizedMaskedFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    XYZOutputPath = r'Population/{}'.format(\n",
    "        YearFile.replace('.tif', '.xyz')) # New file path will be the same as original, but .tif is replaced with .xyz\n",
    "    \n",
    "    # Create an .xyz version of the .tif\n",
    "    XYZ = gdal.Translate(XYZOutputPath, # Specify a destination path\n",
    "                         InputRasterObject, # Input is the masked .tif file\n",
    "                         format='XYZ', \n",
    "                         creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "    print('Finished gdal.Translate() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    InputRasterObject = None # Close the files\n",
    "    XYZ = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c03d18",
   "metadata": {},
   "source": [
    "### 7.4 Create spatial objects (gdf) from the x,y fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove all vectorized cells that contain the raster's NoData value before Near joining to settlements.\n",
    "NoDataVal = -99999 \n",
    "\n",
    "# If starting script from this section:\n",
    "ADM_vec = gpd.read_file(glob.glob('ADM/*.shp')[0])[['geometry']].to_crs(\"ESRI:102022\")\n",
    "ADM_vec['ADM_ID'] = ADM_vec.index\n",
    "\n",
    "AnnualizedXYZFiles = [i for i in os.listdir('Population/') if i.startswith('Masked') and i.endswith('.xyz')]\n",
    "AnnualizedXYZFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for YearFile in AnnualizedXYZFiles:\n",
    "    InputXYZName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputXYZ = pd.read_table(InputXYZName, delim_whitespace=True)\n",
    "    InputXYZ = InputXYZ.loc[InputXYZ['Z'] != NoDataVal] # Subset to only the features that have a raster value.\n",
    "    print('Loaded XYZ file as a pandas dataframe, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    OutputSpatialObject = gpd.GeoDataFrame(InputXYZ, \n",
    "                                           geometry = gpd.points_from_xy(InputXYZ['X'], InputXYZ['Y']), \n",
    "                                           crs = 'ESRI:102022')\n",
    "    print('Created geodataframe from non-NoData points, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    # Add ADM field for thoroughness.\n",
    "    OutputSpatialObject = gpd.sjoin(OutputSpatialObject, ADM_vec, how='left', predicate='within') \n",
    "#   OutputSpatialObject = OutputSpatialObject[['Z', 'ADM_ID', 'geometry']]\n",
    "    \n",
    "    OutputVectorName = YearFile.replace('.xyz', '')\n",
    "    OutputSpatialObject.to_file(driver='GPKG', filename='Population/PopulationWithinBuffer.gpkg', layer=OutputVectorName)\n",
    "    print('Exported as geopackage layer, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "#   # The XYZ files are very large and unnecessary now that we have point objects in a geopackage. Removing XYZ file.\n",
    "#     try:  \n",
    "#         os.remove(InputXYZName)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     print('Removed intermediate XYZ file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished generating raster value points within settlement catchments, all years. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a37640",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3f13a",
   "metadata": {},
   "source": [
    "## 8. JOIN RASTER DATA BY SETTLEMENT GROUP AND PERFORM SUMMARY STATS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0ee77",
   "metadata": {},
   "source": [
    "### 8.1 Merge Sett_ID onto value vectors.\n",
    "Merge settlement ID onto the raster data that we vectorized in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36697537",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestYearSett = gpd.read_file('AnnualSettlements.gpkg', layer='Cu2015')\n",
    "\n",
    "def CreateList(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]\n",
    "\n",
    "AllValYears = CreateList(2000, 2015) # All years for which there will be growth stats in the present study.\n",
    "print(AllValYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestYearSett.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d057d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join nearest settlement ID to each value cell.\n",
    "\n",
    "for item in AllValYears:\n",
    "    Year = str(item)\n",
    "    ValFileName = ''.join(['Masked_', str(item)])\n",
    "    ValObject = gpd.read_file('Population/PopulationWithinBuffer.gpkg', layer=ValFileName)[['Z', 'ADM_ID', 'geometry']]\n",
    "    print('Loaded value data for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    # Sjoin_nearest: No need to group by ADM this time. \n",
    "    ValObject_withID = gpd.sjoin_nearest(ValObject, \n",
    "                                    LatestYearSett, \n",
    "                                    how='left') # No need for max_distance parameter this time. We've already narrowed down to nearby raster cells.\n",
    "    \n",
    "    print('\\nJoined settlement ID onto vectorized raster cells for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValObject_withID.sample(10))\n",
    "    \n",
    "    # We no longer need the spatial information of the raster values because we have their unique settlement ID.\n",
    "    ValObject_withID = pd.DataFrame(ValObject_withID).drop(columns='geometry')\n",
    "    \n",
    "    ValObject_withID.to_csv(''.join([r'Population/', ValFileName, '.csv']))\n",
    "    print('\\nExported as table, year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "print('\\n \\n Finished generating raster value points within settlement catchments, all years. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1aae02",
   "metadata": {},
   "source": [
    "### 8.2 Perform settlement summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to settlement level, then join onto settlements geodataframe.\n",
    "AllSummaries =  LatestYearSett\n",
    "\n",
    "for item in AllValYears:\n",
    "    Year = str(item)\n",
    "    AbbrevYear = Year[2:]\n",
    "    VariableName = ''.join(['PopSum', AbbrevYear])\n",
    "    ValFileName = ''.join(['Masked_', str(item)])\n",
    "    ValObject = pd.read_csv(''.join([r'Population/', ValFileName, '.csv']))\n",
    "    print('Loaded value data for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    ValAggregated = ValObject.groupby('Sett_ID', \n",
    "                                      as_index=False)['Z'].sum().rename(columns={\"Z\": VariableName})\n",
    "    \n",
    "    print('\\nValues aggregated to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValAggregated.sample(10))\n",
    "    \n",
    "    AllSummaries = AllSummaries.merge(ValAggregated, how='left', on='Sett_ID')\n",
    "    print('\\nMerged year %s onto latest year settlement feature layer. %s \\n' % (Year, time.ctime()))\n",
    "    print(AllSummaries.sample(10))\n",
    "\n",
    "print('\\n \\n Finished merging aggregate raster data onto settlements for all years. %s' % time.ctime())\n",
    "\n",
    "AllSummaries.to_file(driver='GPKG', filename='AnnualizedOntoLatestYear.gpkg', layer=('Pop%sto%s' % (2000, 2015)))\n",
    "\n",
    "print('\\n \\n Written to file. %s' % time.ctime())\n",
    "AllSummaries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff1384",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac7167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2da017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
