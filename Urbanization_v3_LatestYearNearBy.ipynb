{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2016b3",
   "metadata": {},
   "source": [
    "# Spatiotemporal Trends in Urbanization: Cameroon\n",
    "*Using yearly estimates (2000-2015) of population, built-area, and economic indicators to track city-by-city growth and change over time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd14a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903293ba",
   "metadata": {},
   "source": [
    "### Research questions \n",
    "\n",
    "#### 1. How has the size of Settlement X changed over time? \n",
    "\n",
    "- Population size \n",
    "\n",
    "- Geographical extents \n",
    "\n",
    "- Population density \n",
    "\n",
    "#### 2. In what year did Settlement X become a new urban class?  \n",
    "\n",
    "- From semi-dense to high-density city \n",
    "\n",
    "- Small settlement area to built-up area \n",
    "\n",
    "- When a hamlet area or small settlement area first appeared\n",
    "\n",
    "#### 3. Is there a discernable pattern between the spatio-temporal distribution of economic density and population density? \n",
    "\n",
    "#### 4. How much of urban space attributable to City X is outside of the administrative limits of the city? \n",
    "\n",
    "- When did this fragment(s) appear? \n",
    "\n",
    "- Which district/municipality/authority has purview over the fragment(s)? \n",
    "\n",
    "#### 5. For the questions above, how does the answer change based on different understandings of urban limits? \n",
    "\n",
    "- Scenario A: where \"city\" is delimited by an official administrative boundary \n",
    "\n",
    "- Scenario B: where \"city\" includes all contiguous (and near-contiguous) built up area \n",
    "\n",
    "#### 6. Subnational and inter-national comparisons. Examples: \n",
    "\n",
    "- Compare the rates (pop, build-up, economicâ€¦) of the fastest growing settlement of each ADM1 region. \n",
    "\n",
    "- Which African metropoles experience the most vs. the least fragmentation? Is there a confluence between amount of urban fragmentation and rate of densification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5dafb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55d1a9",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. Most up-to-date administrative boundaries: **ADM3.**\n",
    "2. Built-up area, yearly: **World Settlement Footprint Evolution.** Resolution: 30m.\n",
    "3. Settlement types: **GRID3 settlement extents.** Captured between 2009-2019.\n",
    "4. Population, yearly: **WorldPop.** UN-adjusted, unconstrained. Resolution: 100m.\n",
    "5. Nighttime lights, yearly: **Harmonization of DMSP and VIIRS.** Resolution: 1km.\n",
    "6. City names: **UCDB, Africapolis, and GeoNames.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50c896",
   "metadata": {},
   "source": [
    "## Joining primary datasets together in raster and vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276b447",
   "metadata": {},
   "source": [
    "### 1. PREPARE WORKSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9093fd6",
   "metadata": {},
   "source": [
    "##### Off-script: Create folders in working directory.\n",
    "> *ADM\n",
    "<br>Buildup\n",
    "<br>PlaceName\n",
    "<br>Population\n",
    "<br>Settlement\n",
    "<br>NTL*\n",
    "\n",
    "##### Off-script: Download datasets (as shapefile, GeoJSON, or tif where possible) and place or extract into corresponding folder:\n",
    "- ADM: *Sourced internally.*\n",
    "- Buildup: https://download.geoservice.dlr.de/WSF_EVO/files/\n",
    "- PlaceName: \n",
    "    - GeoNames: (file: cities500.zip) https://download.geonames.org/export/dump/\n",
    "    - Africapolis: https://africapolis.org/en/data\n",
    "    - Urban Centres Database: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php\n",
    "- Population: https://hub.worldpop.org/geodata/listing?id=69\n",
    "- Settlement: https://data.grid3.org/datasets/GRID3::grid3-cameroon-settlement-extents-version-01-01-/explore\n",
    "- NTL: https://figshare.com/articles/dataset/Harmonization_of_DMSP_and_VIIRS_nighttime_light_data_from_1992-2018_at_the_global_scale/9828827/2\n",
    "\n",
    "##### Other off-script:\n",
    "- Convert GeoNames from .txt file to shape (delimiter = tab, header rows = 0) and rename fields.\n",
    "- If necessary, mosaic WSFE rasters that cover the area of interest to create a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c42795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Most but not all of these packages were used in final form. \n",
    "\n",
    "import os, sys, glob, re, time\n",
    "from os.path import exists\n",
    "\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, Polygon, shape, MultiPoint\n",
    "from shapely.ops import voronoi_diagram\n",
    "from shapely.validation import make_valid, explain_validity\n",
    "import shapely.wkt\n",
    "from longsgis import voronoiDiagram4plg \n",
    "import scipy\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import cascaded_union\n",
    "from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area\n",
    "from geovoronoi import voronoi_regions_from_coords, points_to_coords\n",
    "\n",
    "from xrspatial import zonal_stats \n",
    "import xarray as xr \n",
    "import numpy as np \n",
    "import fiona, rioxarray\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio import features\n",
    "from rasterio.features import shapes\n",
    "from rasterio import mask\n",
    "from osgeo import gdal, osr, ogr, gdal_array\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20dfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\n"
     ]
    }
   ],
   "source": [
    "ProjectFolder = os.getcwd()\n",
    "print(ProjectFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08550",
   "metadata": {},
   "source": [
    "### 2. PREPARE BUILDUP, SETTLEMENT, AND ADMIN DATASETS\n",
    "Projection for all datasets: Africa Albers Equal Area Conic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d194cf",
   "metadata": {},
   "source": [
    "##### WSFE: Check contents and change NoData value as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef56675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_CMN.tif\n",
      "PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n"
     ]
    }
   ],
   "source": [
    "# CheckContents = gdal.Open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "# NoDataValue = CheckContents.GetRasterBand(1).GetNoDataValue()\n",
    "# print(NoDataValue) # What is NoData currently? We might want to change it before using this file as the archetype for other rasterized files.\n",
    "# print(CheckContents.GetDescription())\n",
    "# print(CheckContents.GetProjection())\n",
    "# del CheckContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d266b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open DatasetReader name='C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_CMN.tif' mode='r'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_block_shapes', '_closed', '_count', '_crs', '_crs_wkt', '_descriptions', '_dtypes', '_env', '_gcps', '_get_crs', '_get_rpcs', '_handle_crswkt', '_has_band', '_has_gcps_or_rpcs', '_mask_flags', '_nodatavals', '_offsets', '_read', '_rpcs', '_scales', '_set_all_descriptions', '_set_all_offsets', '_set_all_scales', '_set_all_units', '_set_attrs_from_dataset_handle', '_set_crs', '_set_gcps', '_set_nodatavals', '_set_rpcs', '_transform', '_units', 'block_shapes', 'block_size', 'block_window', 'block_windows', 'bounds', 'checksum', 'close', 'closed', 'colorinterp', 'colormap', 'compression', 'count', 'crs', 'dataset_mask', 'descriptions', 'driver', 'dtypes', 'files', 'gcps', 'get_gcps', 'get_nodatavals', 'get_tag_item', 'get_transform', 'height', 'index', 'indexes', 'interleaving', 'is_tiled', 'lnglat', 'mask_flag_enums', 'meta', 'mode', 'name', 'nodata', 'nodatavals', 'offsets', 'options', 'overviews', 'photometric', 'profile', 'read', 'read_crs', 'read_masks', 'read_transform', 'res', 'rpcs', 'sample', 'scales', 'shape', 'start', 'statistics', 'stop', 'subdatasets', 'tag_namespaces', 'tags', 'transform', 'units', 'width', 'window', 'window_bounds', 'window_transform', 'write_transform', 'xy']\n",
      "ESRI:102022\n",
      "('uint32',)\n",
      "(0.0,)\n",
      "0 4.328172989437514 0.0 2015\n"
     ]
    }
   ],
   "source": [
    "WSFE = rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "print(WSFE) # WSFE values are all 4 digits long (1985-2015)\n",
    "print(dir(WSFE))\n",
    "print(WSFE.crs)\n",
    "print(WSFE.dtypes)\n",
    "NoDataValue = WSFE.nodatavals\n",
    "print(NoDataValue)\n",
    "print(WSFE.read(1).min(), WSFE.read(1).mean(), np.median(WSFE.read(1)), WSFE.read(1).max())\n",
    "\n",
    "# If NoDataValue != 0, change to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a191c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "# Change NoData value to zero, as this won't interfere with a possible value of 99999 in GRID3 and ADM.\n",
    "# Then make sure there are no values above 2015 (such as 99999) or below 1985 in the dataset by reclassifying them as NoData.\n",
    "# Was having trouble with rasterio & gdal here, so moved to QGIS.\n",
    "\n",
    "# processing.run(\"native:reclassifybytable\", {'INPUT_RASTER':'C:/Users/grace/GIS/povertyequity/urban_growth/WSFE_TCD.tif','RASTER_BAND':1,'TABLE':['2016','','0','','1984','0'],'NO_DATA':0,'RANGE_BOUNDARIES':0,'NODATA_FOR_MISSING':False,'DATA_TYPE':5,'OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/WSFE.tif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ea5f",
   "metadata": {},
   "source": [
    "##### GRID3 and Admin areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7946e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ADM_vec \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADM/*.shp\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESRI:102022\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# This glob() function pulls the first file ([0]) in the ADM folder which ended in '.shp'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m GRID3_vec \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSettlement/*.shp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESRI:102022\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m ADM_vec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADM_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ADM_vec\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m      4\u001b[0m GRID3_vec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG3_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m GRID3_vec\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\geopandas\\geodataframe.py:1428\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1428\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     geo_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_geometry_column_name\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mdtype, GeometryDtype):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\geo\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5859\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['type'] not in index\""
     ]
    }
   ],
   "source": [
    "ADM_vec = gpd.read_file(glob.glob('ADM/*.shp')[0])[['geometry']].to_crs(\"ESRI:102022\") # This glob() function pulls the first file ([0]) in the ADM folder which ended in '.shp'\n",
    "GRID3_vec = gpd.read_file(glob.glob('Settlement/*.shp')[0])[['type','geometry']].to_crs(\"ESRI:102022\")\n",
    "ADM_vec['ADM_ID'] = ADM_vec.index\n",
    "GRID3_vec['G3_ID'] = GRID3_vec.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44280f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 360 entries, 0 to 359\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   geometry  360 non-null    geometry\n",
      " 1   ADM_ID    360 non-null    int64   \n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 5.8 KB\n",
      "None \n",
      "\n",
      " ESRI:102022 \n",
      "\n",
      " 3\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 201821 entries, 0 to 201820\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   type      201821 non-null  object  \n",
      " 1   geometry  201821 non-null  geometry\n",
      " 2   G3_ID     201821 non-null  int64   \n",
      "dtypes: geometry(1), int64(1), object(1)\n",
      "memory usage: 4.6+ MB\n",
      "None \n",
      "\n",
      " ESRI:102022 \n",
      "\n",
      " 6\n"
     ]
    }
   ],
   "source": [
    "ADM_out = './ADM/ADM.tif'\n",
    "GRID3_out = './Settlement/GRID3.tif'\n",
    "\n",
    "print(ADM_vec.info(), \"\\n\\n\", \n",
    "      ADM_vec.crs, \"\\n\\n\", \n",
    "      len(str(ADM_vec['ADM_ID'].max()))) # We need to know how many digits need to be allocated to each dataset in the \"join\" serial.\n",
    "print(GRID3_vec.info(), \"\\n\\n\", \n",
    "      GRID3_vec.crs, \"\\n\\n\", \n",
    "      len(str(GRID3_vec['G3_ID'].max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440aa2a2",
   "metadata": {},
   "source": [
    "### 3. WSFE AND ADM; GRID3 AND ADM\n",
    "RASTERIZE: Bring ADM and GRID3 into raster space.\n",
    "\n",
    "RASTER MATH: \"Join\" ADM ID onto GRID3 and onto WSFE by creating unique concatenation string.\n",
    "\n",
    "VECTORIZE: Bring joined data into vector space.\n",
    "\n",
    "VECTOR MATH: Split unique ID from raster math step into separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca195d70",
   "metadata": {},
   "source": [
    "##### Rasterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af504c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and update the metadata from WSFE for the output\n",
    "meta = WSFE.meta.copy()\n",
    "meta.update(compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f778fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(ADM_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(ADM_vec.geometry, ADM_vec.ADM_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, all_touched=False)\n",
    "    out.write_band(1, burned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a368d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(GRID3_out, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(GRID3_vec.geometry, GRID3_vec.G3_ID))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform, all_touched=False)\n",
    "    out.write_band(1, burned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbc740",
   "metadata": {},
   "source": [
    "*Validation: Check the dimensions, type, and basic stats of the three datasets. All should be the same dimension and NoData value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4a7d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UInt32 0.0\n",
      "UInt32 0.0\n",
      "UInt32 0.0\n"
     ]
    }
   ],
   "source": [
    "CheckContents = gdal.Open(r\"ADM/ADM.tif\")\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "CheckContents =  gdal.Open(r\"Settlement/GRID3.tif\")\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "CheckContents = gdal.Open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))\n",
    "print(gdal.GetDataTypeName(CheckContents.GetRasterBand(1).DataType), \n",
    "      CheckContents.GetRasterBand(1).GetNoDataValue())\n",
    "\n",
    "del CheckContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9037fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM/ADM.tif \n",
      "Bands=  1 \n",
      "WxH=  28589 x 42487 \n",
      "\n",
      "\n",
      "Settlement/GRID3.tif \n",
      "Bands=  1 \n",
      "WxH=  28589 x 42487 \n",
      "\n",
      "\n",
      "C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_CMN.tif \n",
      "Bands=  1 \n",
      "WxH=  28589 x 42487 \n",
      "\n",
      "\n",
      "\n",
      " [{'raster': 'ADM/ADM.tif', 'min': 0, 'mean': 63.35967493191019, 'median': 0.0, 'max': 359}, {'raster': 'Settlement/GRID3.tif', 'min': 0, 'mean': 1211.9579491038223, 'median': 0.0, 'max': 201820}, {'raster': 'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_CMN.tif', 'min': 0, 'mean': 4.328172989437514, 'median': 0.0, 'max': 2015}]\n"
     ]
    }
   ],
   "source": [
    "RastersList = [rasterio.open(r\"ADM/ADM.tif\"), \n",
    "               rasterio.open(r\"Settlement/GRID3.tif\"), \n",
    "               rasterio.open(os.path.join(ProjectFolder, \"Buildup\", os.listdir('Buildup/')[0]))]\n",
    "\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "\n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ceaa3",
   "metadata": {},
   "source": [
    "##### Raster math\n",
    "Processing is more rapid when \"joining,\" i.e. creating serial codes out of two datasets, in raster rather than vector space.\n",
    "Here, we are concatenating the ID fields of the two datasets to create a serial number that we can then split in vector space later to create two ID fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a2406",
   "metadata": {},
   "source": [
    "*Adding together the values to create join IDs. This is in effect a concatenation of their ID strings, by way of summation. The number of zeros in the calc multiplication corresponds with number of digits of the maximum value in the \"B\" dataset. (e.g. Chad ADM codes go up 4 digits, so it's calc=(A*10000)+B).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e831cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPEN TERMINAL FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Gdal_calc.py # To see info.\n",
    "\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Settlement\\GRID3_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "# gdal_calc.py -A C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_CMN.tif -B  C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\ADM\\ADM.tif --outfile=C:\\Users\\grace\\GIS\\povertyequity\\urban_growth\\Cameroon\\Buildup\\WSFE_ADM.tif --overwrite --calc=\"(A*1000)+B\"\n",
    "\n",
    "# # END TERMINAL-ONLY ASPECT. RETURN HERE FOR NEXT STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8df5020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildup/WSFE_ADM.tif \n",
      "Bands=  1 \n",
      "WxH=  28589 x 42487 \n",
      "\n",
      "\n",
      "Settlement/GRID3_ADM.tif \n",
      "Bands=  1 \n",
      "WxH=  28589 x 42487 \n",
      "\n",
      "\n",
      "\n",
      " [{'raster': 'Buildup/WSFE_ADM.tif', 'min': 1985001, 'mean': 4285658562.295929, 'median': 4294967293.0, 'max': 4294967293}, {'raster': 'Settlement/GRID3_ADM.tif', 'min': 1354, 'mean': 4242510940.302562, 'median': 4294967293.0, 'max': 4294967293}]\n"
     ]
    }
   ],
   "source": [
    "# Validation: check the basic statistics of the resulting datasets.\n",
    "RastersList = [rasterio.open(r\"Buildup/WSFE_ADM.tif\"), rasterio.open(r\"Settlement/GRID3_ADM.tif\")]\n",
    "for item in RastersList:\n",
    "    print(item.name, \"\\nBands= \", item.count, \"\\nWxH= \", item.width, \"x\", item.height, \"\\n\\n\")\n",
    "    \n",
    "stats = []\n",
    "for item in RastersList:\n",
    "    band = item.read(1)\n",
    "    stats.append({\n",
    "        'raster': item.name,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "\n",
    "# Show stats for each channel\n",
    "print(\"\\n\", stats)\n",
    "\n",
    "RastersList = None\n",
    "band = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea3639",
   "metadata": {},
   "source": [
    "##### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN QGIS FOR THIS PORTION. CODE DOCUMENTED HERE.\n",
    "\n",
    "# Due to dtype errors with both gdal and rasterio here, I decided to run the raster to polygon function in QGIS instead.\n",
    "# It is possible to run QGIS functions within a Jupyter Notebook, but I ran it within the GUI. Arc or R are other options.\n",
    "# Command line code here.\n",
    "\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.tif','BAND':1,'FIELD':'DN','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Settlement/GRID3_ADM.shp'})\n",
    "# processing.run(\"gdal:polygonize\", {'INPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.tif','BAND':1,'FIELD':'DN','EIGHT_CONNECTEDNESS':False,'EXTRA':'','OUTPUT':'C:/Users/grace/GIS/povertyequity/urban_growth/Cameroon/Buildup/WSFE_ADM.shp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03937b47",
   "metadata": {},
   "source": [
    "##### Vector math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb900b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 210151 entries, 0 to 210150\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   DN        210151 non-null  int64   \n",
      " 1   geometry  210151 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 3.2 MB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 584286 entries, 0 to 584285\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   DN        584286 non-null  int64   \n",
      " 1   geometry  584286 non-null  geometry\n",
      "dtypes: geometry(1), int64(1)\n",
      "memory usage: 8.9 MB\n",
      "None \n",
      "\n",
      "                DN                                           geometry\n",
      "175106   20656292  POLYGON ((-1431623.160 501611.765, -1431425.43...\n",
      "184587   26535297  POLYGON ((-1395382.057 469966.025, -1395269.06...\n",
      "14801   180287243  POLYGON ((-1160761.161 1258991.379, -1160676.4...\n",
      "32280   161937240  POLYGON ((-1173641.865 1185425.308, -1173557.1...\n",
      "54080   139068214  POLYGON ((-1145733.673 1024288.108, -1145677.1...\n",
      "139285   53893331  POLYGON ((-1563480.895 646783.831, -1563424.40...\n",
      "30128   165961250  POLYGON ((-1070963.446 1191779.747, -1070850.4...\n",
      "139422   53876301  POLYGON ((-1558283.417 646404.462, -1558198.67...\n",
      "24191   167929253  POLYGON ((-1077629.775 1214415.462, -1077601.5...\n",
      "4630    190081277  POLYGON ((-1119605.052 1314821.925, -1119492.0... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]] \n",
      "\n",
      " None \n",
      "\n",
      "              DN                                           geometry\n",
      "59537   2009230  POLYGON ((-1205984.861 1101363.347, -1205956.6...\n",
      "322680  1999318  POLYGON ((-1512381.786 623389.378, -1512353.53...\n",
      "59958   1988230  POLYGON ((-1203583.853 1101078.820, -1203555.6...\n",
      "424416  2003353  POLYGON ((-1596445.328 482674.904, -1596388.83...\n",
      "405027  2002181  POLYGON ((-1670876.590 510084.351, -1670848.34...\n",
      "10679   2006278  POLYGON ((-1130649.691 1320196.327, -1130621.4...\n",
      "149250  2015299  POLYGON ((-1504585.570 693794.036, -1504557.32...\n",
      "67006   1990236  POLYGON ((-1174941.235 1093270.131, -1174912.9...\n",
      "372541  2012118  POLYGON ((-1585880.891 552984.720, -1585852.64...\n",
      "246305  2000302  POLYGON ((-1545374.466 654055.080, -1545346.21... \n",
      "\n",
      " PROJCS[\"Africa_Albers_Equal_Area_Conic\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",0],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",20],PARAMETER[\"standard_parallel_2\",-23],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"ESRI\",\"102022\"]]\n"
     ]
    }
   ],
   "source": [
    "# Load newly created vectorized datasets.\n",
    "GRID3_ADM = gpd.read_file(r\"Settlement/GRID3_ADM.shp\")\n",
    "WSFE_ADM = gpd.read_file(r\"Buildup/WSFE_ADM.shp\")\n",
    "print(GRID3_ADM.info(), \"\\n\\n\", GRID3_ADM.sample(10), \"\\n\\n\", GRID3_ADM.crs, \"\\n\\n\", \n",
    "      WSFE_ADM.info(), \"\\n\\n\", WSFE_ADM.sample(10), \"\\n\\n\", WSFE_ADM.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f41b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DN                                           geometry  \\\n",
      "42070   154698234  POLYGON ((-1201550.058 1144105.646, -1201437.0...   \n",
      "15701   175589255  POLYGON ((-1142682.980 1256240.950, -1142513.4...   \n",
      "128955   31590120  POLYGON ((-1309397.707 678461.185, -1309312.96...   \n",
      "157954   44378066  POLYGON ((-1094380.340 566199.424, -1094295.59...   \n",
      "49211   147817227  POLYGON ((-1219712.980 1082963.926, -1219628.2...   \n",
      "158862   75991064  POLYGON ((-1121328.129 561488.920, -1121243.38...   \n",
      "21702   176912242  POLYGON ((-1191098.609 1227630.166, -1191013.8...   \n",
      "53709   146907227  POLYGON ((-1228441.352 1030231.564, -1228356.6...   \n",
      "130646   77721058  POLYGON ((-1120706.691 673055.170, -1120650.19...   \n",
      "82783    89138196  POLYGON ((-1515912.680 793473.375, -1515884.43...   \n",
      "\n",
      "       gridstring   G3_ID  ADM_ID  \n",
      "42070   154698234  154698     234  \n",
      "15701   175589255  175589     255  \n",
      "128955  031590120   31590     120  \n",
      "157954  044378066   44378      66  \n",
      "49211   147817227  147817     227  \n",
      "158862  075991064   75991      64  \n",
      "21702   176912242  176912     242  \n",
      "53709   146907227  146907     227  \n",
      "130646  077721058   77721      58  \n",
      "82783   089138196   89138     196                DN                                           geometry gridstring  \\\n",
      "43345   1985268  POLYGON ((-1018141.260 1216912.977, -1018113.0...    1985268   \n",
      "90579   2004220  POLYGON ((-1187737.197 859009.458, -1187708.95...    2004220   \n",
      "253025  1985331  POLYGON ((-1557012.295 652189.847, -1556984.04...    1985331   \n",
      "537235  2003334  POLYGON ((-1291686.739 443283.703, -1291658.49...    2003334   \n",
      "485724  1985341  POLYGON ((-1396992.145 460007.576, -1396935.65...    1985341   \n",
      "98690   2003208  POLYGON ((-1119746.288 780764.497, -1119689.79...    2003208   \n",
      "297551  2000301  POLYGON ((-1553453.153 638690.615, -1553424.90...    2000301   \n",
      "577884  1998037  POLYGON ((-998340.002 307753.945, -998311.755 ...    1998037   \n",
      "219316  2011190  POLYGON ((-1613732.589 663476.090, -1613704.34...    2011190   \n",
      "73894   1999227  POLYGON ((-1196296.086 1049579.409, -1196267.8...    1999227   \n",
      "\n",
      "        year  ADM_ID  \n",
      "43345   1985     268  \n",
      "90579   2004     220  \n",
      "253025  1985     331  \n",
      "537235  2003     334  \n",
      "485724  1985     341  \n",
      "98690   2003     208  \n",
      "297551  2000     301  \n",
      "577884  1998      37  \n",
      "219316  2011     190  \n",
      "73894   1999     227  \n"
     ]
    }
   ],
   "source": [
    "# Split serial back into separate dataset fields.\n",
    "# For Cameroon: WSFE and ADM: 4+3=7 digits. GRID3 and ADM: 6+3=9 digits.\n",
    "GRID3_ADM['gridstring'] = GRID3_ADM['DN'].astype(str).str.zfill(9)\n",
    "WSFE_ADM['gridstring'] = WSFE_ADM['DN'].astype(str).str.zfill(7)\n",
    "\n",
    "GRID3_ADM['G3_ID'] = GRID3_ADM['gridstring'].str[:-3].astype(int) # Remove the last 4 digits to get the GRID3 portion.\n",
    "GRID3_ADM['ADM_ID'] = GRID3_ADM['gridstring'].str[-3:].astype(int) # Keep only the last 4 digits to get the ADM portion.\n",
    "WSFE_ADM['year'] = WSFE_ADM['gridstring'].str[:-3].astype(int)\n",
    "WSFE_ADM['ADM_ID'] = WSFE_ADM['gridstring'].str[-3:].astype(int)\n",
    "\n",
    "print(GRID3_ADM.sample(10), WSFE_ADM.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3127bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 205788 entries, 0 to 205787\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype   \n",
      "---  ------      --------------   -----   \n",
      " 0   G3_ID       205788 non-null  int64   \n",
      " 1   ADM_ID      205788 non-null  int64   \n",
      " 2   geometry    205788 non-null  geometry\n",
      " 3   DN          205788 non-null  int64   \n",
      " 4   gridstring  205788 non-null  object  \n",
      "dtypes: geometry(1), int64(3), object(1)\n",
      "memory usage: 7.9+ MB\n",
      "None    G3_ID  ADM_ID                                           geometry    DN  \\\n",
      "0      1     354  POLYGON ((-1571983.289 271555.771, -1571870.30...  1354   \n",
      "1      2     354  POLYGON ((-1572915.445 272093.211, -1572802.45...  2354   \n",
      "2      3     354  POLYGON ((-1571390.099 273768.760, -1571361.85...  3354   \n",
      "3      4     354  POLYGON ((-1575062.229 274274.586, -1574949.24...  4354   \n",
      "4      5     354  POLYGON ((-1573395.647 277214.700, -1573226.16...  5354   \n",
      "\n",
      "  gridstring  \n",
      "0  000001354  \n",
      "1  000002354  \n",
      "2  000003354  \n",
      "3  000004354  \n",
      "4  000005354  \n"
     ]
    }
   ],
   "source": [
    "# Dissolve any features that have the same G3 and ADM values so that we have a single unique feature per settlement.\n",
    "# Note: we do NOT want to dissolve the WSFE features. Distinct features for noncontiguous builtup areas of the same year is necessary to separate them in the Near tool step.\n",
    "GRID3_ADM = GRID3_ADM.dissolve(by=['G3_ID', 'ADM_ID'], as_index=False)\n",
    "print(GRID3_ADM.info(), GRID3_ADM.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f172cebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: WSFE (584286, 5) and GRID3 (205788, 5)\n",
      "\n",
      "After: WSFE (584286, 5) and GRID3 (205788, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove features where year, settlement, or admin area = 0.\n",
    "# This was supposed to be resolved earlier with the gdal_calc NoDataValue parameter, but guess it didn't work.\n",
    "\n",
    "print(\"Before: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))\n",
    "WSFE_ADM = WSFE_ADM.loc[(WSFE_ADM[\"year\"] != 0) & (WSFE_ADM[\"ADM_ID\"] != 0)] # Since we change the datatype to integer, no need to include all digits. Otherwise, it would need to be: != '0000'\n",
    "GRID3_ADM = GRID3_ADM.loc[(GRID3_ADM[\"G3_ID\"] != 0) & (GRID3_ADM[\"ADM_ID\"] != 0)]\n",
    "print(\"After: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sett_ID is our new unique settlement identifier for subsequent matching steps.\n",
    "GRID3_ADM['Sett_ID'] = GRID3_ADM.index\n",
    "WSFE_ADM['WSFE_ID'] = WSFE_ADM.index\n",
    "GRID3_ADM = GRID3_ADM[['G3_ID', 'Sett_ID', 'ADM_ID', 'geometry']]\n",
    "WSFE_ADM = WSFE_ADM[['year', 'ADM_ID', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f0a8e",
   "metadata": {},
   "source": [
    "### 4. UNIQUE SETTLEMENTS FROM WSFE AND GRID3\n",
    "NEAR JOIN: Join GRID3 ID onto WSFE by spatial (within a distance) and attribute matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b111165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# The sharding step below doesn't work if any ADM group contains features from only one of the two datasets.\n",
    "WSFE_u = sorted(WSFE_ADM.ADM_ID.unique().tolist())\n",
    "GRID3_u = sorted(GRID3_ADM.ADM_ID.unique().tolist())\n",
    "\n",
    "not_matching = list(set(GRID3_u).symmetric_difference(set(WSFE_u)))\n",
    "print(not_matching) # Validate: If there are many ADM_IDs in this list, investigate why GRID3 or WSFE is missing in so many areas.\n",
    "\n",
    "# Take only the features that share an ADM with at least one GRID3 feature.\n",
    "WSFE_matching = WSFE_ADM[~WSFE_ADM[\"ADM_ID\"].isin(not_matching)] \n",
    "GRID3_matching = GRID3_ADM[~GRID3_ADM[\"ADM_ID\"].isin(not_matching)]\n",
    "\n",
    "WSFE_u = sorted(WSFE_matching.ADM_ID.unique().tolist())\n",
    "GRID3_u = sorted(GRID3_matching.ADM_ID.unique().tolist())\n",
    "\n",
    "not_matching = list(set(GRID3_u).symmetric_difference(set(WSFE_u)))\n",
    "print(not_matching) # This should now be empty.\n",
    "\n",
    "del WSFE_u, GRID3_u, not_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0979d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ADM_ID_left</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>G3_ID</th>\n",
       "      <th>Sett_ID</th>\n",
       "      <th>ADM_ID_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <th>24104</th>\n",
       "      <td>2014</td>\n",
       "      <td>243</td>\n",
       "      <td>POLYGON ((-1166467.087 1263164.444, -1166438.8...</td>\n",
       "      <td>178165.0</td>\n",
       "      <td>174895.0</td>\n",
       "      <td>178165.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <th>223054</th>\n",
       "      <td>1998</td>\n",
       "      <td>287</td>\n",
       "      <td>POLYGON ((-1541391.617 662211.525, -1541363.37...</td>\n",
       "      <td>57832.0</td>\n",
       "      <td>56244.0</td>\n",
       "      <td>57832.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <th>269967</th>\n",
       "      <td>2010</td>\n",
       "      <td>138</td>\n",
       "      <td>POLYGON ((-1494670.817 646973.516, -1494642.57...</td>\n",
       "      <td>70562.0</td>\n",
       "      <td>68641.0</td>\n",
       "      <td>70562.0</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <th>325358</th>\n",
       "      <td>2007</td>\n",
       "      <td>318</td>\n",
       "      <td>POLYGON ((-1512805.493 621808.672, -1512777.24...</td>\n",
       "      <td>50516.0</td>\n",
       "      <td>49133.0</td>\n",
       "      <td>50516.0</td>\n",
       "      <td>318.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <th>391614</th>\n",
       "      <td>1985</td>\n",
       "      <td>124</td>\n",
       "      <td>POLYGON ((-1422019.127 534332.386, -1421990.88...</td>\n",
       "      <td>64837.0</td>\n",
       "      <td>63066.0</td>\n",
       "      <td>64837.0</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <th>271587</th>\n",
       "      <td>1999</td>\n",
       "      <td>302</td>\n",
       "      <td>POLYGON ((-1548820.619 646625.761, -1548792.37...</td>\n",
       "      <td>57836.0</td>\n",
       "      <td>56244.0</td>\n",
       "      <td>57836.0</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <th>4813</th>\n",
       "      <td>1985</td>\n",
       "      <td>281</td>\n",
       "      <td>POLYGON ((-1060173.031 1429612.816, -1060144.7...</td>\n",
       "      <td>195068.0</td>\n",
       "      <td>191350.0</td>\n",
       "      <td>195068.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <th>235980</th>\n",
       "      <td>1985</td>\n",
       "      <td>287</td>\n",
       "      <td>POLYGON ((-1540628.944 657880.389, -1540600.69...</td>\n",
       "      <td>57832.0</td>\n",
       "      <td>56244.0</td>\n",
       "      <td>57832.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <th>500283</th>\n",
       "      <td>1995</td>\n",
       "      <td>324</td>\n",
       "      <td>POLYGON ((-1396003.494 453747.979, -1395975.24...</td>\n",
       "      <td>19745.0</td>\n",
       "      <td>19120.0</td>\n",
       "      <td>19745.0</td>\n",
       "      <td>324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <th>406079</th>\n",
       "      <td>2011</td>\n",
       "      <td>173</td>\n",
       "      <td>POLYGON ((-1655425.394 506385.498, -1655397.14...</td>\n",
       "      <td>46476.0</td>\n",
       "      <td>45191.0</td>\n",
       "      <td>46476.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            year  ADM_ID_left  \\\n",
       "240 24104   2014          243   \n",
       "284 223054  1998          287   \n",
       "136 269967  2010          138   \n",
       "315 325358  2007          318   \n",
       "122 391614  1985          124   \n",
       "299 271587  1999          302   \n",
       "278 4813    1985          281   \n",
       "284 235980  1985          287   \n",
       "321 500283  1995          324   \n",
       "171 406079  2011          173   \n",
       "\n",
       "                                                     geometry  index_right  \\\n",
       "240 24104   POLYGON ((-1166467.087 1263164.444, -1166438.8...     178165.0   \n",
       "284 223054  POLYGON ((-1541391.617 662211.525, -1541363.37...      57832.0   \n",
       "136 269967  POLYGON ((-1494670.817 646973.516, -1494642.57...      70562.0   \n",
       "315 325358  POLYGON ((-1512805.493 621808.672, -1512777.24...      50516.0   \n",
       "122 391614  POLYGON ((-1422019.127 534332.386, -1421990.88...      64837.0   \n",
       "299 271587  POLYGON ((-1548820.619 646625.761, -1548792.37...      57836.0   \n",
       "278 4813    POLYGON ((-1060173.031 1429612.816, -1060144.7...     195068.0   \n",
       "284 235980  POLYGON ((-1540628.944 657880.389, -1540600.69...      57832.0   \n",
       "321 500283  POLYGON ((-1396003.494 453747.979, -1395975.24...      19745.0   \n",
       "171 406079  POLYGON ((-1655425.394 506385.498, -1655397.14...      46476.0   \n",
       "\n",
       "               G3_ID   Sett_ID  ADM_ID_right  \n",
       "240 24104   174895.0  178165.0         243.0  \n",
       "284 223054   56244.0   57832.0         287.0  \n",
       "136 269967   68641.0   70562.0         138.0  \n",
       "315 325358   49133.0   50516.0         318.0  \n",
       "122 391614   63066.0   64837.0         124.0  \n",
       "299 271587   56244.0   57836.0         302.0  \n",
       "278 4813    191350.0  195068.0         281.0  \n",
       "284 235980   56244.0   57832.0         287.0  \n",
       "321 500283   19120.0   19745.0         324.0  \n",
       "171 406079   45191.0   46476.0         173.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shard the dataframe whose variables we want to join into a dict\n",
    "shards = {k:d for k, d in GRID3_matching.groupby('ADM_ID', as_index=False)}\n",
    "\n",
    "# Take the dataframe whose geometry we want to retain.\n",
    "# Group by ADM, then sjoin_nearest among the smaller dataframe's matching ADM shard\n",
    "WSFE_GRID3 = WSFE_matching.groupby('ADM_ID', as_index=False).apply(\n",
    "    lambda d: gpd.sjoin_nearest(\n",
    "    d, shards[d['ADM_ID'].values[0]], \n",
    "        how='left', \n",
    "        max_distance=500))\n",
    "\n",
    "WSFE_GRID3.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd1e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "MultiIndex: 586113 entries, (0, 560868) to (356, 3948)\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype   \n",
      "---  ------        --------------   -----   \n",
      " 0   year          586113 non-null  int32   \n",
      " 1   ADM_ID_left   586113 non-null  int32   \n",
      " 2   geometry      586113 non-null  geometry\n",
      " 3   index_right   581895 non-null  float64 \n",
      " 4   G3_ID         581895 non-null  float64 \n",
      " 5   Sett_ID       581895 non-null  float64 \n",
      " 6   ADM_ID_right  581895 non-null  float64 \n",
      "dtypes: float64(4), geometry(1), int32(2)\n",
      "memory usage: 50.8 MB\n"
     ]
    }
   ],
   "source": [
    "WSFE_GRID3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d22854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 86839 entries, 0 to 86838\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype   \n",
      "---  ------        --------------  -----   \n",
      " 0   year          86839 non-null  int64   \n",
      " 1   Sett_ID       86839 non-null  float64 \n",
      " 2   geometry      86839 non-null  geometry\n",
      " 3   ADM_ID_left   86839 non-null  int32   \n",
      " 4   index_right   86839 non-null  float64 \n",
      " 5   G3_ID         86839 non-null  float64 \n",
      " 6   ADM_ID_right  86839 non-null  float64 \n",
      "dtypes: float64(4), geometry(1), int32(1), int64(1)\n",
      "memory usage: 4.3 MB\n",
      "None    year  Sett_ID                                           geometry  \\\n",
      "0  1985      0.0  POLYGON ((-1572265.761 271366.086, -1572124.52...   \n",
      "1  1985      1.0  MULTIPOLYGON (((-1572802.457 271302.858, -1572...   \n",
      "2  1985      2.0  POLYGON ((-1571277.110 273737.146, -1571220.61...   \n",
      "3  1985      3.0  MULTIPOLYGON (((-1574808.005 273768.760, -1574...   \n",
      "4  1985      4.0  POLYGON ((-1573536.883 276835.330, -1573508.63...   \n",
      "\n",
      "   ADM_ID_left  index_right  G3_ID  ADM_ID_right  \n",
      "0          354          0.0    1.0         354.0  \n",
      "1          354          1.0    2.0         354.0  \n",
      "2          354          2.0    3.0         354.0  \n",
      "3          354          3.0    4.0         354.0  \n",
      "4          354          4.0    5.0         354.0  \n"
     ]
    }
   ],
   "source": [
    "# Now we can dissolve with the WSFE years, now that we can group them by their settlement ID.\n",
    "WSFE_GRID3 = WSFE_GRID3.dissolve(by=['year', 'Sett_ID'], as_index=False)\n",
    "print(WSFE_GRID3.info(), WSFE_GRID3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02684aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "WSFE_GRID3 = WSFE_GRID3.rename(columns={\"ADM_ID_left\": \"ADM_ID\"})[['ADM_ID', 'year', 'Sett_ID', 'geometry']]\n",
    "WSFE_GRID3.to_file(driver='GPKG', filename='WSFE_GRID3.gpkg', layer='WSFE_GRID3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027eba",
   "metadata": {},
   "source": [
    "### 5. CUMULATIVE ANNUALIZED SETTLEMENT EXTENTS\n",
    "DISSOLVE BY YEAR SETS: Create separate feature layers of each cumulative year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60c3aff1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WSFE_GRID3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCreateList\u001b[39m(r1, r2):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r1, r2\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m----> 4\u001b[0m CuStart, CuEnd \u001b[38;5;241m=\u001b[39m \u001b[43mWSFE_GRID3\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), WSFE_GRID3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m      5\u001b[0m StudyStart, StudyEnd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1999\u001b[39m, WSFE_GRID3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m      7\u001b[0m AllCuYears \u001b[38;5;241m=\u001b[39m CreateList(CuStart, CuEnd) \u001b[38;5;66;03m# All years in the WSFE dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WSFE_GRID3' is not defined"
     ]
    }
   ],
   "source": [
    "# WSFE_GRID3 = gpd.read_file('WSFE_GRID3.gpkg', layer='WSFE_GRID3')\n",
    "\n",
    "def CreateList(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]\n",
    "\n",
    "CuStart, CuEnd = WSFE_GRID3['year'].min(), WSFE_GRID3['year'].max()\n",
    "StudyStart, StudyEnd = 1999, WSFE_GRID3['year'].max()\n",
    "\n",
    "AllCuYears = CreateList(CuStart, CuEnd) # All years in the WSFE dataset\n",
    "AllStudyYears = CreateList(StudyStart, StudyEnd) # All years for which there will be growth stats in the present study.\n",
    "print(AllCuYears, AllStudyYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3c1be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting to cumulative area for year: 1999. Sun Nov  6 08:31:11 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:31:11 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:31:58 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2000. Sun Nov  6 08:32:14 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:32:14 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:33:11 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2001. Sun Nov  6 08:33:28 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:33:29 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:34:29 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2002. Sun Nov  6 08:34:46 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:34:46 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:35:52 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2003. Sun Nov  6 08:36:10 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:36:10 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:37:17 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2004. Sun Nov  6 08:37:33 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:37:33 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:38:43 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2005. Sun Nov  6 08:39:00 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:39:00 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:40:15 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2006. Sun Nov  6 08:40:32 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:40:32 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:41:48 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2007. Sun Nov  6 08:42:05 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:42:05 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:43:17 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2008. Sun Nov  6 08:43:35 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:43:35 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:44:59 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2009. Sun Nov  6 08:45:16 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:45:16 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:46:39 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2010. Sun Nov  6 08:46:55 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:46:55 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:48:14 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2011. Sun Nov  6 08:48:30 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:48:30 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:50:00 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2012. Sun Nov  6 08:50:16 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:50:16 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:51:49 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2013. Sun Nov  6 08:52:05 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:52:05 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:53:44 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2014. Sun Nov  6 08:53:59 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:53:59 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:55:42 2022\n",
      "\n",
      "Subsetting to cumulative area for year: 2015. Sun Nov  6 08:55:58 2022\n",
      "\n",
      "Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. Sun Nov  6 08:55:58 2022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_30068\\1773681542.py:6: FutureWarning: Boolean inputs to the `inclusive` argument are deprecated in favour of `both` or `neither`.\n",
      "  CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to file. Sun Nov  6 08:57:45 2022\n",
      "\n",
      "Done with all years in set. Sun Nov  6 08:58:01 2022\n"
     ]
    }
   ],
   "source": [
    "# For each year in the growth stats study, we are taking features from all years prior to and including that year, \n",
    "# dissolving those features, and exporting as its own file.\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYearSet = WSFE_GRID3[WSFE_GRID3['year'].between(CuStart, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    CuYearDissolve = CuYearSet.dissolve(by='Sett_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\"}, # Though ADM_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    CuYearName = ''.join(['Cu', str(item)])\n",
    "    CuYearDissolve.to_file(driver='GPKG', filename='AnnualSettlements.gpkg', layer=CuYearName)\n",
    "    del CuYearSet, CuYearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c0303",
   "metadata": {},
   "source": [
    "##### Join area information from each cumulative layer onto the latest year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc1490af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 14468 entries, 0 to 14467\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Sett_ID   14468 non-null  float64 \n",
      " 1   year      14468 non-null  int64   \n",
      " 2   ADM_ID    14468 non-null  int64   \n",
      " 3   geometry  14468 non-null  geometry\n",
      "dtypes: float64(1), geometry(1), int64(2)\n",
      "memory usage: 452.2 KB\n"
     ]
    }
   ],
   "source": [
    "LatestYearSett = gpd.read_file('AnnualSettlements.gpkg', layer=''.join(['Cu', str(StudyEnd)]))\n",
    "LatestYearSett.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2ab4dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2014,\n",
       " 2013,\n",
       " 2012,\n",
       " 2011,\n",
       " 2010,\n",
       " 2009,\n",
       " 2008,\n",
       " 2007,\n",
       " 2006,\n",
       " 2005,\n",
       " 2004,\n",
       " 2003,\n",
       " 2002,\n",
       " 2001,\n",
       " 2000,\n",
       " 1999]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReversedStudyYears = []\n",
    "for i in AllStudyYears:\n",
    "    ReversedStudyYears.insert(0,i)\n",
    "ReversedStudyYears.remove(StudyEnd)\n",
    "ReversedStudyYears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b33dc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cumulative layer for year 2014. Sun Nov  6 09:11:52 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:11:55 2022\n",
      "\n",
      "Merging variables from 2014 onto our latest year (2015) via table join. Sun Nov  6 09:11:55 2022\n",
      "\n",
      "Loading cumulative layer for year 2013. Sun Nov  6 09:11:55 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:11:59 2022\n",
      "\n",
      "Merging variables from 2013 onto our latest year (2015) via table join. Sun Nov  6 09:11:59 2022\n",
      "\n",
      "Loading cumulative layer for year 2012. Sun Nov  6 09:11:59 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:02 2022\n",
      "\n",
      "Merging variables from 2012 onto our latest year (2015) via table join. Sun Nov  6 09:12:02 2022\n",
      "\n",
      "Loading cumulative layer for year 2011. Sun Nov  6 09:12:02 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:05 2022\n",
      "\n",
      "Merging variables from 2011 onto our latest year (2015) via table join. Sun Nov  6 09:12:05 2022\n",
      "\n",
      "Loading cumulative layer for year 2010. Sun Nov  6 09:12:05 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:09 2022\n",
      "\n",
      "Merging variables from 2010 onto our latest year (2015) via table join. Sun Nov  6 09:12:09 2022\n",
      "\n",
      "Loading cumulative layer for year 2009. Sun Nov  6 09:12:09 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:12 2022\n",
      "\n",
      "Merging variables from 2009 onto our latest year (2015) via table join. Sun Nov  6 09:12:12 2022\n",
      "\n",
      "Loading cumulative layer for year 2008. Sun Nov  6 09:12:12 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:15 2022\n",
      "\n",
      "Merging variables from 2008 onto our latest year (2015) via table join. Sun Nov  6 09:12:15 2022\n",
      "\n",
      "Loading cumulative layer for year 2007. Sun Nov  6 09:12:15 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:18 2022\n",
      "\n",
      "Merging variables from 2007 onto our latest year (2015) via table join. Sun Nov  6 09:12:18 2022\n",
      "\n",
      "Loading cumulative layer for year 2006. Sun Nov  6 09:12:18 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:22 2022\n",
      "\n",
      "Merging variables from 2006 onto our latest year (2015) via table join. Sun Nov  6 09:12:22 2022\n",
      "\n",
      "Loading cumulative layer for year 2005. Sun Nov  6 09:12:22 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:24 2022\n",
      "\n",
      "Merging variables from 2005 onto our latest year (2015) via table join. Sun Nov  6 09:12:24 2022\n",
      "\n",
      "Loading cumulative layer for year 2004. Sun Nov  6 09:12:24 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:27 2022\n",
      "\n",
      "Merging variables from 2004 onto our latest year (2015) via table join. Sun Nov  6 09:12:27 2022\n",
      "\n",
      "Loading cumulative layer for year 2003. Sun Nov  6 09:12:28 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:31 2022\n",
      "\n",
      "Merging variables from 2003 onto our latest year (2015) via table join. Sun Nov  6 09:12:31 2022\n",
      "\n",
      "Loading cumulative layer for year 2002. Sun Nov  6 09:12:31 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:34 2022\n",
      "\n",
      "Merging variables from 2002 onto our latest year (2015) via table join. Sun Nov  6 09:12:34 2022\n",
      "\n",
      "Loading cumulative layer for year 2001. Sun Nov  6 09:12:34 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:37 2022\n",
      "\n",
      "Merging variables from 2001 onto our latest year (2015) via table join. Sun Nov  6 09:12:37 2022\n",
      "\n",
      "Loading cumulative layer for year 2000. Sun Nov  6 09:12:37 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:40 2022\n",
      "\n",
      "Merging variables from 2000 onto our latest year (2015) via table join. Sun Nov  6 09:12:40 2022\n",
      "\n",
      "Loading cumulative layer for year 1999. Sun Nov  6 09:12:40 2022\n",
      "\n",
      "Adding area field and converting to non-spatial dataframe. Sun Nov  6 09:12:44 2022\n",
      "\n",
      "Merging variables from 1999 onto our latest year (2015) via table join. Sun Nov  6 09:12:44 2022\n",
      "\n",
      "Done merging annualized areas onto latest year geometries. Saving to file. Sun Nov  6 09:12:44 2022\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "str.join() takes exactly one argument (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     LatestYearSett \u001b[38;5;241m=\u001b[39m LatestYearSett\u001b[38;5;241m.\u001b[39mmerge(YearLayer, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSett_ID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone merging annualized areas onto latest year geometries. Saving to file. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mctime()))\n\u001b[1;32m---> 12\u001b[0m LatestYearSett\u001b[38;5;241m.\u001b[39mto_file(driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPKG\u001b[39m\u001b[38;5;124m'\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnualizedAreasOntoLatestYear.gpkg\u001b[39m\u001b[38;5;124m'\u001b[39m, layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAreas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStudyStart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStudyEnd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: str.join() takes exactly one argument (4 given)"
     ]
    }
   ],
   "source": [
    "# We have settlement IDs, so no need to join spatially!\n",
    "for item in ReversedStudyYears:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file('AnnualSettlements.gpkg', layer=''.join(['Cu', str(item)]))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['Area', str(item)[2:]])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Sett_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, StudyEnd, time.ctime()))\n",
    "    LatestYearSett = LatestYearSett.merge(YearLayer, how='left', on='Sett_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c896f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 14468 entries, 0 to 14467\n",
      "Data columns (total 20 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Sett_ID   14468 non-null  float64 \n",
      " 1   year      14468 non-null  int64   \n",
      " 2   ADM_ID    14468 non-null  int64   \n",
      " 3   geometry  14468 non-null  geometry\n",
      " 4   Area14    14362 non-null  float64 \n",
      " 5   Area13    14219 non-null  float64 \n",
      " 6   Area12    14086 non-null  float64 \n",
      " 7   Area11    13957 non-null  float64 \n",
      " 8   Area10    13853 non-null  float64 \n",
      " 9   Area09    13741 non-null  float64 \n",
      " 10  Area08    13671 non-null  float64 \n",
      " 11  Area07    13564 non-null  float64 \n",
      " 12  Area06    13471 non-null  float64 \n",
      " 13  Area05    13379 non-null  float64 \n",
      " 14  Area04    13296 non-null  float64 \n",
      " 15  Area03    13231 non-null  float64 \n",
      " 16  Area02    13153 non-null  float64 \n",
      " 17  Area01    13069 non-null  float64 \n",
      " 18  Area00    12972 non-null  float64 \n",
      " 19  Area99    12810 non-null  float64 \n",
      "dtypes: float64(17), geometry(1), int64(2)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "LatestYearSett.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdaccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestYearSett.to_file(driver='GPKG', filename='AnnualizedOntoLatestYear.gpkg', layer=('Areas%sto%s' % (StudyStart, StudyEnd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bb2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating buffer layer. Mon Nov  7 08:07:22 2022\n",
      "Buffer created. Mon Nov  7 08:36:58 2022\n"
     ]
    }
   ],
   "source": [
    "# Create buffer layer(s) to use as maximum distance for Near joins.\n",
    "\n",
    "Distance = 2000\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = LatestYearSett\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(make_valid).buffer(Distance) # make_valid is a workaround for any null geometries.\n",
    "BufferFileName = ''.join(['Buff', str(Distance/1000), 'km_', str(StudyEnd)])\n",
    "BufferLayer.to_file(driver='GPKG', filename='Catchment.gpkg', layer=BufferFileName)\n",
    "print('Buffer created and saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b086ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "585db18c",
   "metadata": {},
   "source": [
    "### 6. PREPARE YEARLY DATASETS\n",
    "Population, nighttime lights, and any other annualized rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd172",
   "metadata": {},
   "source": [
    "##### Population: Reproject to settlements CRS.\n",
    "##### Then reclassify so that we only need to work with cells within X distance of settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47e4f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmr_ppp_2000_UNadj.tif',\n",
       " 'cmr_ppp_2001_UNadj.tif',\n",
       " 'cmr_ppp_2002_UNadj.tif',\n",
       " 'cmr_ppp_2003_UNadj.tif',\n",
       " 'cmr_ppp_2004_UNadj.tif',\n",
       " 'cmr_ppp_2005_UNadj.tif',\n",
       " 'cmr_ppp_2006_UNadj.tif',\n",
       " 'cmr_ppp_2007_UNadj.tif',\n",
       " 'cmr_ppp_2008_UNadj.tif',\n",
       " 'cmr_ppp_2009_UNadj.tif',\n",
       " 'cmr_ppp_2010_UNadj.tif',\n",
       " 'cmr_ppp_2011_UNadj.tif',\n",
       " 'cmr_ppp_2012_UNadj.tif',\n",
       " 'cmr_ppp_2013_UNadj.tif',\n",
       " 'cmr_ppp_2014_UNadj.tif',\n",
       " 'cmr_ppp_2015_UNadj.tif']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProjCRS = gdal.WarpOptions(dstSRS='ESRI:102022')\n",
    "AnnualizedSourceFiles = [i for i in os.listdir('Population/') if i.endswith('.tif')]\n",
    "\n",
    "with fiona.open(\"Catchment.gpkg\", mode=\"r\", layer=BufferFileName) as shapefile:\n",
    "    MaskGeom = [feature[\"geometry\"] for feature in shapefile] # Identify the bounding areas of the mask.\n",
    "# Mask_out = './LatestYearBuffer.tif'\n",
    "AnnualizedSourceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d9e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block changes each annual population raster's projection (gdal.Warp()), \n",
    "# then masks it to within a specified distance of the settlements (rasterio.mask.mask()).\n",
    "\n",
    "for YearFile in AnnualizedSourceFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    TempOutputName = \"Wpop_\" + Year + \"_albers.tif\"\n",
    "    TempOutputPath = os.path.join(ProjectFolder, \"Population\", TempOutputName)\n",
    "    if exists(TempOutputPath):\n",
    "        pass\n",
    "    else:\n",
    "        # Reproject to same CRS as settlements.\n",
    "        Warp = gdal.Warp(TempOutputPath, # Where to store the warped raster\n",
    "                     InputRasterObject, # Which raster to warp\n",
    "                     format='GTiff', \n",
    "                     options=ProjCRS) # Reproject to Africa Albers Equal Area Conic\n",
    "        print('Finished gdal.Warp() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "        \n",
    "        Warp = None # Close the files\n",
    "        InputRasterObject = None\n",
    "\n",
    "        # Reclassify as nodata if outside settlement buffer zones.\n",
    "        with rasterio.open(TempOutputPath) as InputRasterObject:\n",
    "            MaskedOutputRaster, OutTransform = rasterio.mask.mask(InputRasterObject, MaskGeom) # Anything outside the mask is reclassed to the raster's NoData value.\n",
    "            OutMetaData = InputRasterObject.meta.copy()\n",
    "        print('Finished rasterio.mask.mask() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "        OutMetaData.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": MaskedOutputRaster.shape[1],\n",
    "                         \"width\": MaskedOutputRaster.shape[2],\n",
    "                         \"transform\": OutTransform})\n",
    "        FinalOutputPath = os.path.join(ProjectFolder, \"Population\", ''.join(['Masked_', Year, '.tif'])) # ''.join([r'Population/', 'Masked_', Year, '.tif']\n",
    "        with rasterio.open(FinalOutputPath), \"w\", **OutMetaData) as dest:\n",
    "            dest.write(MaskedOutputRaster)\n",
    "        print('Written to file. %s \\n' % time.ctime())\n",
    "    InputRasterObject = None\n",
    "    \n",
    "    try:  # Finally, remove the intermediate file from disk\n",
    "        os.remove(TempOutputPath)\n",
    "    except OSError:\n",
    "        pass\n",
    "    print('Removed intermediate file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished all years in list. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0f89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cmr_ppp_2000_UNadj.tif', 'cmr_ppp_2001_UNadj.tif', 'cmr_ppp_2002_UNadj.tif', 'cmr_ppp_2003_UNadj.tif', 'cmr_ppp_2004_UNadj.tif', 'cmr_ppp_2005_UNadj.tif', 'cmr_ppp_2006_UNadj.tif', 'cmr_ppp_2007_UNadj.tif', 'cmr_ppp_2008_UNadj.tif', 'cmr_ppp_2009_UNadj.tif', 'cmr_ppp_2010_UNadj.tif', 'cmr_ppp_2011_UNadj.tif', 'cmr_ppp_2012_UNadj.tif', 'cmr_ppp_2013_UNadj.tif', 'cmr_ppp_2014_UNadj.tif', 'cmr_ppp_2015_UNadj.tif', 'Masked_2000.tif', 'Masked_2001.tif', 'Masked_2002.tif', 'Masked_2003.tif', 'Masked_2004.tif', 'Masked_2005.tif', 'Masked_2006.tif', 'Masked_2007.tif', 'Masked_2008.tif', 'Masked_2009.tif', 'Masked_2010.tif', 'Masked_2011.tif', 'Masked_2012.tif', 'Masked_2013.tif', 'Masked_2014.tif', 'Masked_2015.tif']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('Population/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14670a5b",
   "metadata": {},
   "source": [
    "##### Convert each annualized raster to .xyz where cell centers are stored as x and y. Similar to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de62e82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Masked_2000.tif',\n",
       " 'Masked_2001.tif',\n",
       " 'Masked_2002.tif',\n",
       " 'Masked_2003.tif',\n",
       " 'Masked_2004.tif',\n",
       " 'Masked_2005.tif',\n",
       " 'Masked_2006.tif',\n",
       " 'Masked_2007.tif',\n",
       " 'Masked_2008.tif',\n",
       " 'Masked_2009.tif',\n",
       " 'Masked_2010.tif',\n",
       " 'Masked_2011.tif',\n",
       " 'Masked_2012.tif',\n",
       " 'Masked_2013.tif',\n",
       " 'Masked_2014.tif',\n",
       " 'Masked_2015.tif']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnnualizedMaskedFiles = [i for i in os.listdir('Population/') if i.startswith('Masked') and i.endswith('.tif')]\n",
    "AnnualizedMaskedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a540c3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished gdal.Translate() for year 2000. Thu Nov 10 12:05:09 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2001. Thu Nov 10 12:07:38 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2002. Thu Nov 10 12:32:41 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2003. Thu Nov 10 12:57:17 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2004. Thu Nov 10 13:21:25 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2005. Thu Nov 10 13:45:36 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2006. Thu Nov 10 14:10:00 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2007. Thu Nov 10 14:36:04 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2008. Thu Nov 10 15:02:07 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2009. Thu Nov 10 15:27:57 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2010. Thu Nov 10 15:53:31 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2011. Thu Nov 10 16:19:13 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2012. Thu Nov 10 16:58:09 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2013. Thu Nov 10 17:23:53 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2014. Thu Nov 10 17:49:25 2022 \n",
      "\n",
      "Finished gdal.Translate() for year 2015. Thu Nov 10 18:14:58 2022 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for YearFile in AnnualizedMaskedFiles:\n",
    "    InputRasterName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputRasterObject = gdal.Open(InputRasterName)\n",
    "    XYZOutputPath = r'Population/{}'.format(\n",
    "        YearFile.replace('.tif', '.xyz')) # New file path will be the same as original, but .tif is replaced with .xyz\n",
    "    \n",
    "    # Create an .xyz version of the .tif\n",
    "    XYZ = gdal.Translate(XYZOutputPath, # Specify a destination path\n",
    "                         InputRasterObject, # Input is the masked .tif file\n",
    "                         format='XYZ', \n",
    "                         creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "    print('Finished gdal.Translate() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    InputRasterObject = None # Close the files\n",
    "    XYZ = None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb33123",
   "metadata": {},
   "source": [
    "##### Then we create spatial objects (gdf) from the x,y fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0a48cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Masked_2000.xyz',\n",
       " 'Masked_2001.xyz',\n",
       " 'Masked_2002.xyz',\n",
       " 'Masked_2003.xyz',\n",
       " 'Masked_2004.xyz',\n",
       " 'Masked_2005.xyz',\n",
       " 'Masked_2006.xyz',\n",
       " 'Masked_2007.xyz',\n",
       " 'Masked_2008.xyz',\n",
       " 'Masked_2009.xyz',\n",
       " 'Masked_2010.xyz',\n",
       " 'Masked_2011.xyz',\n",
       " 'Masked_2012.xyz',\n",
       " 'Masked_2013.xyz',\n",
       " 'Masked_2014.xyz',\n",
       " 'Masked_2015.xyz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will remove all vectorized cells that contain the raster's NoData value before Near joining to settlements.\n",
    "NoDataVal = -99999 \n",
    "\n",
    "# If starting script from this section:\n",
    "ADM_vec = gpd.read_file(glob.glob('ADM/*.shp')[0])[['geometry']].to_crs(\"ESRI:102022\")\n",
    "ADM_vec['ADM_ID'] = ADM_vec.index\n",
    "\n",
    "AnnualizedXYZFiles = [i for i in os.listdir('Population/') if i.startswith('Masked') and i.endswith('.xyz')]\n",
    "AnnualizedXYZFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f292f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XYZ file as a pandas dataframe, year 2000. Thu Nov 10 18:17:54 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2000. Thu Nov 10 18:17:57 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2000. Thu Nov 10 18:38:00 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2001. Thu Nov 10 18:40:56 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2001. Thu Nov 10 18:41:01 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2001. Thu Nov 10 19:01:12 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2002. Thu Nov 10 19:04:09 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2002. Thu Nov 10 19:04:13 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2002. Thu Nov 10 19:24:28 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2003. Thu Nov 10 19:27:25 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2003. Thu Nov 10 19:27:30 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2003. Thu Nov 10 19:47:41 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2004. Thu Nov 10 19:50:38 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2004. Thu Nov 10 19:50:43 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2004. Thu Nov 10 20:10:54 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2005. Thu Nov 10 20:13:52 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2005. Thu Nov 10 20:13:57 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2005. Thu Nov 10 20:34:11 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2006. Thu Nov 10 20:37:10 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2006. Thu Nov 10 20:37:15 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2006. Thu Nov 10 20:57:30 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2007. Thu Nov 10 21:00:31 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2007. Thu Nov 10 21:00:36 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2007. Thu Nov 10 21:21:09 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2008. Thu Nov 10 21:24:12 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2008. Thu Nov 10 21:24:17 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2008. Thu Nov 10 21:44:40 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2009. Thu Nov 10 21:47:41 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2009. Thu Nov 10 21:47:46 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2009. Thu Nov 10 22:08:00 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2010. Thu Nov 10 22:11:11 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2010. Thu Nov 10 22:11:16 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2010. Thu Nov 10 22:31:35 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2011. Thu Nov 10 22:34:38 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2011. Thu Nov 10 22:34:43 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2011. Thu Nov 10 22:55:04 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2012. Thu Nov 10 22:58:11 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2012. Thu Nov 10 22:58:16 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2012. Thu Nov 10 23:18:38 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2013. Thu Nov 10 23:21:42 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2013. Thu Nov 10 23:21:47 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2013. Thu Nov 10 23:42:03 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2014. Thu Nov 10 23:45:10 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2014. Thu Nov 10 23:45:15 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2014. Fri Nov 11 00:05:35 2022 \n",
      "\n",
      "Loaded XYZ file as a pandas dataframe, year 2015. Fri Nov 11 00:08:41 2022 \n",
      "\n",
      "Created geodataframe from non-NoData points, year 2015. Fri Nov 11 00:08:46 2022 \n",
      "\n",
      "Exported as geopackage layer, year 2015. Fri Nov 11 00:29:07 2022 \n",
      "\n",
      "\n",
      " \n",
      " Finished generating raster value points within settlement catchments, all years. Fri Nov 11 00:29:07 2022\n"
     ]
    }
   ],
   "source": [
    "for YearFile in AnnualizedXYZFiles:\n",
    "    InputXYZName = os.path.join(ProjectFolder, \"Population\", YearFile)\n",
    "    Year = str(re.sub(r'[^0-9]', '', YearFile))\n",
    "    InputXYZ = pd.read_table(InputXYZName, delim_whitespace=True)\n",
    "    InputXYZ = InputXYZ.loc[InputXYZ['Z'] != NoDataVal] # Subset to only the features that have a raster value.\n",
    "    print('Loaded XYZ file as a pandas dataframe, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    OutputSpatialObject = gpd.GeoDataFrame(InputXYZ, \n",
    "                                           geometry = gpd.points_from_xy(InputXYZ['X'], InputXYZ['Y']), \n",
    "                                           crs = 'ESRI:102022')\n",
    "    print('Created geodataframe from non-NoData points, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    # Add ADM field so that we can group_by in the sharding, next section.\n",
    "    OutputSpatialObject = gpd.sjoin(OutputSpatialObject, ADM_vec, how='left', predicate='within') #, lsuffix=\"G3\", rsuffix=\"GN\")\n",
    "    \n",
    "    OutputVectorName = YearFile.replace('.xyz', '')\n",
    "    OutputSpatialObject.to_file(driver='GPKG', filename='Population/PopulationWithinBuffer.gpkg', layer=OutputVectorName)\n",
    "    print('Exported as geopackage layer, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "#   # The XYZ files are very large and unnecessary now that we have point objects in a geopackage. Removing XYZ file.\n",
    "#     try:  \n",
    "#         os.remove(InputXYZName)\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     print('Removed intermediate XYZ file. %s \\n' % time.ctime())\n",
    "\n",
    "print('\\n \\n Finished generating raster value points within settlement catchments, all years. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c05347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e13fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b12174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e1624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ac156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a3f13a",
   "metadata": {},
   "source": [
    "### 7. JOIN RASTER DATA BY SETTLEMENT GROUP AND PERFORM SUMMARY STATS\n",
    "Merge settlement ID onto the raster data that we vectorized in previous section, and perform settlement summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37daf832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n"
     ]
    }
   ],
   "source": [
    "LatestYearSett = gpd.read_file('AnnualSettlements.gpkg', layer='Cu2015')\n",
    "\n",
    "def CreateList(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]\n",
    "\n",
    "AllValYears = CreateList(2000, 2015) # All years for which there will be growth stats in the present study.\n",
    "print(AllValYears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fb1680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 14468 entries, 0 to 14467\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Sett_ID   14468 non-null  float64 \n",
      " 1   year      14468 non-null  int64   \n",
      " 2   ADM_ID    14468 non-null  int64   \n",
      " 3   geometry  14468 non-null  geometry\n",
      "dtypes: float64(1), geometry(1), int64(2)\n",
      "memory usage: 452.2 KB\n"
     ]
    }
   ],
   "source": [
    "LatestYearSett.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3600b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [2000, 2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sharding step below doesn't work if any ADM group contains features from only one of the two datasets.\n",
    "\n",
    "for item in test:\n",
    "    Year = str(item)\n",
    "    ValFileName = ''.join(['Masked_', str(item)])\n",
    "    ValObject = gpd.read_file('Population/PopulationWithinBuffer.gpkg', layer=ValFileName)\n",
    "    Val_u = sorted(ValObject.ADM_ID.unique().tolist())\n",
    "    Sett_u = sorted(LatestYearSett.ADM_ID.unique().tolist())\n",
    "    \n",
    "    not_matching = list(set(Sett_u).symmetric_difference(set(Val_u)))\n",
    "    print('Year: %s. List of admin area IDs where one dataset has values and the other does not. %s \\n' % (Year, time.ctime()))\n",
    "    print(not_matching)\n",
    "\n",
    "    # Take only the features that share ADMs.\n",
    "    Sett_matching = LatestYearSett[~LatestYearSett[\"ADM_ID\"].isin(not_matching)] \n",
    "    Val_matching = ValObject[~ValObject[\"ADM_ID\"].isin(not_matching)]\n",
    "\n",
    "    Sett_u = sorted(Sett_matching.ADM_ID.unique().tolist())\n",
    "    Val_u = sorted(Val_matching.ADM_ID.unique().tolist())\n",
    "\n",
    "    not_matching = list(set(Sett_u).symmetric_difference(set(Val_u)))\n",
    "    print('\\n Year: %s. If the below list is NOT empty, something went wrong. End process and inspect. %s \\n' % (Year, time.ctime()))\n",
    "    print(not_matching) # This should now be empty.\n",
    "\n",
    "    del Val_u, Sett_u, not_matching\n",
    "    \n",
    "    # Now to create the shards and group_by, and perform near join.\n",
    "    # Shard the dataframe whose variables we want to join into a dict\n",
    "    shards = {k:d for k, d in Sett_matching.groupby('ADM_ID', as_index=False)}\n",
    "    print('Created shards for settlements in year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "    # Take the dataframe whose geometry we want to retain.\n",
    "    # Group by ADM, then sjoin_nearest among the smaller dataframe's matching ADM shard\n",
    "    ValObject_withID = Val_matching.groupby('ADM_ID', as_index=False).apply(\n",
    "        lambda d: gpd.sjoin_nearest(\n",
    "        d, shards[d['ADM_ID'].values[0]], \n",
    "            how='left')) # No need for max_distance parameter this time. We've already narrowed down to nearby raster cells.\n",
    "    \n",
    "    print('Joined settlement ID onto vectorized raster cells for year %s. %s \\n' % (Year, time.ctime()))\n",
    "    print(ValObject_withID.sample(10))\n",
    "    \n",
    "    # We no longer need the spatial information of the raster values because we have their unique settlement ID.\n",
    "    ValObject_withID = pd.DataFrame(ValObject_withID).drop(columns='geometry')\n",
    "    \n",
    "    OutputTableName = ''.join([ValFileName, '_table'])\n",
    "    ValObject_withID.to_file(driver='GPKG', filename='Population/PopulationWithinBuffer.gpkg', layer=OutputTableName)\n",
    "    print('Exported as table, year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "print('\\n \\n Finished generating raster value points within settlement catchments, all years. %s' % time.ctime())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pts_summarized_YY['PopSumYY'] = Pts.groupby('Sett_ID', as_index=False).apply(sum(Pts['Val']))\n",
    "Year2010 = YearFile.groupby('Sett_ID', as_index =False)['Z'].sum()\n",
    "# rename Year2010['sum'] as 'PopSum10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllSummaries = merge(Pts_summarized_YY, Pts_summarized_YY+1, by='Sett_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9686b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LatestYearSett = merge(LatestYearSett, AllSummaries, by='Sett_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65795adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7defd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c70c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb3c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a5c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36234c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2908f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a24ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e33ce84a",
   "metadata": {},
   "source": [
    "### 8. ADD NAMES ACCORDING TO LATEST YEAR CATCHMENTS\n",
    "JOIN FEATURES: UCDB, Africapolis, and GeoNames onto the new settlement vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456bf69",
   "metadata": {},
   "source": [
    "##### Place names: Three options to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "612f815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 13135 entries, 0 to 13134\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   UCDB_Name  13135 non-null  object  \n",
      " 1   geometry   13135 non-null  geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 7720 entries, 0 to 7719\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   Afpl_Name  7720 non-null   object  \n",
      " 1   geometry   7720 non-null   geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 120.8+ KB\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 199390 entries, 0 to 199389\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype   \n",
      "---  ------    --------------   -----   \n",
      " 0   GeoName   199390 non-null  object  \n",
      " 1   geometry  199390 non-null  geometry\n",
      "dtypes: geometry(1), object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None \n",
      "\n",
      "\n",
      " None \n",
      "\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# Load, pull name field, rename, and reproject to match the catchments CRS.\n",
    "UCDB = gpd.read_file('PlaceName/GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_2.gpkg', layer=0)[['UC_NM_MN', 'geometry']].rename(columns={\"UC_NM_MN\": \"UCDB_Name\"}).to_crs(\"ESRI:102022\")\n",
    "Africapolis = gpd.read_file('PlaceName/AFRICAPOLIS2020.shp')[['agglosName', 'geometry']].rename(columns={\"agglosName\": \"Afpl_Name\"}).to_crs(\"ESRI:102022\")\n",
    "GeoNames = gpd.read_file('PlaceName/GeoNames.gpkg', layer=0)[['GeoName', 'geometry']].to_crs(\"ESRI:102022\")\n",
    "print(UCDB.info(), \"\\n\\n\\n\", Africapolis.info(), \"\\n\\n\\n\", GeoNames.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "106f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestYearCatchments = gpd.sjoin(LatestYearCatchments, GeoNames, how='left', predicate='contains', lsuffix=\"G3\", rsuffix=\"GN\") # Name file is point type, so we can do contain.\n",
    "LatestYearCatchments = gpd.sjoin(LatestYearCatchments, Africapolis, how='left', predicate='intersects', lsuffix=\"G3\", rsuffix=\"Af\") # Name file is polygon type.\n",
    "LatestYearCatchments = gpd.sjoin(LatestYearCatchments, UCDB, how='left', predicate='intersects', lsuffix=\"G3\", rsuffix=\"UC\") # Name file is polygon type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e2f0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 86897 entries, 0 to 86838\n",
      "Data columns (total 10 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   ADM_ID     86897 non-null  int32   \n",
      " 1   year       86897 non-null  int64   \n",
      " 2   Sett_ID    86897 non-null  float64 \n",
      " 3   geometry   86897 non-null  geometry\n",
      " 4   index_GN   117 non-null    float64 \n",
      " 5   GeoName    117 non-null    object  \n",
      " 6   index_Af   13566 non-null  float64 \n",
      " 7   Afpl_Name  13566 non-null  object  \n",
      " 8   index_UC   3052 non-null   float64 \n",
      " 9   UCDB_Name  3052 non-null   object  \n",
      "dtypes: float64(4), geometry(1), int32(1), int64(1), object(3)\n",
      "memory usage: 7.0+ MB\n",
      "None \n",
      "\n",
      "        ADM_ID  year   Sett_ID  \\\n",
      "82120      78  2015    1605.0   \n",
      "29947     148  2000   93736.0   \n",
      "74968     179  2013   49810.0   \n",
      "12179      79  1985  202546.0   \n",
      "69119      72  2012    4346.0   \n",
      "25591     137  1999   70818.0   \n",
      "56776     241  2008  178146.0   \n",
      "73929     125  2013   13563.0   \n",
      "51823     179  2007   49796.0   \n",
      "28374     173  2000   46484.0   \n",
      "\n",
      "                                                geometry  index_GN GeoName  \\\n",
      "82120  MULTIPOLYGON (((-1418121.019 254104.774, -1418...       NaN     NaN   \n",
      "29947  MULTIPOLYGON (((-1486196.670 740741.013, -1486...       NaN     NaN   \n",
      "74968  MULTIPOLYGON (((-1597377.484 601796.930, -1597...       NaN     NaN   \n",
      "12179  MULTIPOLYGON (((-1409957.590 408413.322, -1409...       NaN     NaN   \n",
      "69119  MULTIPOLYGON (((-1316148.778 361276.660, -1316...       NaN     NaN   \n",
      "25591  MULTIPOLYGON (((-1453401.719 639101.599, -1453...       NaN     NaN   \n",
      "56776  POLYGON ((-1171608.070 1210558.538, -1171579.8...       NaN     NaN   \n",
      "73929  MULTIPOLYGON (((-1576954.789 602808.582, -1576...       NaN     NaN   \n",
      "51823  MULTIPOLYGON (((-1607348.731 597434.181, -1607...       NaN     NaN   \n",
      "28374  MULTIPOLYGON (((-1649945.446 515743.280, -1649...       NaN     NaN   \n",
      "\n",
      "       index_Af                Afpl_Name  index_UC UCDB_Name  \n",
      "82120    3349.0  Ebebiyin/Kye Ossi [GNQ]       NaN       NaN  \n",
      "29947       NaN                      NaN       NaN       NaN  \n",
      "74968       NaN                      NaN       NaN       NaN  \n",
      "12179       NaN                      NaN       NaN       NaN  \n",
      "69119       NaN                      NaN       NaN       NaN  \n",
      "25591       NaN                      NaN       NaN       NaN  \n",
      "56776       NaN                      NaN       NaN       NaN  \n",
      "73929       NaN                      NaN       NaN       NaN  \n",
      "51823       NaN                      NaN       NaN       NaN  \n",
      "28374       NaN                      NaN       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "print(LatestYearCatchments.info(), \"\\n\\n\", LatestYearCatchments.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LatestYearCatchments['GeoName'].count(), \n",
    "      LatestYearCatchments['Afpl_Name'].count(), \n",
    "      LatestYearCatchments['UCDB_Name'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LatestYearCatchments = LatestYearCatchments[['year', 'Sett_ID', 'GeoName', 'Afpl_Name', 'UCDB_Name', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single name column where non-named settlements are \"UNK\" but all others use one of the three name sources.\n",
    "LatestYearCatchments['SettName'] = \"UNK\"\n",
    "\n",
    "LatestYearCatchments.loc[\n",
    "    LatestYearCatchments['Afpl_Name'].isnan == False, \n",
    "    'SettName'] = LatestYearCatchments['Afpl_Name']\n",
    "\n",
    "LatestYearCatchments.loc[\n",
    "    LatestYearCatchments['SettName'] == \"UNK\" & LatestYearCatchments['UCDB_Name'].isnan == False, \n",
    "    'SettName'] = LatestYearCatchments['UCDB_Name']\n",
    "\n",
    "LatestYearCatchments.loc[\n",
    "    LatestYearCatchments['SettName'] == \"UNK\" & LatestYearCatchments['GeoName'].isnan == False, \n",
    "    'SettName'] = LatestYearCatchments['GeoName']\n",
    "\n",
    "LatestYearCatchments.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d202603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate file.\n",
    "LatestYearCatchments.to_file(driver='GPKG', filename='Catchment.gpkg', layer=''.join(['Catchment_', str(StudyEnd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac7167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2da017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fc33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8798b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e40dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ff5cec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd09224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing chance of null or invalid geometries. Mon Nov  7 17:47:01 2022\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sett_ID</th>\n",
       "      <th>year</th>\n",
       "      <th>ADM_ID</th>\n",
       "      <th>Area14</th>\n",
       "      <th>Area13</th>\n",
       "      <th>Area12</th>\n",
       "      <th>Area11</th>\n",
       "      <th>Area10</th>\n",
       "      <th>Area09</th>\n",
       "      <th>Area08</th>\n",
       "      <th>Area07</th>\n",
       "      <th>Area06</th>\n",
       "      <th>Area05</th>\n",
       "      <th>Area04</th>\n",
       "      <th>Area03</th>\n",
       "      <th>Area02</th>\n",
       "      <th>Area01</th>\n",
       "      <th>Area00</th>\n",
       "      <th>Area99</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>354</td>\n",
       "      <td>0.02679</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>POINT (-1572152.772 271144.788)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>354</td>\n",
       "      <td>0.02679</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>POINT (-1572181.019 271144.788)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>354</td>\n",
       "      <td>0.02679</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>POINT (-1572209.266 271144.788)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>354</td>\n",
       "      <td>0.02679</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>POINT (-1572209.266 271176.402)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>354</td>\n",
       "      <td>0.02679</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025897</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>POINT (-1572237.514 271176.402)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sett_ID  year  ADM_ID   Area14    Area13    Area12    Area11    Area10  \\\n",
       "0      0.0  2015     354  0.02679  0.025897  0.025897  0.025897  0.025897   \n",
       "0      0.0  2015     354  0.02679  0.025897  0.025897  0.025897  0.025897   \n",
       "0      0.0  2015     354  0.02679  0.025897  0.025897  0.025897  0.025897   \n",
       "0      0.0  2015     354  0.02679  0.025897  0.025897  0.025897  0.025897   \n",
       "0      0.0  2015     354  0.02679  0.025897  0.025897  0.025897  0.025897   \n",
       "\n",
       "     Area09    Area08    Area07    Area06    Area05    Area04    Area03  \\\n",
       "0  0.025897  0.025897  0.025897  0.025897  0.025004  0.024111  0.024111   \n",
       "0  0.025897  0.025897  0.025897  0.025897  0.025004  0.024111  0.024111   \n",
       "0  0.025897  0.025897  0.025897  0.025897  0.025004  0.024111  0.024111   \n",
       "0  0.025897  0.025897  0.025897  0.025897  0.025004  0.024111  0.024111   \n",
       "0  0.025897  0.025897  0.025897  0.025897  0.025004  0.024111  0.024111   \n",
       "\n",
       "     Area02    Area01    Area00    Area99                         geometry  \n",
       "0  0.024111  0.024111  0.024111  0.024111  POINT (-1572152.772 271144.788)  \n",
       "0  0.024111  0.024111  0.024111  0.024111  POINT (-1572181.019 271144.788)  \n",
       "0  0.024111  0.024111  0.024111  0.024111  POINT (-1572209.266 271144.788)  \n",
       "0  0.024111  0.024111  0.024111  0.024111  POINT (-1572209.266 271176.402)  \n",
       "0  0.024111  0.024111  0.024111  0.024111  POINT (-1572237.514 271176.402)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, doing a couple things to avoid null or invalid geometries in advance of the Voronoi generation.\n",
    "print('Reducing chance of null or invalid geometries. %s\\n' % time.ctime())\n",
    "Settlements = LatestYearSett.copy()\n",
    "Settlements = gpd.GeoDataFrame.explode(Settlements, index_parts=False)\n",
    "Settlements.geometry = Settlements.geometry.apply(lambda x: MultiPoint(list(x.exterior.coords)))\n",
    "Settlements = gpd.GeoDataFrame.explode(Settlements, index_parts=False)\n",
    "Settlements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a9edb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14468"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settlements['Sett_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce2c39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 14471 entries, 0 to 14467\n",
      "Data columns (total 20 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Sett_ID   14471 non-null  float64 \n",
      " 1   year      14471 non-null  int64   \n",
      " 2   ADM_ID    14471 non-null  int64   \n",
      " 3   Area14    14365 non-null  float64 \n",
      " 4   Area13    14222 non-null  float64 \n",
      " 5   Area12    14089 non-null  float64 \n",
      " 6   Area11    13960 non-null  float64 \n",
      " 7   Area10    13856 non-null  float64 \n",
      " 8   Area09    13744 non-null  float64 \n",
      " 9   Area08    13674 non-null  float64 \n",
      " 10  Area07    13567 non-null  float64 \n",
      " 11  Area06    13474 non-null  float64 \n",
      " 12  Area05    13382 non-null  float64 \n",
      " 13  Area04    13299 non-null  float64 \n",
      " 14  Area03    13234 non-null  float64 \n",
      " 15  Area02    13156 non-null  float64 \n",
      " 16  Area01    13072 non-null  float64 \n",
      " 17  Area00    12975 non-null  float64 \n",
      " 18  Area99    12813 non-null  float64 \n",
      " 19  geometry  14471 non-null  geometry\n",
      "dtypes: float64(17), geometry(1), int64(2)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "Settlements.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4636423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask_out = './LatestYearBuffer.tif'\n",
    "# PopName = 'Wpop_2005_albers'\n",
    "\n",
    "# with fiona.open(\"Catchment.gpkg\", mode=\"r\", layer=\"Buff_2015\") as shapefile:\n",
    "#     geoms = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "# with rasterio.open(r\"Population/{}.tif\".format(PopName)) as src:\n",
    "#     out_image, out_transform = rasterio.mask.mask(src, geoms)\n",
    "#     out_meta = src.meta.copy()\n",
    "\n",
    "# out_meta.update({\"driver\": \"GTiff\",\n",
    "#                  \"height\": out_image.shape[1],\n",
    "#                  \"width\": out_image.shape[2],\n",
    "#                  \"transform\": out_transform})\n",
    "\n",
    "# with rasterio.open(''.join(['Masked_', str(PopName), '.tif']), \"w\", **out_meta) as dest:\n",
    "#     dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     try:\n",
    "#         os.remove(r'Population/{}.csv'.format(item))\n",
    "#     except OSError:\n",
    "#         pass\n",
    "#     print('Finished os.remove() try-except, year %s. %s \\n' % (Year, time.ctime()))\n",
    "    \n",
    "    \n",
    "#     os.rename(r'Population/{}.xyz'.format(item), r'Population/{}.csv'.format(item))\n",
    "#     print('Finished os.rename(), year %s. %s \\n' % (Year, time.ctime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3543e19",
   "metadata": {},
   "source": [
    "### Associating yearly datasets. (current notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a676eff",
   "metadata": {},
   "source": [
    "#### Prepare workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b76a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install voronoi-diagram-for-polygons xarray-spatial rioxarray pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb292b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --user --upgrade pygeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --user ogr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb93aaf",
   "metadata": {},
   "source": [
    "#### Input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632212c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BuiltAreaList = fiona.listlayers(\"WSFE_cumulativelayers.gdb\")\n",
    "print(BuiltAreaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boundary = gpd.read_file('WSFE.gdb', layer=1)#; Boundary.crs = \"ESRI:102022\"\n",
    "print(Boundary.info())\n",
    "print(Boundary.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PopRasterList = [rioxarray.open_rasterio(item, masked=True) \n",
    "                 for item in glob.glob(r'C:\\Users\\wb527163\\GEO_Cdrive_Grace\\urban_growth\\WorldPop_tifs_albers' \n",
    "                                       + '**/*.tif', recursive=True)] \n",
    "\n",
    "print(PopRasterList)\n",
    "\n",
    "for item in PopRasterList:\n",
    "    print(item.rio.crs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54df54",
   "metadata": {},
   "source": [
    "#### Settlement extents by year (cumulative built areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48770865",
   "metadata": {},
   "outputs": [],
   "source": [
    "BuiltAllYears = gpd.GeoDataFrame(\n",
    "    columns=['GRID3_splitID', 'MAX_year', 'Shape_Length', 'Shape_Area', 'CuYear', 'geometry'], \n",
    "    geometry='geometry', crs = \"ESRI:102022\")\n",
    "print(BuiltAllYears.info())\n",
    "print(BuiltAllYears.crs)\n",
    "print(BuiltAllYears.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in BuiltAreaList:\n",
    "    CuYear = re.sub(r'[^0-9]', '', item)\n",
    "    TempItem = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=item)\n",
    "    TempItem[\"CuYear\"] = CuYear\n",
    "    BuiltAllYears = pd.concat([BuiltAllYears, TempItem])\n",
    "    \n",
    "BuiltAllYears.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BuiltAllYears.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da43fc8",
   "metadata": {},
   "source": [
    "#### Thiessen polygons (Voronoi polygons): For each year, demarcate the surrounding space which is closest to a particular feature than to any other feature in the year set.\n",
    "#### Then, buffer area of each built-up polygon and use that buffer to clip the Thiessen areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If CRSs do not match:\n",
    "#PopRasterList = [item.rio.reproject_match(BuiltArea) for item in PopRasterList]\n",
    "#Boundary = Boundary.to_crs(BuiltArea.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boundary = Boundary.dissolve()\n",
    "Boundary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e22fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BuiltThiessenBuffer = gpd.GeoDataFrame(\n",
    "#     columns=['GRID3_splitID', 'MAX_year', 'Shape_Length', 'Shape_Area', 'CuYear', 'geometry'], \n",
    "#     geometry='geometry', crs = \"ESRI:102022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in BuiltAreaList:\n",
    "    print('Loading layer %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYear = re.sub(r'[^0-9]', '', item) # Pull the year of feature layer (e.g. \"2005\") from the numeric portion of the layer name.\n",
    "    Layer = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=item) # Read in the layer as a geodataframe.\n",
    "    print('Create a buffer around the features in the original layer. %s\\n' % time.ctime())\n",
    "    BufferLayer = Layer\n",
    "    BufferLayer['geometry'] = BufferLayer['geometry'].apply(make_valid) # This is a workaround for any null geometries.\n",
    "    BufferLayer['geometry'] = BufferLayer['geometry'].buffer(2000) # Create a 2km buffer around the original feature.\n",
    "    BufferLayer.to_file(driver='ESRI Shapefile', filename=''.join(['Buff_', CuYear, '.shp']))\n",
    "    print('Buffered version finished and saved to file. %s\\n' % time.ctime())\n",
    "    del CuYear, Layer, BufferLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ['cu_sel_2011', 'cu_sel_2012', 'cu_sel_2013', 'cu_sel_2014', 'cu_sel_2015']:\n",
    "# Error happened in cu_sel_2010. See OneNote page for details.\n",
    "# ValueError: No Shapely geometry can be created from null value\n",
    "# for item in BuiltAreaList:\n",
    "    print('Loading layer %s. %s\\n' % (item, time.ctime()))\n",
    "    CuYear = re.sub(r'[^0-9]', '', item) # Pull the year of feature layer (e.g. \"2005\") from the numeric portion of the layer name.\n",
    "    Layer = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=item) # Read in the layer as a geodataframe.\n",
    "    Buffer = gpd.read_file(filename=''.join(['Buff_', CuYear, '.shp']))\n",
    "    print('Loaded. Assigning year field. %s\\n' % time.ctime())\n",
    "    Layer[\"CuYear\"] = CuYear # Give geodataframe a field where every value is the year of cumulative buildup represented by the layer. This will be useful if concatenating all the layers together into a single dataset.\n",
    "    print('Assigned. Drawing Thiessen (Voronoi) polygons using buffer as the bounding area. %s\\n' % time.ctime())\n",
    "    ThiessenLayer = voronoiDiagram4plg(Layer, Buffer) # Demarcate the area around each feature which is closer to that feature than any other feature.\n",
    "    ThiessenLayer.to_file(driver='ESRI Shapefile', filename=''.join(['ThBuf_', CuYear, '.shp']))\n",
    "    print('Polygons drawn and written to file. %s\\n' % time.ctime())\n",
    "    del CuYear, Layer, ThiessenLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3df28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was the only year that gave me trouble. Running it outside of the list now that the issues are resolved.\n",
    "item = 'cu_sel_2010'\n",
    "print('Loading layer %s. %s\\n' % (item, time.ctime()))\n",
    "CuYear = re.sub(r'[^0-9]', '', item) # Pull the year of feature layer (e.g. \"2005\") from the numeric portion of the layer name.\n",
    "Layer = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=item) # Read in the layer as a geodataframe.\n",
    "Buffer = gpd.read_file(filename=''.join(['Buff_', CuYear, '.shp']))\n",
    "print('Loaded. Assigning year field. %s\\n' % time.ctime())\n",
    "Layer[\"CuYear\"] = CuYear # Give geodataframe a field where every value is the year of cumulative buildup represented by the layer. This will be useful if concatenating all the layers together into a single dataset.\n",
    "print('Assigned. Drawing Thiessen (Voronoi) polygons. %s\\n' % time.ctime())\n",
    "Layer['geometry'] = Layer['geometry'].apply(make_valid).buffer(0.1).simplify(10) # Using several workarounds for any null geometries. Our inputs don't need a high level of precision.\n",
    "Buffer['geometry'] = Buffer['geometry'].apply(make_valid)\n",
    "ThiessenLayer = voronoiDiagram4plg(Layer, Buffer) # Demarcate the area around each feature which is closer to that feature than any other feature.\n",
    "ThiessenLayer.to_file(driver='ESRI Shapefile', filename=''.join(['ThBuf_', CuYear, '.shp']))\n",
    "print('Polygons drawn and written to file. %s\\n' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BuiltListless2010 = BuiltAreaList.remove('cu_sel_2010')\n",
    "BuiltAreaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69cc84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Thiessen polygons did not retain feature attributes. Need to join back on spatially. \\n\\n')\n",
    "for item in BuiltAreaList:\n",
    "    print('Year: %s %s\\n' % (item, time.ctime()))\n",
    "    CuYear = re.sub(r'[^0-9]', '', str(item))\n",
    "    ThiessenLayer = gpd.read_file(filename=''.join(['ThBuf_', CuYear, '.shp']))\n",
    "    Layer = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=item)\n",
    "    print('Loaded original features and Thiessen layers. Joining together. %s\\n' % time.ctime())\n",
    "    ThiessenLayer = gpd.sjoin(ThiessenLayer, Layer, how='left', predicate='contains')\n",
    "    print('Finished! Writing to file. %s\\n' % time.ctime())\n",
    "    ThiessenLayer.to_file(driver='ESRI Shapefile', filename=''.join(['ThB2_', CuYear, '.shp']))\n",
    "    del CuYear, ThiessenLayer, Layer\n",
    "print('All years finished (except 2010).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in BuiltAreaList:\n",
    "#     print('Loading Thiessen areas and buffered polygons from layer %s. %s\\n' % (item, time.ctime()))\n",
    "#     CuYear = re.sub(r'[^0-9]', '', item) # Pull the year of feature layer (e.g. \"2005\") from the numeric portion of the layer name.\n",
    "#     ThiessenLayer = gpd.read_file(filename=''.join(['Thies_', CuYear, '.shp'])) # Read in the layer as a geodataframe.\n",
    "#     BufferLayer = gpd.read_file(filename=''.join(['Buff_', CuYear, '.shp']))\n",
    "#     print('Now clipping the Thiessen polygons with the buffer. %s\\n' % time.ctime())\n",
    "#     ThiessenBufferLayer = gpd.clip(ThiessenLayer, BufferLayer) # Clip the demarcated area so that coverage ends at the 2km mark. This will be both the mask used to reduce the file size of the population rasters, and the zones used to summarize the pop data during zonal statistics.\n",
    "#     print('Clipped. Polygons did not retain feature attributes. Joining back on. %s\\n' % time.ctime())\n",
    "#     ThiessenBufferLayer = ThiessenBufferLayer.merge(Layer, how='left', left_index=True, right_index=True) # Voronoi function does not retain the attributes (leaves them all Null for whatever reason). Just joining it all back together.\n",
    "#     print('Finished! Writing to file. %s\\n' % time.ctime())\n",
    "#     ThiessenBufferLayer.to_file(driver='ESRI Shapefile', filename=''.join(['ThBuf_', CuYear, '.shp']))\n",
    "#     print(ThiessenBufferLayer.sample(5))\n",
    "#     print('\\nNext layer. %s\\n' % time.ctime())\n",
    "#     del CuYear, ThiessenLayer, BufferLayer, ThiessenBufferLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d705d",
   "metadata": {},
   "source": [
    "#### Zonal statistics\n",
    "https://gis.stackexchange.com/questions/77993/issue-trying-to-create-zonal-statistics-using-gdal-and-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats(feat, input_zone_polygon, input_value_raster, statistic='sum'):\n",
    "\n",
    "    # Open data\n",
    "    raster = gdal.Open(input_value_raster)\n",
    "    shp = ogr.Open(input_zone_polygon)\n",
    "    lyr = shp.GetLayer()\n",
    "\n",
    "    # Get raster georeference info\n",
    "    transform = raster.GetGeoTransform()\n",
    "    xOrigin = transform[0]\n",
    "    yOrigin = transform[3]\n",
    "    pixelWidth = transform[1]\n",
    "    pixelHeight = transform[5]\n",
    "\n",
    "    # Get extent of feat\n",
    "    geom = feat.GetGeometryRef()\n",
    "    if (geom.GetGeometryName() == 'MULTIPOLYGON'):\n",
    "        count = 0\n",
    "        pointsX = []; pointsY = []\n",
    "        for polygon in geom:\n",
    "            geomInner = geom.GetGeometryRef(count)    \n",
    "            ring = geomInner.GetGeometryRef(0)\n",
    "            numpoints = ring.GetPointCount()\n",
    "            for p in range(numpoints):\n",
    "                    lon, lat, z = ring.GetPoint(p)\n",
    "                    pointsX.append(lon)\n",
    "                    pointsY.append(lat)    \n",
    "            count += 1\n",
    "    elif (geom.GetGeometryName() == 'POLYGON'):\n",
    "        ring = geom.GetGeometryRef(0)\n",
    "        numpoints = ring.GetPointCount()\n",
    "        pointsX = []; pointsY = []\n",
    "        for p in range(numpoints):\n",
    "                lon, lat, z = ring.GetPoint(p)\n",
    "                pointsX.append(lon)\n",
    "                pointsY.append(lat)\n",
    "\n",
    "    else:\n",
    "        sys.exit()\n",
    "\n",
    "    xmin = min(pointsX)\n",
    "    xmax = max(pointsX)\n",
    "    ymin = min(pointsY)\n",
    "    ymax = max(pointsY)\n",
    "\n",
    "    # Specify offset and rows and columns to read\n",
    "    xoff = int((xmin - xOrigin)/pixelWidth)\n",
    "    yoff = int((yOrigin - ymax)/pixelWidth)\n",
    "    xcount = int((xmax - xmin)/pixelWidth)+1\n",
    "    ycount = int((ymax - ymin)/pixelWidth)+1\n",
    "\n",
    "    # Create memory target raster\n",
    "    target_ds = gdal.GetDriverByName('MEM').Create('', xcount, ycount, gdal.GDT_Byte)\n",
    "    target_ds.SetGeoTransform((\n",
    "        xmin, pixelWidth, 0,\n",
    "        ymax, 0, pixelHeight,\n",
    "    ))\n",
    "\n",
    "    # Create for target raster the same projection as for the value raster\n",
    "    raster_srs = osr.SpatialReference()\n",
    "    raster_srs.ImportFromWkt(raster.GetProjectionRef())\n",
    "    target_ds.SetProjection(raster_srs.ExportToWkt())\n",
    "\n",
    "    # Rasterize zone polygon to raster\n",
    "    gdal.RasterizeLayer(target_ds, [1], lyr, burn_values=[1])\n",
    "\n",
    "    # Read raster as arrays\n",
    "    banddataraster = raster.GetRasterBand(1)\n",
    "    dataraster = banddataraster.ReadAsArray(xoff, yoff, xcount, ycount).astype(numpy.float)\n",
    "\n",
    "    bandmask = target_ds.GetRasterBand(1)\n",
    "    datamask = bandmask.ReadAsArray(0, 0, xcount, ycount).astype(numpy.float)\n",
    "\n",
    "    # Mask zone of raster\n",
    "    zoneraster = numpy.ma.masked_array(dataraster,  numpy.logical_not(datamask))\n",
    "\n",
    "    # Calculate statistics of zonal raster\n",
    "    if (statistic == 'sum'):\n",
    "        return numpy.sum(zoneraster)\n",
    "    elif (statistic == 'mean'):\n",
    "        return numpy.mean(zoneraster)\n",
    "    elif (statistic == 'count'):\n",
    "        return numpy.ma.count(zoneraster)\n",
    "    elif (statistic == 'median'):\n",
    "        return numpy.median(zoneraster)\n",
    "    elif (statistic == 'min'):\n",
    "        return numpy.min(zoneraster)\n",
    "    elif (statistic == 'max'):\n",
    "        return numpy.max(zoneraster)\n",
    "    elif (statistic == 'std'):\n",
    "        return numpy.std(zoneraster)\n",
    "    else:\n",
    "        return numpy.sum(zoneraster)\n",
    "    \n",
    "    \n",
    "def loop_zonal_stats(input_zone_polygon, input_value_raster, statistic):\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         input_zone_polygon: string\n",
    "#             File path to read from using ogr.Open().\n",
    "#         input_value_raster : string, default None\n",
    "#             The OGR format driver used to write the vector file.\n",
    "#             If not specified, it attempts to infer it from the file extension.\n",
    "#             If no extension is specified, it saves ESRI Shapefile to a folder.\n",
    "#         statistic : string, default = 'sum'\n",
    "#             The statistic used to aggregate values in the input raster. \n",
    "#             Output will be a single value associated with the input zone.\n",
    "#             Options: 'sum', 'mean', 'count', 'median', 'min', 'max', 'std'\n",
    "\n",
    "    shp = ogr.Open(input_zone_polygon)\n",
    "    lyr = shp.GetLayer()\n",
    "    featList = range(lyr.GetFeatureCount())\n",
    "    statDict = {}\n",
    "\n",
    "    for FID in featList:\n",
    "        feat = lyr.GetFeature(FID)\n",
    "        statValue = zonal_stats(feat, input_zone_polygon, input_value_raster, statistic)\n",
    "        statDict[FID] = statValue\n",
    "    return statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b567f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PopRasterList = glob.glob(r'WorldPop_tifs_albers/*.tif') # List all files in the WorldPop folder that have a .tif extension\n",
    "StatType = 'sum'\n",
    "\n",
    "print('Running zonal statistics. \\n')\n",
    "for item in PopRasterList:\n",
    "    Year = re.sub(r'[^0-9]', '', str(item)) # Get year of raster dataset. This will be used to choose the right polygon zones.\n",
    "    print('Year: %s %s' % (Year, time.ctime()))\n",
    "    RasterLayer = item\n",
    "    ZoneLayerName = ''.join(['ThB2_', Year, '.shp'])\n",
    "    ColumnName = ''.join(['Pop', Year]) # Zonal stats outputs will be named after the year and dataset, as a column in the settlements polygon.\n",
    "    print('Zonal statistics (%s) for year %s. %s' % (StatType, Year, time.ctime()))\n",
    "    zStats = loop_zonal_stats(ZoneLayerName, RasterLayer, StatType)\n",
    "    print('Stats finished for year %s! Joining back onto Thiessen polygons. %s' % (Year, time.ctime()))\n",
    "    zStats = pd.DataFrame.from_dict(zStats, orient = 'index', columns = [ColumnName])\n",
    "    ZoneLayer = gpd.read_file(ZoneLayerName)\n",
    "    ZoneWithStats = ZoneLayer.merge(zStats, how='left', left_index=True, right_index=True)\n",
    "    print('And now joining the Thiessen polygon attributes back onto the settlements via spatial join. %s' % time.ctime())\n",
    "    SettlementLayer = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer=''.join(['cu_sel_', Year]))\n",
    "    SettlementLayer = gpd.sjoin(SettlementLayer, ZoneWithStats, how='left', predicate='within')\n",
    "    print('Joined. Writing the settlements to file with zonal info in attributes. %s\\n' % time.ctime())\n",
    "    SettlementLayer.to_file(''.join([StatType, '_', Year]))\n",
    "    del Year, RasterLayer, ZoneLayer, ColumnName, zStats, ZoneWithStats, SettlementLayer\n",
    "print('All years finished (except 2010). Outputs stored as shapefiles in project workspace.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673d782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bd62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b44560af",
   "metadata": {},
   "source": [
    "### Scratch and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce908717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raster dataset\n",
    "input_value_raster = r'WorldPop_tifs_albers/cmr_ppp_2000_UNadj.tif'\n",
    "# Vector dataset(zones)\n",
    "input_zone_polygon = 'ThB2_2000.shp'\n",
    "\n",
    "Year2000 = loop_zonal_stats(input_zone_polygon, input_value_raster, statistic='sum')\n",
    "Year2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = gpd.read_file('ThB2_2000.shp')\n",
    "zstats = pd.DataFrame.from_dict(Year2000, orient = 'index', columns = ['Pop2000'])\n",
    "poly = poly.merge(zstats, how='left', left_index=True, right_index=True)\n",
    "poly.to_file(driver='ESRI Shapefile', filename='test2000_zonal.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b85e8",
   "metadata": {},
   "source": [
    "##### EARLIER VERSION CODE BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daShapefile = r'ThB2_2000.shp'\n",
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "dataSource = driver.Open(daShapefile, 0) # 0 means read-only. 1 means writeable.\n",
    "\n",
    "# Check to see if shapefile is found.\n",
    "if dataSource is None:\n",
    "    print('Could not open %s' % (daShapefile))\n",
    "else:\n",
    "    print('Opened %s' % (daShapefile))\n",
    "    layer = dataSource.GetLayer()\n",
    "    featureCount = layer.GetFeatureCount()\n",
    "    print(\"Number of features in %s: %d\" % (os.path.basename(daShapefile),featureCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52544b4",
   "metadata": {},
   "source": [
    "##### VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edebce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2010 = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer='cu_sel_2010')\n",
    "\n",
    "ext_vertices = []\n",
    "int_vertices = []\n",
    "\n",
    "for i, row in layer2010.iterrows():\n",
    "    # It's better to check if multigeometry\n",
    "    multi = row.geometry.type.startswith(\"Multi\")\n",
    "\n",
    "    if multi:\n",
    "        n = 0\n",
    "        allparts = []\n",
    "        # iterate over all parts of multigeometry\n",
    "        for part in row.geometry:\n",
    "            part_length = len(part.exterior.coords)\n",
    "            allparts.append(part_length)\n",
    "            n += min(allparts)\n",
    "    else:\n",
    "        n = len(row.geometry.exterior.coords)\n",
    "    ext_vertices.append(n) ###\n",
    "\n",
    "\n",
    "layer2010[\"ext_vertices\"] = ext_vertices\n",
    "print(layer2010.sample(10))\n",
    "\n",
    "invalid2010 = layer2010[layer2010['ext_vertices']<4]\n",
    "print(invalid2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2011 = gpd.read_file(\"WSFE_cumulativelayers.gdb\", layer='cu_sel_2011')\n",
    "\n",
    "ext_vertices = []\n",
    "int_vertices = []\n",
    "\n",
    "for i, row in layer2011.iterrows():\n",
    "    # It's better to check if multigeometry\n",
    "    multi = row.geometry.type.startswith(\"Multi\")\n",
    "\n",
    "    if multi:\n",
    "        n = 0\n",
    "        allparts = []\n",
    "        # iterate over all parts of multigeometry\n",
    "        for part in row.geometry:\n",
    "            part_length = len(part.exterior.coords)\n",
    "            allparts.append(part_length)\n",
    "            n += min(allparts)\n",
    "    else:\n",
    "        n = len(row.geometry.exterior.coords)\n",
    "    ext_vertices.append(n) ###\n",
    "\n",
    "\n",
    "layer2011[\"ext_vertices\"] = ext_vertices\n",
    "print(layer2011.sample(10))\n",
    "\n",
    "invalid2011 = layer2011[layer2011['ext_vertices']<4]\n",
    "print(invalid2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb93377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(layer2010['ext_vertices']))\n",
    "print(min(layer2011['ext_vertices']))\n",
    "print(layer2010['ext_vertices'].isna().sum())\n",
    "print(layer2011['ext_vertices'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31726176",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_multi = []\n",
    "\n",
    "for i, row in layer2010.iterrows():\n",
    "    multi = row.geometry.type.startswith(\"Multi\")\n",
    "    \n",
    "    if multi:\n",
    "        pass\n",
    "    else:\n",
    "        n = row\n",
    "    non_multi.append(n) ###\n",
    "print(len(non_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_vertices = []\n",
    "int_vertices = []\n",
    "\n",
    "for i, row in layer2010.iterrows():\n",
    "    # It's better to check if multigeometry\n",
    "    try:\n",
    "        n = len(row.geometry.interior.coords)\n",
    "    except:\n",
    "        pass\n",
    "    int_vertices.append(n) ###\n",
    "\n",
    "layer2010[\"int_vertices\"] = int_vertices\n",
    "print(layer2010.sample(10))\n",
    "\n",
    "invalid2010 = layer2010[layer2010['int_vertices']<4]\n",
    "print(invalid2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6aeedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
