{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2016b3",
   "metadata": {},
   "source": [
    "# Spatiotemporal Trends in Urbanization\n",
    "## Overview\n",
    "This repository investigates year-by-year change in cities and settlements in Central and Western Africa (CWA). The goal is to capture activity for every settlement locality in a country to produce indicators that are high frequency, spatially granular, and timely. The Jupyter Notebook is the primary script used to construct each country's dataset. It tracks population, built-area, and economic and climate indicators across a 16-year timeframe from 2000 to 2015. \n",
    "\n",
    "The repository is split into three sections: methodology and notebooks, source data, and outputs. Outputs are organized by country and include growth tables (\"urban panel datasets\"), charts, and country briefs.\n",
    "\n",
    "\n",
    "## Datasets\n",
    "Datasets used to create each African country's urban panel data are as follows:\n",
    "1. Most up-to-date administrative boundaries.\n",
    "2. City names: **UCDB, Africapolis, and GeoNames.**\n",
    "3. Settlement types: **GRID3 settlement extents.** Captured between 2009-2019.\n",
    "4. Built-up area, yearly: **World Settlement Footprint Evolution.** Resolution: 30m.\n",
    "5. Population, yearly: **WorldPop.** UN-adjusted, unconstrained. Resolution: 100m.\n",
    "6. Nighttime lights, yearly: **Harmonization of DMSP and VIIRS.** Resolution: 1km and 500m.\n",
    "7. Flood extents, by return period: **FATHOM.** Resolution: 90m.\n",
    "\n",
    "\n",
    "## Accessing Data\n",
    "Source data are available to the public by providers listed in the previous section, with the exception of flood data. Please note that the source data files in this repository have been fit for purpose and may not cover your area of interest. Some sources are also not global; GRID3 settlement extents are only available for sub-Saharan Africa, and Africapolis names for Africa.\n",
    "\n",
    "Results from the analysis are currently available for Cameroon and are under development for Central African Republic and four Sahel countries: Burkina Faso, Chad, Mali, and Niger. Results are available in the outputs folder by country. Please contact the CWA Geospatial team to inquire about new locations.\n",
    "<br>\n",
    "> **Walker Kosmidou-Bradley**, wkosmidoubradley@worldbank.org\n",
    "<br>\n",
    "> **Grace Doherty**, gdoherty2@worldbank.org\n",
    "\n",
    "## License\n",
    "Materials under this repository are open-source under an MIT license. The community is invited to test, adapt, and re-purpose materials as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd14a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276b447",
   "metadata": {},
   "source": [
    "## 1. PREPARE WORKSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0190f8",
   "metadata": {},
   "source": [
    "### 1.1 Off-script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9093fd6",
   "metadata": {},
   "source": [
    "##### Off-script: Create folders in your working directory. (The folder where you are storing this script).\n",
    "> *ADM\n",
    "<br>Buildup\n",
    "<br>PlaceName\n",
    "<br>Population\n",
    "<br>Settlement\n",
    "<br>NTL*\n",
    "\n",
    "##### Before starting: Download datasets (as shapefile, GeoJSON, or tif where possible) and place or extract into corresponding folder. You can download the cleaned files from our [GitHub Repository](https://github.com/worldbank/Urban_Spatio_Temporal_Trends) or access original sources here:\n",
    "- ADM: *Varies by source.*\n",
    "- Buildup: https://download.geoservice.dlr.de/WSF_EVO/files/\n",
    "    - If more than one tif, name the tifs as follows: WSFE1.tif, WSFE2.tif... etc.\n",
    "- PlaceName: \n",
    "    - GeoNames: (file: cities500.zip) https://download.geonames.org/export/dump/\n",
    "    - Africapolis: https://africapolis.org/en/data\n",
    "    - Urban Centres Database: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php\n",
    "- Population: https://hub.worldpop.org/geodata/listing?id=69\n",
    "- Settlement: https://data.grid3.org/datasets/GRID3::grid3-cameroon-settlement-extents-version-01-01-/explore\n",
    "- Nighttime Lights: https://eogdata.mines.edu/products/dmsp/#v4 and https://eogdata.mines.edu/products/vnl/#annual_v2\n",
    "\n",
    "##### Other off-script:\n",
    "- Convert GeoNames from .txt file to shape (delimiter = tab, header rows = 0) and rename fields.\n",
    "- If necessary, mosaic WSFE rasters that cover the area of interest to create a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2548",
   "metadata": {},
   "source": [
    "### 1.2 Load all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in:\n",
    "# dir(), print(), range(), format(), int(), len(), list(), max(), min(), zip(), sorted(), sum(), open(), del, = None, try except, with as, for in, if elif else\n",
    "# Also: list.append(), list.insert(), list.remove(), count(), startswith(), endswith(), contains(), replace()\n",
    "\n",
    "import os, sys, glob, re, time, subprocess, string # os.getcwd(), os.path.join(), os.listdir(), os.remove(), time.ctime(), glob.glob(), string.zfill(), string.join()\n",
    "from os.path import exists # exists()\n",
    "from functools import reduce # reduce()\n",
    "\n",
    "import geopandas as gpd # read_file(), GeoDataFrame(), sjoin_nearest(), to_crs(), to_file(), .crs, buffer(), dissolve()\n",
    "import pandas as pd # .dtypes, Series(), concat(), DataFrame(), read_table(), merge(), to_csv(), .loc[], head(), sample(), astype(), unique(), rename(), between(), drop(), fillna(), idxmax(), isna(), isin(), apply(), info(), sort_values(), notna(), groupby(), value_counts(), duplicated(), drop_duplicates()\n",
    "from shapely.geometry import Point, LineString, Polygon, shape, MultiPoint\n",
    "from shapely.ops import cascaded_union\n",
    "from shapely.validation import make_valid  # in apply(make_valid)\n",
    "import shapely.wkt\n",
    "\n",
    "import numpy as np # median(), mean(), tolist(), .inf\n",
    "import fiona, rioxarray # fiona.open()\n",
    "import rasterio # open(), write_band(), .name, .count, .width, .height. nodatavals, .meta, update(), copy(), write()\n",
    "from rasterio.plot import show\n",
    "from rasterio import features # features.rasterize()\n",
    "from rasterio.features import shapes\n",
    "from rasterio import mask # rasterio.mask.mask()\n",
    "from rasterio.enums import Resampling # rasterio.enums.Resampling()\n",
    "from rasterstats import zonal_stats # zonal_stats()\n",
    "from osgeo import gdal, osr, ogr, gdal_array, gdalconst # Open(), SpatialReference, WarpOptions(), Warp(), GetDataTypeName(), GetRasterBand(), GetNoDataValue(), Translate(), GetProjection(), GetAttrValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f9d7",
   "metadata": {},
   "source": [
    "### 1.3 Set up workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Workspace = os.getcwd()\n",
    "Source = os.path.join(Workspace, 'Source')\n",
    "Intermediate = os.path.join(Workspace, 'Intermediate')\n",
    "Results = os.path.join(Workspace, 'Results')\n",
    "\n",
    "print('\\n'.join([Workspace, Source, Intermediate, Results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267db21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListFromRange(r1, r2):\n",
    "    return [item for item in range(r1, r2+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFEYears = ListFromRange(1985, 2015) # All years in the WSFE dataset.\n",
    "AllStudyYears = ListFromRange(1999, 2015) # All years for which there will be growth stats in the present study.\n",
    "ReversedStudyYears = []\n",
    "for i in AllStudyYears:\n",
    "    ReversedStudyYears.insert(0,i)\n",
    "print(WSFEYears, '\\n\\n', AllStudyYears, '\\n\\n', ReversedStudyYears)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc3ea3",
   "metadata": {},
   "source": [
    "### 1.4 User-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Stack Exchange @RutgerH\n",
    "# https://gis.stackexchange.com/questions/163685/reclassify-a-raster-value-to-9999-and-set-it-to-the-nodata-value-using-python-a\n",
    "def readRaster(filename):\n",
    "    filehandle = gdal.Open(filename)\n",
    "    band1 = filehandle.GetRasterBand(1)\n",
    "    geotransform = filehandle.GetGeoTransform()\n",
    "    geoproj = filehandle.GetProjection()\n",
    "    Z = band1.ReadAsArray()\n",
    "    xsize = filehandle.RasterXSize\n",
    "    ysize = filehandle.RasterYSize\n",
    "    return xsize,ysize,geotransform,geoproj,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default arguments can be changed here, or can be specified below when running the functions.\n",
    "def writeRaster(filename,geotransform,geoprojection,data, NoDataVal=0, dst_datatype=gdal.GDT_UInt32):\n",
    "    (x,y) = data.shape\n",
    "    Dformat = \"GTiff\"\n",
    "    driver = gdal.GetDriverByName(Dformat)\n",
    "    # you can change the dataformat but be sure to be able to store negative values including -9999\n",
    "    dst_ds = driver.Create(filename,y,x,1,dst_datatype)\n",
    "    dst_ds.GetRasterBand(1).WriteArray(data)\n",
    "    dst_ds.SetGeoTransform(geotransform)\n",
    "    dst_ds.SetProjection(geoprojection)\n",
    "    dst_ds.GetRasterBand(1).SetNoDataValue(NoDataVal)\n",
    "    return 1\n",
    "    dst_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Stack Exchange @Kurt Schwehr:\n",
    "# https://stackoverflow.com/questions/10454316/how-to-project-and-resample-a-grid-to-match-another-grid-with-gdal-python\n",
    "def resampleRaster(InRaster_Path, MatchRaster_Path, OutFile_Path, \n",
    "                   DataType = gdalconst.GDT_UInt32, \n",
    "                   ResampType = gdal.GRA_Bilinear, NoDataVal = 0):\n",
    "    print('Loading for %s. %s' % (InRaster_Path, time.ctime()))\n",
    "    \n",
    "    RasterObject = gdal.Open(InRaster_Path)\n",
    "    In_proj = RasterObject.GetProjection()\n",
    "    [Match_x, Match_y, Match_geo, Match_proj, Match_Z] = readRaster(MatchRaster_Path)\n",
    "    print('---Specs to match to: \\n', \n",
    "      Match_proj, '\\n', Match_geo, '\\n', Match_x, '\\n', Match_y, '\\n')\n",
    "        \n",
    "    OutFile = gdal.GetDriverByName('GTiff').Create(OutFile_Path, Match_x, Match_y, 1, DataType)\n",
    "    OutFile.SetGeoTransform(Match_geo)\n",
    "    OutFile.SetProjection(Match_proj)\n",
    "    print('---Created raster file for upsampled version. %s' % time.ctime())\n",
    "    \n",
    "    gdal.ReprojectImage(RasterObject, OutFile, In_proj, Match_proj, ResampType)\n",
    "    print('---Resampled values onto an empty raster matching the dimensions of the buildup layer. %s \\n\\n' % time.ctime())\n",
    "    \n",
    "    OutFile.GetRasterBand(1).SetNoDataValue(NoDataVal)\n",
    "    \n",
    "    RasterObject = Outfile = None\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcShell(A, OutFile, Calculation, OutType = '', \n",
    "              B=None, C=None, D=None, E=None, F=None, G=None):\n",
    "    \"\"\"Raster math using gdal_calc.py.\n",
    "\n",
    "    The OSgeo package for Python API does not make raster calculations\n",
    "    easy outside of the shell. This function plugs up to 6 raster files\n",
    "    into a string which subprocess.call() then commits to the terminal.\n",
    "\n",
    "        A : str\n",
    "            File path to the first raster for the calculation.\n",
    "        B : str\n",
    "            File path to the second raster for the calculation.\n",
    "        OutFile : str\n",
    "            File path where to store the raster generated from the calculation.\n",
    "        Calculation : str\n",
    "            Algebra that uses A and B to create a new raster. Use double quotes.\n",
    "    \"\"\"\n",
    "    print('Running for %s. %s' % (A, time.ctime()))\n",
    "    cmd = 'gdal_calc.py -A ' + A\n",
    "    if B is not None:\n",
    "        cmd = cmd + ' -B ' + B \n",
    "    if C is not None:\n",
    "        cmd = cmd + ' -C ' + C \n",
    "    if D is not None:\n",
    "        cmd = cmd + ' -D ' + D\n",
    "    if E is not None:\n",
    "        cmd = cmd + ' -E ' + E\n",
    "    if F is not None:\n",
    "        cmd = cmd + ' -F ' + F\n",
    "    if G is not None:\n",
    "        cmd = cmd + ' -G ' + G\n",
    "    cmd = cmd + OutType + ' --outfile=' + OutFile + ' --overwrite --calc=' + Calculation\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = A = B = C = D = E = F = G = None\n",
    "    print('Ran in shell. See OutFile folder to inspect results. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac123cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaicShell(A, B, OutFile, Band = 1, OutType = '',\n",
    "                  C=None, D=None, E=None, F=None, G=None):\n",
    "    print('Running for %s. %s' % (A, time.ctime()))\n",
    "    \n",
    "    StringFiles = ' '.join([A,B])\n",
    "    \n",
    "    for RasterName in [C,D,E,F,G]:\n",
    "        if RasterName is not None:\n",
    "            StringFiles = ' '.join([StringFiles, RasterName])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    cmd = 'gdal_merge.py -o ' + OutFile + OutType + ' -of gtiff ' + StringFiles\n",
    "    \n",
    "    subprocess.call(cmd, shell=True)\n",
    "    print('Ran in shell. See OutFile folder to inspect results. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RasterToShapefile(InRasterPath, OutFilePath = 'RastToShp.shp', Band=1, \n",
    "                      OutName='RastToShp', VariableName='value', Driver = 'ESRI Shapefile'):\n",
    "    \"\"\"Raster tiff to vector polygon shapefile.\n",
    "    Can also be used for other file types like geopackage, but note that this code\n",
    "    currently does not account for writing into an existing file. It will write over\n",
    "    the file if specified as the file path.\n",
    "    \n",
    "    \"\"\"\n",
    "    Raster = gdal.Open(InRasterPath)\n",
    "    RasterBand = Raster.GetRasterBand(Band)\n",
    "    \n",
    "    OutDriver = ogr.GetDriverByName(Driver)\n",
    "    InProj = Raster.GetProjectionRef()\n",
    "    SpatRef = osr.SpatialReference()\n",
    "    SpatRef.ImportFromWkt(InProj)\n",
    "    print(InProj, '\\n\\n', SpatRef)\n",
    "    \n",
    "    if exists(OutFilePath):\n",
    "        OutFile = ogr.Open(OutFilePath)\n",
    "    else:\n",
    "        OutFile = OutDriver.CreateDataSource(OutFilePath)\n",
    "    OutLayer = OutFile.CreateLayer(OutName, srs = SpatRef, geom_type = ogr.wkbPolygon)\n",
    "    OutField = ogr.FieldDefn(VariableName, ogr.OFTInteger)\n",
    "    OutLayer.CreateField(OutField)\n",
    "    OutField = OutLayer.GetLayerDefn().GetFieldIndex(VariableName)\n",
    "    print('\\n', OutFile, '\\n', OutLayer, '\\n', OutField)\n",
    "    \n",
    "    print('Vectorizing. Input: %s. %s' % (InRasterPath, time.ctime()))\n",
    "    gdal.Polygonize(RasterBand, None, OutLayer, 0, [], callback=None)\n",
    "    print('Completed polygons. Stored as: %s. %s' % (OutFilePath, time.ctime()))\n",
    "\n",
    "    del Raster, RasterBand, OutFile, OutLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rioStats(InRasterPath, Band = 1):\n",
    "    out = rasterio.open(InRasterPath)\n",
    "    stats = []\n",
    "    band = out.read(Band)\n",
    "    stats.append({\n",
    "        'raster': out.name,\n",
    "        'bands': out.count,\n",
    "        'data type': out.dtypes,\n",
    "        'no data value': out.nodatavals,\n",
    "        'width': out.width,\n",
    "        'height': out.height,\n",
    "        'min': band.min(),\n",
    "        'mean': band.mean(),\n",
    "        'median': np.median(band),\n",
    "        'max': band.max()})\n",
    "    print(\"\\n\", stats)\n",
    "    \n",
    "    out = band = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShapeToRaster(Shapefile, ValueVar, MetaRasterPath, OutFilePath = 'ShpToRast.tif', Band=1, NewDType=None):\n",
    "    \"\"\"\n",
    "    Polygon spatial object to raster tiff.\n",
    "    \"\"\"\n",
    "    # Copy and update the metadata from another raster for the output\n",
    "    MetaRaster = rasterio.open(MetaRasterPath)\n",
    "    meta = MetaRaster.meta.copy()\n",
    "    meta.update(compress='lzw')\n",
    "    if NewDType is not None:\n",
    "        meta.update(dtype=NewDType)\n",
    "    MetaRaster.meta\n",
    "\n",
    "    print(\"Rasterizing dataset. %s\" % time.ctime())\n",
    "    with rasterio.open(OutFilePath, 'w+', **meta) as out:\n",
    "        out_arr = out.read(Band)\n",
    "\n",
    "        # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "        shapes = ((geom,value) for geom, value in zip(Shapefile.geometry, Shapefile[ValueVar]))\n",
    "\n",
    "        burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "        out.write_band(1, burned)\n",
    "    out = burned = shapes = None\n",
    "    \n",
    "    print(\"Finished rasterizing. Checking contents. %s\" % time.ctime())\n",
    "    rioStats(OutFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchZonal(RasterFileList, Zones, KeepFields = None, \n",
    "               RasterDirectory = os.getcwd(),\n",
    "               OutPath = 'BatchZonal.csv',\n",
    "               Statistics=['count', 'sum', 'mean'],\n",
    "               NoDataVal = None,\n",
    "               Prefix = '',\n",
    "               Suffix = '', \n",
    "               DropStatName = False):\n",
    "    '''\n",
    "    Choose a single file for the raster source, or multiple. If multiple, then each stat field\n",
    "    will receive a suffix with the raster's position in the RasterFileList.\n",
    "    \n",
    "        RasterFileList: list\n",
    "                        List of file paths, e.g. tifs. Loaded with rasterio.\n",
    "        Zones:          geodataframe object\n",
    "        KeepFields:     list\n",
    "                        List of field names (string format).\n",
    "        OutPath:        string\n",
    "                        File path for the results.\n",
    "        Statistics:     list\n",
    "                        Full Statistics options: \n",
    "                        ['max', 'min', 'mean', 'count', 'sum', 'median', 'std', 'majority', 'minority', \n",
    "                        'unique', 'range']\n",
    "                        There's also a percentile option in zonal_stats(). I believe you \n",
    "                        write it after an underscore in string format, e.g.: 'percentile_25'.\n",
    "        NoDataVal:      numeric\n",
    "        Prefix:         string\n",
    "        Suffix:         string\n",
    "        DropStatName:   boolean\n",
    "    \n",
    "    '''\n",
    "    if KeepFields is None:\n",
    "        AllSummaries = pd.DataFrame(Zones)[[]]\n",
    "    else:\n",
    "        AllSummaries = pd.DataFrame(Zones)[KeepFields]\n",
    "    print('Dataframe to merge with: \\n', AllSummaries)\n",
    "    \n",
    "    \n",
    "    for idx, RasterFile in enumerate(RasterFileList):\n",
    "        print('Loading with rasterio. %s\\nRaster: %s' % (time.ctime(), RasterFile))\n",
    "        \n",
    "        InRasterPath = os.path.join(RasterDirectory, RasterFile)\n",
    "        \n",
    "        Year = re.search('\\d{4}', os.path.basename(RasterFile))\n",
    "        if Year is None:\n",
    "            Year = '_' + str(idx)\n",
    "        else:\n",
    "            Year = Year.group(0)\n",
    "\n",
    "        with rasterio.open(InRasterPath) as src:\n",
    "            transform = src.meta['transform']\n",
    "            array = src.read(1)\n",
    "\n",
    "        print(transform)\n",
    "        print(array)\n",
    "        \n",
    "        print('Zonal statistics. %s' % time.ctime())\n",
    "        zStats = zonal_stats(Zones, array, affine=transform, stats = Statistics, nodata=NoDataVal)\n",
    "        #print(zStats)\n",
    "\n",
    "        ValByZone = pd.DataFrame(Zones)[[]].join(pd.DataFrame(zStats))\n",
    "        \n",
    "        if DropStatName is False:\n",
    "            for stat in Statistics:\n",
    "                StatField = ''.join([Prefix, stat, Year, Suffix])\n",
    "                ValByZone = ValByZone.rename(columns={stat:StatField})\n",
    "            print('%s stat output field for %s: %s' % (stat, RasterFile, StatField))\n",
    "        else:\n",
    "            for stat in Statistics:\n",
    "                StatField = ''.join([Prefix, Year, Suffix])\n",
    "                ValByZone = ValByZone.rename(columns={stat:StatField})\n",
    "            print('%s stat output field for %s: %s' % (stat, RasterFile, StatField))\n",
    "\n",
    "        AllSummaries = AllSummaries.join(ValByZone)\n",
    "        print(AllSummaries.sample(5))\n",
    "        \n",
    "    print('Final dataframe: \\n', AllSummaries.sample(5))\n",
    "\n",
    "    AllSummaries.to_csv(OutPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccda52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskByZone(MaskPath, SourceFolder, DestFolder, SourceList = None,\n",
    "               MaskLayerName = None, dstSRS = 'ESRI:102022'):\n",
    "    \"\"\"\n",
    "    Reduces the size of a raster's valid data cells to vector areas of interest.\n",
    "    This is useful if the raster data needs to be vectorized later to save space.\n",
    "    \n",
    "    The script prepares the vector zones as a list of geometries in the desired\n",
    "    spatial reference system, then warps each raster in the specified source\n",
    "    folder to the same SRS. Masking in rasterio then reclassifies any raster cells\n",
    "    falling outside of a mask polygon as NoData.\n",
    "    \"\"\"\n",
    "    \n",
    "    ProjSRS = osr.SpatialReference()\n",
    "    ProjSRS.SetFromUserInput(dstSRS)\n",
    "    ProjWarp = gdal.WarpOptions(dstSRS = dstSRS)\n",
    "    \n",
    "    if SourceList is not None:\n",
    "        SourceFiles = SourceList\n",
    "    else:\n",
    "        SourceFiles = []\n",
    "        SourceFiles = SourceFiles + [i for i in os.listdir(''.join([SourceFolder, r'/'])) if i.endswith('tif')]\n",
    "        print(SourceFiles)\n",
    "\n",
    "    \n",
    "    ### 1. ASSIGN SPATIAL REFERENCE SYSTEM OF VECTOR MASK AND LOAD GEOMETRIES\n",
    "    Vector = gpd.read_file(filename=MaskPath, layer=MaskLayerName)\n",
    "    if Vector.crs != dstSRS:\n",
    "        if MaskLayerName == None:\n",
    "            MaskPath = MaskPath + '_temp'\n",
    "        else:\n",
    "            MaskLayerName = MaskLayerName + '_temp'\n",
    "        Vector.to_crs(dstSRS).to_file(filename=MaskPath, layer=MaskLayerName)\n",
    "    Vector = None # We're reloading the geometries with fiona\n",
    "    \n",
    "    with fiona.open(MaskPath, mode=\"r\", layer=MaskLayerName) as Vector:\n",
    "        MaskGeom = [feature[\"geometry\"] for feature in Vector] # Identify the bounding areas of the mask.\n",
    "    \n",
    "    \n",
    "    ### 2. PREPARE DESTINATION FILES\n",
    "    for FileName in SourceFiles:\n",
    "    \n",
    "        InputRasterPath = os.path.join(Workspace, SourceFolder, FileName)\n",
    "        \n",
    "        Sensor = re.search('[A-Z]+_', FileName)\n",
    "        if Sensor is None:\n",
    "            Sensor = ''\n",
    "        else:\n",
    "            Sensor = Sensor.group(0)\n",
    "\n",
    "        Year = re.search('\\d{4}', FileName)\n",
    "        if Year is None:\n",
    "            Year = ''\n",
    "        else:\n",
    "            Year = Year.group(0)\n",
    "\n",
    "        if FileName.endswith('avg.tif') == True:\n",
    "            IndicType = '_avg'\n",
    "        elif FileName.endswith('cfc.tif') == True:\n",
    "            IndicType = '_cfc'\n",
    "        else:\n",
    "            IndicType = ''\n",
    "\n",
    "        TempOutputName = 'Temp_' + Sensor + Year + IndicType + '.tif'\n",
    "        TempOutputPath = os.path.join(Workspace, DestFolder, TempOutputName)\n",
    "        FinalOutputName = 'Msk_' + Sensor + Year + IndicType + '.tif'\n",
    "        FinalOutputPath = os.path.join(Workspace, DestFolder, FinalOutputName)\n",
    "\n",
    "    ### 3. ASSIGN SPATIAL REFERENCE SYSTEM OF RASTER(S)\n",
    "        InputRasterObject = gdal.Open(InputRasterPath)\n",
    "        SourceSRS = osr.SpatialReference(wkt=InputRasterObject.GetProjection())\n",
    "        print('Source projection: ', SourceSRS.GetAttrValue('projcs'))\n",
    "        print('Destination projection: ', ProjSRS.GetAttrValue('projcs'))\n",
    "\n",
    "        if SourceSRS.GetAttrValue('projcs') != ProjSRS.GetAttrValue('projcs'):\n",
    "            Warp = gdal.Warp(TempOutputPath, # Where to store the warped raster\n",
    "                         InputRasterObject, # Which raster to warp\n",
    "                         format='GTiff', \n",
    "                         options=ProjWarp) # Reproject to Africa Albers Equal Area Conic\n",
    "            print('Finished gdal.Warp() for %s. %s \\n' % (FileName, time.ctime()))\n",
    "\n",
    "            Warp = None # Close the files\n",
    "        else:\n",
    "            pass\n",
    "        InputRasterObject = None\n",
    "        \n",
    "    ### 4. RECLASSIFY AS NODATA IF OUTSIDE OF SETTLEMENT BUFFER ZONE.\n",
    "        if exists(TempOutputPath):\n",
    "            NewInputPath = TempOutputPath \n",
    "            print(\"We warped the data, so we'll use that file for next step.\")\n",
    "        else:\n",
    "            NewInputPath = InputRasterPath \n",
    "            print(\"We skipped the warp, so we continue to use the source file.\")\n",
    "\n",
    "        with rasterio.open(NewInputPath) as InputRasterObject:\n",
    "            MaskedOutputRaster, OutTransform = rasterio.mask.mask(\n",
    "                InputRasterObject, MaskGeom, crop=True) # Anything outside the mask is reclassed to the raster's NoData value.\n",
    "            OutMetaData = InputRasterObject.meta.copy()\n",
    "        print('Finished rasterio.mask.mask() for %s. %s \\n' % (FileName, time.ctime()))\n",
    "\n",
    "        OutMetaData.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": MaskedOutputRaster.shape[1],\n",
    "                         \"width\": MaskedOutputRaster.shape[2],\n",
    "                         \"transform\": OutTransform})\n",
    "\n",
    "        with rasterio.open(FinalOutputPath, \"w\", **OutMetaData) as dest:\n",
    "            dest.write(MaskedOutputRaster)\n",
    "        print('Written to file. %s \\n' % time.ctime())\n",
    "        InputRasterObject = None\n",
    "\n",
    "        if exists(TempOutputPath):\n",
    "            try:  # Finally, remove the intermediate file from disk\n",
    "                os.remove(TempOutputPath)\n",
    "            except OSError:\n",
    "                pass\n",
    "            print('Removed intermediate file. %s \\n' % time.ctime())\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    print('\\n \\n Finished all years in list. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchZonalStats(FolderName, Zones, \n",
    "                    CRS = 'ESRI:102022', \n",
    "                    JoinField = 'Sett_ID',\n",
    "                    StatsWanted = ['count', 'sum', 'mean', 'max', 'min'],\n",
    "                    SeriesStart = 1999, SeriesEnd = 2015, \n",
    "                    AnnualizedFiles = None, VarPrefix = None):\n",
    "    \"\"\"\n",
    "    Normally, we would use numpy to generate a point gdf from the raster's matrix. \n",
    "    However, I was running into a lot of memory errors with that method.\n",
    "    This method uses some extra steps: tif to xyz to df to gdf. But it saves to file\n",
    "    and deletes intermediate files along the way, circumventing memory issues.\n",
    "    \n",
    "    Run MaskByZone() prior to reduce the raster to only your area(s) of interest.\n",
    "    \n",
    "    \"\"\"\n",
    "    if AnnualizedFiles is None:\n",
    "        AnnualizedFiles = [i for i in os.listdir(FolderName) if i.endswith('.tif')]\n",
    "    print(AnnualizedFiles)\n",
    "    AllSummaries = pd.DataFrame(Zones).drop(columns='geometry')[[JoinField]]\n",
    "    print(AllSummaries)\n",
    "    \n",
    "    if VarPrefix is None:\n",
    "        VarPrefix = FolderName[:3].upper()\n",
    "    \n",
    "    for FileName in AnnualizedFiles:\n",
    "    ### STEP 1: TIF TO XYZ ###\n",
    "        print('Loading data for %s. %s \\n' % (FileName, time.ctime()))\n",
    "        \n",
    "        Sensor = re.search('[A-Z]+_', FileName)\n",
    "        if Sensor is None:\n",
    "            Sensor = ''\n",
    "        else:\n",
    "            Sensor = Sensor.group(0)\n",
    "            \n",
    "        Year = re.search('\\d{4}', FileName)\n",
    "        if Year is None:\n",
    "            Year = ''\n",
    "        else:\n",
    "            Year = Year.group(0)\n",
    "        \n",
    "        InputRasterPath = os.path.join(Workspace, FolderName, FileName)\n",
    "        InputRasterObject = gdal.Open(InputRasterPath)\n",
    "        XYZOutputPath = FolderName + r'/{}'.format(\n",
    "            FileName.replace('.tif', '.xyz')) # New file path will be the same as original, but .tif is replaced with .xyz\n",
    "\n",
    "        # Create an .xyz version of the .tif\n",
    "        if exists(XYZOutputPath):\n",
    "            print(\"Already created xyz file.\")\n",
    "        else:\n",
    "            print(\"Creating XYZ (gdal.Translate()).\")\n",
    "            XYZ = gdal.Translate(XYZOutputPath, # Specify a destination path\n",
    "                                 InputRasterObject, # Input is the masked .tif file\n",
    "                                 format='XYZ', \n",
    "                                 creationOptions=[\"ADD_HEADER_LINE=YES\"])\n",
    "            print('Finished gdal.Translate() for year %s. %s \\n' % (Year, time.ctime()))\n",
    "            XYZ = None # Reload XYZ as a point geodataframe\n",
    "\n",
    "        InputRasterObject = None\n",
    "\n",
    "\n",
    "    ### STEP 2: GENERATE GEODATAFRAME WITH JOIN FIELD ###\n",
    "        InputXYZ = pd.read_table(XYZOutputPath, delim_whitespace=True)\n",
    "        InputXYZ = InputXYZ.loc[InputXYZ['Z'] > 0] # Subset to only the features that have a value.\n",
    "        \n",
    "        if re.search('WSFE', FileName) is not None: # Scale back up to years if working with flood/building data.\n",
    "            InputXYZ['Z'] = InputXYZ['Z'] + 1900\n",
    "            \n",
    "        print('Loaded XYZ file as a pandas dataframe. %s \\n' % time.ctime())\n",
    "        ValObject = gpd.GeoDataFrame(InputXYZ,\n",
    "                                     geometry = gpd.points_from_xy(InputXYZ['X'], InputXYZ['Y']),\n",
    "                                     crs = CRS)\n",
    "        print('Created geodataframe from non-NoData points. %s \\n' % time.ctime())\n",
    "        del InputXYZ\n",
    "\n",
    "        # Sjoin_nearest: No need to group by ADM this time. \n",
    "        ValObject_withID = pd.DataFrame(gpd.sjoin_nearest(ValObject, \n",
    "                                        Zones, \n",
    "                                        how='left')).drop(columns='geometry')[['Z', JoinField]] # No need for max_distance parameter this time. We've already narrowed down to nearby raster cells.\n",
    "\n",
    "        print('\\nJoined zone ID onto vectorized raster cells. %s \\n' % time.ctime())\n",
    "        print(ValObject_withID.sample(10))\n",
    "        del ValObject\n",
    "\n",
    "        ValObject_withID.to_csv(''.join([FolderName, r'/', FileName.replace('.tif', '.csv')]))\n",
    "        print('\\nExported as table. %s \\n' % time.ctime())\n",
    "\n",
    "#         # Remove the temporary xyz file.\n",
    "#         try:  \n",
    "#             os.remove(os.path.join(XYZOutputPath))\n",
    "#         except OSError:\n",
    "#             pass\n",
    "#         print('Removed (or skipped if error) intermediate xyz file. %s \\n' % time.ctime())\n",
    "\n",
    "\n",
    "    ### STEP 3: AGGREGATE BY SETTLEMENT AND MERGE ONTO SUMMARIES TABLE ###\n",
    "        GroupedVals = ValObject_withID[ValObject_withID['Z'].notna()].groupby(JoinField, as_index=False)\n",
    "        \n",
    "        # Run this block if the variable is about cloud-free coverage.\n",
    "        if re.search('cfc', FileName) is not None:\n",
    "            VariableName = ''.join([VarPrefix, 'cfc_', Sensor, Year])\n",
    "            AllSummaries = AllSummaries.merge(GroupedVals.mean().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            print('\\nCount of cloud-free observations averaged to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "            # Save in-progress results\n",
    "            AllSummaries.to_csv(os.path.join(Results, ''.join([VarPrefix, '%sto%s.csv' % (SeriesStart, SeriesEnd)])))\n",
    "            print(AllSummaries.sample(10))\n",
    "        \n",
    "        # Run this block if we're working with the flooded buildings data.\n",
    "        elif re.search('WSFE', FileName) is not None:\n",
    "            for BuiltYear in AllStudyYears:\n",
    "                Grouped_Subset = GroupedVals[GroupedVals['Z'].between(\n",
    "                    1985, BuiltYear, inclusive=True)] # Inclusive parameter means we include the years 1985 and the named year.\n",
    "                if 'count' in StatsWanted:\n",
    "                    VariableName = ''.join([VarPrefix, 'ct', Sensor, BuiltYear])\n",
    "                    AllSummaries = AllSummaries.merge(GroupedVals.count().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "                if 'sum' in StatsWanted:\n",
    "                    VariableName = ''.join([VarPrefix, 'sum', Sensor, BuiltYear])\n",
    "                    AllSummaries = AllSummaries.merge(GroupedVals.sum().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "                if 'mean' in StatsWanted:\n",
    "                    VariableName = ''.join([VarPrefix, 'avg', Sensor, BuiltYear])\n",
    "                    AllSummaries = AllSummaries.merge(GroupedVals.mean().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "                if 'max' in StatsWanted:\n",
    "                    VariableName = ''.join([VarPrefix, 'max', Sensor, BuiltYear])\n",
    "                    AllSummaries = AllSummaries.merge(GroupedVals.max().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "                if 'min' in StatsWanted:\n",
    "                    VariableName = ''.join([VarPrefix, 'min', Sensor, BuiltYear])\n",
    "                    AllSummaries = AllSummaries.merge(GroupedVals.min().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "                print('\\nDesired aggregation methods applied to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "\n",
    "                # Save in-progress results\n",
    "                AllSummaries.to_csv(os.path.join(Results, ''.join([VarPrefix, '%sto%s.csv' % (SeriesStart, SeriesEnd)])))\n",
    "                print(AllSummaries.sample(10))\n",
    "        \n",
    "        # Anything else takes the standard aggregation method.\n",
    "        else:\n",
    "            if 'count' in StatsWanted:\n",
    "                VariableName = ''.join([VarPrefix, 'ct', Sensor, Year])\n",
    "                AllSummaries = AllSummaries.merge(GroupedVals.count().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            if 'sum' in StatsWanted:\n",
    "                VariableName = ''.join([VarPrefix, 'sum', Sensor, Year])\n",
    "                AllSummaries = AllSummaries.merge(GroupedVals.sum().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            if 'mean' in StatsWanted:\n",
    "                VariableName = ''.join([VarPrefix, 'avg', Sensor, Year])\n",
    "                AllSummaries = AllSummaries.merge(GroupedVals.mean().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            if 'max' in StatsWanted:\n",
    "                VariableName = ''.join([VarPrefix, 'max', Sensor, Year])\n",
    "                AllSummaries = AllSummaries.merge(GroupedVals.max().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            if 'min' in StatsWanted:\n",
    "                VariableName = ''.join([VarPrefix, 'min', Sensor, Year])\n",
    "                AllSummaries = AllSummaries.merge(GroupedVals.min().rename(columns={'Z': VariableName}), how = 'left', on=JoinField)\n",
    "            print('\\nDesired aggregation methods applied to settlement level, year %s. %s \\n' % (Year, time.ctime()))\n",
    "            \n",
    "            # Save in-progress results\n",
    "            AllSummaries.to_csv(os.path.join(Results, ''.join([VarPrefix, '%sto%s.csv' % (SeriesStart, SeriesEnd)])))\n",
    "            print(AllSummaries.sample(10))\n",
    "\n",
    "    \n",
    "    print('\\n\\nFinished. All years masked and assigned their nearest settlement. %s' % time.ctime())\n",
    "    print(AllSummaries.sample(10))\n",
    "    AllSummaries.to_csv(os.path.join(Results, ''.join([VarPrefix, '%sto%s.csv' % (SeriesStart, SeriesEnd)])))\n",
    "    print('Saved to file. %s \\n' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584def0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5537c1ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08550",
   "metadata": {},
   "source": [
    "## 2. PREPARE BUILDUP, SETTLEMENT, AND ADMIN DATASETS\n",
    "Projection for all datasets: Africa Albers Equal Area Conic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ea5f",
   "metadata": {},
   "source": [
    "### 2.1 Prepare GRID3 and Admin area files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc10ab7",
   "metadata": {},
   "source": [
    "#### Admin areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a759f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the first file ([0]) which ends in '.shp' from the specified folder, drop all variables, and reproject.\n",
    "ADM_vec = gpd.read_file(glob.glob('Source/ADM/*.shp')[0])[['geometry']].to_crs('ESRI:102022')\n",
    "\n",
    "# Create a fresh unique ID\n",
    "ADM_vec['ADM_ID'] = range(0, len(ADM_vec))\n",
    "ADM_vec['ADM_ID'] = ADM_vec['ADM_ID'] + 1 \n",
    "# We have to add 1 if we want our rasterized version's NoData value to be 0. Otherwise the first feature won't be valid.\n",
    "ADM_vec.to_file(driver='GPKG', filename=os.path.join(Source, 'ADM', 'ADM_withID.gpkg'), layer='ADM')\n",
    "\n",
    "# Now reload.\n",
    "ADM_vec = gpd.read_file(os.path.join(Source, 'ADM', 'ADM_withID.gpkg'), layer='ADM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732244bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need to know how many digits need to be allocated to each dataset in the \"join\" serial.\n",
    "len_ADM = len(str(ADM_vec['ADM_ID'].max()))\n",
    "\n",
    "print(ADM_vec.info(), \"\\n\\n\", \n",
    "      ADM_vec.sample(5),\n",
    "      ADM_vec.crs, \"\\n\\n\", \n",
    "      'Number of digits: ', len_ADM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24f3f9",
   "metadata": {},
   "source": [
    "#### Settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7946e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GRID3, we will still retain the 'type' field.\n",
    "GRID3_vec = gpd.read_file(glob.glob('Source/Settlement/*.shp')[0])[['type','geometry']].to_crs(\"ESRI:102022\")\n",
    "\n",
    "GRID3_vec['G3_ID'] = range(0,len(GRID3_vec))\n",
    "GRID3_vec['G3_ID'] = GRID3_vec['G3_ID'] +1\n",
    "\n",
    "GRID3_vec.to_file(driver='GPKG', filename=os.path.join(Source, 'Settlement', 'Settlement_withID.gpkg'), layer='GRID3')\n",
    "\n",
    "GRID3_vec = gpd.read_file(os.path.join(Source, 'Settlement', 'Settlement_withID.gpkg'), layer='GRID3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44280f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len_G3 =  len(str(GRID3_vec['G3_ID'].max()))\n",
    "\n",
    "print(GRID3_vec.info(), \"\\n\\n\",\n",
    "      GRID3_vec.sample(5),\n",
    "      GRID3_vec.crs, \"\\n\\n\", \n",
    "      'Number of digits: ', len_G3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca195d70",
   "metadata": {},
   "source": [
    "### 2.2 Reproject WSFE to project CRS, and ensure valid data range is only 1985-2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d96d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(os.path.join(Source, 'Buildup', 'WSFE.tif')):\n",
    "    pass\n",
    "else:\n",
    "    # If WSFE hasn't been mosaicked yet:\n",
    "    A=os.path.join(Source, 'Buildup', 'Premosaic', 'WSFE1.tif')\n",
    "    B=os.path.join(Source, 'Buildup', 'Premosaic', 'WSFE2.tif')\n",
    "    C=os.path.join(Source, 'Buildup', 'Premosaic', 'WSFE3.tif')\n",
    "    D=os.path.join(Source, 'Buildup', 'Premosaic', 'WSFE4.tif')\n",
    "\n",
    "    Out=os.path.join(Intermediate, 'Buildup', 'WSFE.tif')\n",
    "\n",
    "    mosaicShell(A=A, B=B, C=C, D=D, OutFile=Out, OutType = ' -ot UInt32 ')\n",
    "\n",
    "    Out=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44102e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "InPath = glob.glob('Source/Buildup/*.tif')[0]\n",
    "OutWGS = os.path.join(Intermediate, 'WSFE_reclass.tif')\n",
    "\n",
    "# Together, x and y define the data's \"shape\".\n",
    "# geotransform contains the parameters detailing how the raster should be stretched and aligned.\n",
    "# geoproj is the map projection\n",
    "# Z are the values in the raster band.\n",
    "[xsize,ysize,geotransform,geoproj,Z] = readRaster(InPath)\n",
    "Z[Z<1985] = 0\n",
    "Z[Z>2015] = 0\n",
    "\n",
    "writeRaster(OutWGS,geotransform,geoproj,Z, NoDataVal=0, dst_datatype=gdal.GDT_UInt32)\n",
    "print('Wrote the reclassed raster to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutEqArea = os.path.join(Intermediate, 'WSFE_equalarea.tif')\n",
    "\n",
    "# Whenever we want to work in a projected CRS, we'll use Africa Albers Equal Area Conic.\n",
    "ProjEqArea = gdal.WarpOptions(dstSRS='ESRI:102022')\n",
    "Warp = gdal.Warp(OutEqArea, # Where to store the warped raster\n",
    "                 OutWGS, # Which raster to warp\n",
    "                 format='GTiff', \n",
    "                 options=ProjEqArea)\n",
    "print('Wrote the reclassed and reprojected raster to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rioStats(OutWGS)\n",
    "rioStats(OutEqArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "InPath = Warp = OutWGS = OutEqArea = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44be4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440aa2a2",
   "metadata": {},
   "source": [
    "## 3. WSFE AND ADM; GRID3 AND ADM\n",
    "RASTERIZE: Bring ADM and GRID3 into raster space.\n",
    "\n",
    "RASTER MATH: \"Join\" ADM ID onto GRID3 and onto WSFE by creating unique concatenation string.\n",
    "\n",
    "VECTORIZE: Bring joined data into vector space.\n",
    "\n",
    "VECTOR MATH: Split unique ID from raster math step into separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9663bd1",
   "metadata": {},
   "source": [
    "### 3.1 Rasterize admin areas and GRID3 using WSFE specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea871242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and update the metadata from WSFE for the output\n",
    "WSFE = os.path.join(Intermediate, 'WSFE_equalarea.tif')\n",
    "ADM_out = os.path.join(Intermediate, 'ADM_rasterized.tif')\n",
    "GRID3_out = os.path.join(Intermediate, 'GRID3_rasterized.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af504c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADM raster can be unsigned int 16. Consider 32 if there are thousands of admin areas.\n",
    "ShapeToRaster(Shapefile=ADM_vec, ValueVar=\"ADM_ID\", MetaRasterPath=WSFE, OutFilePath=ADM_out, NewDType = 'uint16')\n",
    "\n",
    "# To be safe, we'll do unsigned 32 for settlements.\n",
    "ShapeToRaster(GRID3_vec, \"G3_ID\", WSFE, GRID3_out, NewDType='uint32')\n",
    "\n",
    "# Check printed stats here to make sure the min is 0 and the max is the number of vector features in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ceaa3",
   "metadata": {},
   "source": [
    "### 3.2 Raster math to \"join\" admin to GRID3 and to WSFE.\n",
    "Processing is more rapid when \"joining,\" i.e. creating serial codes out of two datasets, in raster rather than vector space.\n",
    "Here, we are concatenating the ID fields of the two datasets to create a serial number that we can then split in vector space later to create two ID fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28140ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In paths\n",
    "InG3 = os.path.join(Intermediate, 'GRID3_rasterized.tif')\n",
    "InWSFE = os.path.join(Intermediate, 'WSFE_equalarea.tif')\n",
    "InADM = os.path.join(Intermediate, 'ADM_rasterized.tif')\n",
    "\n",
    "# Out paths\n",
    "G3_ADM = os.path.join(Intermediate, 'GRID3_ADM.tif')\n",
    "WSFE_ADM = os.path.join(Intermediate, 'WSFE_ADM.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "G3_rio = rasterio.open(InG3).read(1)\n",
    "WSFE_rio = rasterio.open(InWSFE).read(1)\n",
    "ADM_rio = rasterio.open(InADM).read(1)\n",
    "\n",
    "# Recalculate number of digits for each dataset if starting from Section 3\n",
    "len_G3 = len(str(G3_rio.max()))\n",
    "len_WSFE = len(str(WSFE_rio.max()))\n",
    "len_ADM = len(str(ADM_rio.max()))\n",
    "\n",
    "G3_rio = WSFE_rio = ADM_rio = None\n",
    "print('DIGITS', '\\nGRID3: ', len_G3, '\\nWSFE: ', len_WSFE, '\\nADM: ', len_ADM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e831cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations\n",
    "# The number of digits in the largest ADM index value (len_ADM) is \n",
    "# the number of zeroes we tack onto the first variable in the serial.\n",
    "\n",
    "Calc = \"(A*\" + str(10**len_ADM) + \")+B\" \n",
    "\n",
    "calcShell(A=InG3, B=InADM, OutFile=G3_ADM, Calculation=Calc)\n",
    "calcShell(A=InWSFE, B=InADM, OutFile=WSFE_ADM, Calculation=Calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a2406",
   "metadata": {},
   "source": [
    "*Adding together the values to create join IDs. This is in effect a concatenation of their ID strings, by way of summation. The number of zeros in the calc multiplication corresponds with number of digits of the maximum value in the \"B\" dataset. (e.g. Chad ADM codes go up 4 digits, so it's calc=(A*10000)+B).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "rioStats(G3_ADM)\n",
    "rioStats(WSFE_ADM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea3639",
   "metadata": {},
   "source": [
    "### 3.3 Vectorize serialized layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "G3_in = os.path.join(Intermediate, 'GRID3_ADM.tif')\n",
    "G3_out = os.path.join(Intermediate, 'GRID3_ADM.shp')\n",
    "WSFE_in = os.path.join(Intermediate, 'WSFE_ADM.tif')\n",
    "WSFE_out = os.path.join(Intermediate, 'WSFE_ADM.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a76c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "RasterToShapefile(G3_in, G3_out, OutName='GRID3_ADM', VariableName='gridcode', Driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "RasterToShapefile(WSFE_in, WSFE_out, OutName='WSFE_ADM', VariableName='gridcode', Driver = 'ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03937b47",
   "metadata": {},
   "source": [
    "### 3.4 Vector math to split raster strings into admin area, GRID3, and WSFE year assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb900b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load newly created vectorized datasets.\n",
    "GRID3_ADM = gpd.read_file(G3_out).to_crs(\"ESRI:102022\")\n",
    "WSFE_ADM = gpd.read_file(WSFE_out).to_crs(\"ESRI:102022\")\n",
    "print(GRID3_ADM.info(), \"\\n\\n\", GRID3_ADM.sample(10), \"\\n\\n\", GRID3_ADM.crs, \"\\n\\n\", \n",
    "      WSFE_ADM.info(), \"\\n\\n\", WSFE_ADM.sample(10), \"\\n\\n\", WSFE_ADM.crs, \"\\n\\n\", \n",
    "      GRID3_ADM['gridcode'].max(), WSFE_ADM['gridcode'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split serial back into separate dataset fields.\n",
    "# For example, Burkina: WSFE and ADM: 4+3=7 digits. GRID3 and ADM: 6+3=9 digits.\n",
    "G3_Fill = len_G3 + len_ADM\n",
    "WSFE_Fill = len_WSFE + len_ADM\n",
    "\n",
    "GRID3_ADM['gridstring'] = GRID3_ADM['gridcode'].astype(str).str.zfill(G3_Fill)\n",
    "WSFE_ADM['gridstring'] = WSFE_ADM['gridcode'].astype(str).str.zfill(WSFE_Fill)\n",
    "\n",
    "GRID3_ADM['Sett_ID'] = GRID3_ADM['gridstring'].str[:-len_ADM].astype(int) # Remove the last 3 digits to get the GRID3 portion.\n",
    "GRID3_ADM['ADM_ID'] = GRID3_ADM['gridstring'].str[-len_ADM:].astype(int) # Keep only the last 3 digits to get the ADM portion.\n",
    "WSFE_ADM['year'] = WSFE_ADM['gridstring'].str[:-len_ADM].astype(int)\n",
    "WSFE_ADM['ADM_ID'] = WSFE_ADM['gridstring'].str[-len_ADM:].astype(int)\n",
    "\n",
    "print(GRID3_ADM.sample(10), WSFE_ADM.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve any features that have the same G3 and ADM values so that we have a single unique feature per settlement.\n",
    "# Note: we do NOT want to dissolve the WSFE features. Distinct features for noncontiguous builtup areas of the same year is necessary to separate them in the Near tool step.\n",
    "\n",
    "print(time.ctime())\n",
    "GRID3_ADM = GRID3_ADM.dissolve(by=['Sett_ID', 'ADM_ID'], as_index=False)\n",
    "print(GRID3_ADM.info(), GRID3_ADM.head(), \"\\n\\n\", time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features where year, settlement, or admin area = 0.\n",
    "# This was supposed to be resolved earlier with the gdal_calc NoDataValue parameter. Being thorough.\n",
    "\n",
    "print(\"Before: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))\n",
    "WSFE_ADM = WSFE_ADM.loc[(WSFE_ADM[\"year\"] != 0) & (WSFE_ADM[\"ADM_ID\"] != 0)] # Since we change the datatype to integer, no need to include all digits. Otherwise, it would need to be: != '0000'\n",
    "GRID3_ADM = GRID3_ADM.loc[(GRID3_ADM[\"Sett_ID\"] != 0) & (GRID3_ADM[\"ADM_ID\"] != 0)]\n",
    "print(\"After: WSFE %s and GRID3 %s\\n\" % (WSFE_ADM.shape, GRID3_ADM.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bounded_ID is our new unique settlement identifier for subsequent matching steps.\n",
    "GRID3_ADM['Bounded_ID'] = GRID3_ADM.index\n",
    "WSFE_ADM['WSFE_ID'] = WSFE_ADM.index\n",
    "GRID3_ADM = GRID3_ADM[['Sett_ID', 'Bounded_ID', 'ADM_ID', 'geometry']]\n",
    "WSFE_ADM = WSFE_ADM[['WSFE_ID', 'year', 'ADM_ID', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: \n",
    "# The first two printed numbers should be the same. There shouldn't be any GRID3 rows with matching Sett_ID and ADM_IDs.\n",
    "# The latter two numbers should be different, and the first should be larger. We never dissolved WSFE by any column.\n",
    "\n",
    "print(len(GRID3_ADM[['Sett_ID', 'ADM_ID']]),\n",
    "      len(GRID3_ADM[['Sett_ID', 'ADM_ID']].drop_duplicates()),\n",
    "      len(WSFE_ADM[['year', 'ADM_ID']]),\n",
    "      len(WSFE_ADM[['year', 'ADM_ID']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82419540",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID3_ADM.to_file(\n",
    "    driver='GPKG', filename=os.path.join(Intermediate,'GRID3_ADM.gpkg'), layer='GRID3_ADM_cleaned')\n",
    "WSFE_ADM.to_file(\n",
    "    driver='GPKG', filename=os.path.join(Intermediate,' WSFE_ADM.gpkg'), layer='WSFE_ADM_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd7f5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f0a8e",
   "metadata": {},
   "source": [
    "## 4. UNIQUE SETTLEMENTS FROM WSFE AND GRID3: TWO VERSIONS\n",
    "\n",
    "Note that there are 2 versions here, so that we can create a fragmentation index:\n",
    "1. **Boundless, aka boundary-agnostic settlements**: Unique settlements are linked to GRID3 settlement IDs. Administrative areas do not influence the extents of the settlement.\n",
    "2. **Bounded, aka politically-defined settlements**: Settlements in the Boundless dataset which spread across more than one administrative area are split into separate settlements in the Bounded dataset. The largest polygon after the split is considered the \"principal\" settlement, and polygons in other admin areas are considered \"fragments.\" By dividing the fragment area(s) of the Bounded settlement by the area of the Boundless settlement, we can acquire a fragmentation index for each locality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6624",
   "metadata": {},
   "source": [
    "### 4.1 BOUNDED SETTLEMENTS: Near Join by ADM group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d294c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of admin areas with GRID3 features: %s\" % len(GRID3_ADM['ADM_ID'].unique().tolist()))\n",
    "print(\"Number of admin areas with WSFE features: %s\" % len(WSFE_ADM['ADM_ID'].unique().tolist()))\n",
    "print(\"Number of admin areas where one dataset is observed but the other is not: %s\" % (\n",
    "    len(GRID3_ADM['ADM_ID'].unique().tolist()) - len(WSFE_ADM['ADM_ID'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1c2f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ADM_IDs = sorted(GRID3_ADM['ADM_ID'].unique().tolist())\n",
    "ADM_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72693d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating this field to help in removing duplicates from the sjoin_nearest, next section.\n",
    "GRID3_ADM['G3_Area'] = GRID3_ADM['geometry'].area / 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bafaff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create empty geodataframe to append onto using the dataframe whose geometry we want to retain.\n",
    "Bounded = GRID3_ADM[0:0]\n",
    "Bounded[\"year\"] = pd.Series(dtype='int')\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40bc28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ID in ADM_IDs:\n",
    "    WSFE_shard = WSFE_ADM.loc[WSFE_ADM['ADM_ID'] == ID]\n",
    "    GRID3_shard = GRID3_ADM.loc[GRID3_ADM['ADM_ID'] == ID]\n",
    "    WSFE_GRID3_shard = gpd.sjoin_nearest(WSFE_shard, \n",
    "                                         GRID3_shard, \n",
    "                                         how='inner',\n",
    "                                         max_distance=500)\n",
    "    Bounded = pd.concat([Bounded, WSFE_GRID3_shard])\n",
    "    print('Completed near join in admin area %s. %s \\n' % (ID, time.ctime()))\n",
    "print('Completed near join for all ADMs. %s \\n' % time.ctime())\n",
    "\n",
    "del WSFE_shard, GRID3_shard, WSFE_GRID3_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bounded.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b56278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc68243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove WSFE features that did not match any GRID3 settlements.\n",
    "Bounded = Bounded.loc[~Bounded['Sett_ID'].isna()]\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5736e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del GRID3_ADM, ADM_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bd29d",
   "metadata": {},
   "source": [
    "### 4.2 Remove duplicates: where buildup polygons intersected with more than one GRID3 settlement extent.\n",
    "This happens when the first dataset (WSFE) intersects (distance = 0) with more than one feature of the second dataset (GRID3). More common for large cities. For example, Yaoundé, CMN has a large contiguous 1985 WSFE polygon which overlaps several small GRID3 features that are not Yaoundé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49786b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first number should always be zero. \n",
    "# The second tells us whether/how many WSFE polygons were duplicated by the Near join.\n",
    "\n",
    "print(len(WSFE_ADM[WSFE_ADM.duplicated('WSFE_ID')]), len(Bounded[Bounded.duplicated('WSFE_ID')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are duplicate WSFE_IDs, then we need to choose between them.\n",
    "# We'll pick the one that joined with the largest GRID3 polygon.\n",
    "# To do that, we can just sort the dataframe by GRID3 areas, then drop_duplicates. \n",
    "# It will retain the first row of each WSFE_ID group.\n",
    "Bounded = Bounded.sort_values('G3_Area', ascending=False).drop_duplicates(['WSFE_ID'])\n",
    "Bounded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6141b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Bounded[Bounded.duplicated('WSFE_ID')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22854d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can dissolve with the WSFE years, now that we can group them by their administratively split ID.\n",
    "Bounded = Bounded.dissolve(by=['year', 'Bounded_ID'], as_index=False)\n",
    "print(Bounded.info(), Bounded.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Bounded = Bounded[['ADM_ID_left', 'year', 'Bounded_ID', 'Sett_ID', 'geometry']].rename(columns={\"ADM_ID_left\": \"ADM_ID\"})\n",
    "Bounded = Bounded.astype({\"ADM_ID\":'int', \"Bounded_ID\":'int', \"Sett_ID\":'int', \"year\":'int'})\n",
    "print(Bounded.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ca6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bounded.to_file(\n",
    "    driver='GPKG', filename=os.path.join(Intermediate,'NonCumulativeSettlements.gpkg'), layer='Settlements_Bounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e43acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del WSFE_ADM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eadab",
   "metadata": {},
   "source": [
    "### 4.3 BOUNDLESS SETTLEMENTS: Dissolve features that were split by an ADM boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2638d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragments of any bounded settlement will be combined into a single \"boundless\" settlement in this version.\n",
    "# It is based on their \"Sett_ID\", which is a direct loan from the GRID3 settlement features.\n",
    "Boundless = Bounded.dissolve(by=['year', 'Sett_ID'], as_index=False)\n",
    "print(Boundless.info(), Boundless.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and save to file.\n",
    "Boundless.to_file(driver='GPKG', \n",
    "                  filename=os.path.join(Intermediate,'NonCumulativeSettlements.gpkg'), layer='Settlements_Boundless')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba12eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027eba",
   "metadata": {},
   "source": [
    "## 5. CUMULATIVE ANNUALIZED SETTLEMENT EXTENTS\n",
    "DISSOLVE BY YEAR SETS: Create separate feature layers of each cumulative year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d70322",
   "metadata": {},
   "source": [
    "### 5.1 Define study years for each for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boundless = gpd.read_file(os.path.join(Intermediate,'NonCumulativeSettlements.gpkg'), layer='Settlements_Boundless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReversedStudyYears_to2014 = []\n",
    "for i in AllStudyYears:\n",
    "    ReversedStudyYears_to2014.insert(0,i)\n",
    "ReversedStudyYears_to2014.remove(2015)\n",
    "print('\\n\\n', ReversedStudyYears_to2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de5331",
   "metadata": {},
   "source": [
    "### 5.2 Starting with main Boundless dataset, create a cumulative area feature layer for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1be53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each year in the growth stats study, we are taking features from all years prior to and including that year, \n",
    "# dissolving those features, and exporting as its own file.\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    YearSet = Boundless[Boundless['year'].between(\n",
    "        1985, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Sett_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    YearDissolve = YearSet.dissolve(by='Sett_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\"}, # Though ADM_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    YearName = ''.join(['Cu', str(item), '_Boundless'])\n",
    "    YearDissolve.to_file(driver='GPKG', filename=os.path.join(Results,'CumulativeSettlements.gpkg'), layer=YearName)\n",
    "    del YearSet, YearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c4746",
   "metadata": {},
   "source": [
    "##### Join area information from each cumulative layer onto the latest year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The latest year in the study contains all settlements. Merge all other years' areas onto this dataset.\n",
    "SettAreas = gpd.read_file(os.path.join(Results,'CumulativeSettlements.gpkg'), layer=\n",
    "                          ''.join(['Cu', str(2015), '_Boundless'])) \n",
    "SettAreas['AREA2015'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry') # We have settlement IDs, so no need to join spatially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33dc9c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in ReversedStudyYears_to2014:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(os.path.join(Results,'CumulativeSettlements.gpkg'), \n",
    "                              layer=''.join(['Cu', str(item), '_Boundless']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['AREA', str(item)])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Sett_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, 2015, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Sett_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(Results, 'Area.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0879f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cafcd",
   "metadata": {},
   "source": [
    "### 5.3 Repeat for Bounded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dcf6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bounded = gpd.read_file(r'Results/NonCumulativeSettlements.gpkg', layer='Settlements_Bounded')\n",
    "\n",
    "for item in AllStudyYears:\n",
    "    print('Subsetting to cumulative area for year: %s. %s\\n' % (item, time.ctime()))\n",
    "    YearSet = Bounded[Bounded['year'].between(1985, item, inclusive=True)] # Inclusive parameter means we include the years 1985 and \"item\" rather than only between them.\n",
    "    print('Dissolving so that each unique settlement (Bounded_ID) has a single cumulative WSFE feature. %s\\n' % time.ctime())\n",
    "    YearDissolve = YearSet.dissolve(by='Bounded_ID', \n",
    "                                        aggfunc={\"year\": \"max\", \"ADM_ID\":\"min\", \"Sett_ID\":\"min\"}, # Though ADM_ID and Sett_ID should be matching every time.\n",
    "                                        as_index=False)\n",
    "    print('Write to file. %s\\n' % time.ctime())\n",
    "    YearName = ''.join(['Cu', str(item), '_Bounded'])\n",
    "    YearDissolve.to_file(driver='GPKG', filename=os.path.join(Results,'CumulativeSettlements.gpkg'), layer=YearName)\n",
    "    del YearSet, YearDissolve\n",
    "print(\"Done with all years in set. %s\" % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettAreas = gpd.read_file(os.path.join(Results,'CumulativeSettlements.gpkg'), \n",
    "                          layer=''.join(['Cu', str(2015), '_Bounded']))\n",
    "SettAreas['AREA2015'] = SettAreas['geometry'].area / 10**6\n",
    "SettAreas = pd.DataFrame(SettAreas).drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaeaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in ReversedStudyYears_to2014:\n",
    "    print(\"Loading cumulative layer for year %s. %s\\n\" % (item, time.ctime()))\n",
    "    YearLayer = gpd.read_file(os.path.join(Results,'CumulativeSettlements.gpkg'), \n",
    "                              layer=''.join(['Cu', str(item), '_Bounded']))\n",
    "    print(\"Adding area field and converting to non-spatial dataframe. %s\\n\" % (time.ctime()))\n",
    "    AreaYearName = ''.join(['AREA', str(item)])\n",
    "    YearLayer[AreaYearName] = YearLayer['geometry'].area/ 10**6 \n",
    "    YearLayer = pd.DataFrame(YearLayer)[['Bounded_ID', AreaYearName]]\n",
    "    print(\"Merging variables from %s onto our latest year (%s) via table join. %s\\n\" % (item, 2015, time.ctime()))\n",
    "    SettAreas = SettAreas.merge(YearLayer, how='left', on='Bounded_ID')\n",
    "print(\"Done merging annualized areas onto latest year geometries. Saving to file. %s\\n\" % (time.ctime()))\n",
    "\n",
    "print(SettAreas.info())\n",
    "SettAreas.to_csv(os.path.join(Results, 'Area_Bounded.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7574cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettAreas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ecaf8",
   "metadata": {},
   "source": [
    "### 5.4 One settlement geofile to rule them all. ...and in the Sett_ID bind them.\n",
    "The annualized values can be stored as distinct non-spatial dataframes. Their Sett_IDs will be used to join onto this geoversion with place names for the summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80315077",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settlements = gpd.read_file(os.path.join(Results,'CumulativeSettlements.gpkg'), \n",
    "                           layer=''.join(['Cu', str(2015), '_Boundless']))[['Sett_ID', 'ADM_ID', 'geometry']]\n",
    "print(Settlements.info())\n",
    "print(Settlements.crs)\n",
    "Settlements.to_file(driver='GPKG', \n",
    "                       filename=os.path.join(Results,'SETTLEMENTS.gpkg'), \n",
    "                       layer='SETTLEMENTS_equalarea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the final products as WGS84.\n",
    "Settlements_WGS = Settlements.to_crs(4326) \n",
    "print(Settlements_WGS.info())\n",
    "print(Settlements_WGS.crs)\n",
    "Settlements_WGS.to_file(driver='GPKG', \n",
    "                       filename=os.path.join(Results,'SETTLEMENTS.gpkg'), \n",
    "                       layer='SETTLEMENTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8856d7",
   "metadata": {},
   "source": [
    "### 5.5 Buffer the area of the Boundless dataset's latest year to mask raster data in later sections.\n",
    "The Bounded dataset would also be fine for our purposes here. The buffer is dissolved to a single feature to be used for its total extents, which are identical between Bounded & Boundless datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buffer layer(s) to use as maximum distance for Near joins.\n",
    "\n",
    "# Population buffer: 2km\n",
    "Distance = 2000 # The Africa Albers projection is in meters. Saving in this projection to use in later sections.\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = Settlements[['Sett_ID', 'geometry']]\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(\n",
    "    make_valid).buffer(Distance) # make_valid is a workaround for any null geometries.\n",
    "print('Finished buffer layer creation. %s' % time.ctime())\n",
    "\n",
    "BufferFileName1 = ''.join(['Buff', str(Distance), 'm_', str(2015)])\n",
    "BufferLayer.to_file(driver='GPKG', filename=os.path.join(Intermediate,'Catchment.gpkg'), layer=BufferFileName1)\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1556d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nighttime Lights buffer: 250m\n",
    "Distance = 250\n",
    "\n",
    "print('Creating buffer layer. %s' % time.ctime())\n",
    "BufferLayer = Settlements[['Sett_ID', 'geometry']]\n",
    "BufferLayer['geometry'] = BufferLayer['geometry'].apply(\n",
    "    make_valid).buffer(Distance) # make_valid is a workaround for any null geometries.\n",
    "print('Finished buffer layer creation. %s' % time.ctime())\n",
    "\n",
    "BufferFileName2 = ''.join(['Buff', str(Distance), 'm_', str(2015)])\n",
    "BufferLayer.to_file(driver='GPKG', filename=os.path.join(Intermediate,'Catchment.gpkg'), layer=BufferFileName2)\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef5b7f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ce84a",
   "metadata": {},
   "source": [
    "## 6. PLACE NAMES\n",
    "Join urban place names from UCDB, Africapolis, and GeoNames onto the settlement vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456bf69",
   "metadata": {},
   "source": [
    "### 6.1 Load placename datasets, filter, and project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f815f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Anytime we use a spatial join or work with area, \n",
    "# my preference is to keep it in a planar, equal area, meters projection. So we'll load as the Africa Albers.\n",
    "Settlements = gpd.read_file(os.path.join(Results, 'SETTLEMENTS.gpkg'), layer='SETTLEMENTS_equalarea')\n",
    "Settlements['AREA2015'] = Settlements['geometry'].area / 10**6\n",
    "\n",
    "# Load, pull name field, rename, and reproject to match the catchments CRS.\n",
    "UCDB = gpd.read_file(os.path.join(Source, 'PlaceName', 'GHS_STAT_UCDB2015MT_GLOBE_R2019A_V1_2.gpkg'), \n",
    "                     layer=0)[['UC_NM_MN', 'geometry']].rename(\n",
    "    columns={\"UC_NM_MN\": \"UCDB_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "Africapolis = gpd.read_file(os.path.join(Source, 'PlaceName', 'AFRICAPOLIS2020.shp'))[['agglosName', 'geometry']].rename(\n",
    "    columns={\"agglosName\": \"Afpl_Name\"}).to_crs(\"ESRI:102022\")\n",
    "\n",
    "GeoNames = gpd.read_file(os.path.join(Source, 'PlaceName', 'GeoNames.gpkg'), \n",
    "                         layer=0)[['GeoName', 'geometry']].to_crs(\"ESRI:102022\")\n",
    "\n",
    "print(Settlements.info(), UCDB.info(), Africapolis.info(), GeoNames.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e934567",
   "metadata": {},
   "source": [
    "### 6.2 Join placenames onto settlements geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap it in pd.DataFrame() since the sjoin() is the last time we need the geometry.\n",
    "\n",
    "GeoNames = pd.DataFrame(gpd.sjoin_nearest(GeoNames, Settlements, \n",
    "                             how='left', distance_col=\"distGN\", max_distance=250, \n",
    "                             lsuffix=\"G3\", rsuffix=\"GN\")).drop(columns='geometry')\n",
    "Africapolis = pd.DataFrame(gpd.sjoin_nearest(Africapolis, Settlements, \n",
    "                             how='left', distance_col=\"distAF\", max_distance=250,\n",
    "                             lsuffix=\"G3\", rsuffix=\"Af\")).drop(columns='geometry')\n",
    "UCDB = pd.DataFrame(gpd.sjoin_nearest(UCDB, Settlements, \n",
    "                             how='left', distance_col=\"distUC\", max_distance=250,\n",
    "                             lsuffix=\"G3\", rsuffix=\"UC\")).drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f0ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(GeoNames.info())\n",
    "print(Africapolis.info())\n",
    "print(UCDB.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldatasets = [pd.DataFrame(Settlements).drop(columns='geometry'),\n",
    "               Africapolis[['Sett_ID', 'Afpl_Name', 'distAF']], \n",
    "               GeoNames[['Sett_ID', 'GeoName', 'distGN']],\n",
    "               UCDB[['Sett_ID', 'UCDB_Name', 'distUC']]]\n",
    "\n",
    "SettlementsNamed = reduce(lambda left,right: pd.merge(left,right,on=['Sett_ID'], how='left'), alldatasets)\n",
    "SettlementsNamed[['Afpl_Name', 'GeoName', 'UCDB_Name']] = SettlementsNamed[['Afpl_Name', 'GeoName', 'UCDB_Name']].fillna('UNK')\n",
    "\n",
    "# Replace NaN values with a countable distance.\n",
    "SettlementsNamed[['distAF', 'distGN', 'distUC']] = SettlementsNamed[['distAF', 'distGN', 'distUC']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af429c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(SettlementsNamed.info())\n",
    "print(SettlementsNamed.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del UCDB, Africapolis, GeoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fc7cd",
   "metadata": {},
   "source": [
    "The near joins should have prevented duplication of rows, but if df1 intersects with two features in df2, it creates a new row. Two of our placenames sources are polygons, so there may be instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed[SettlementsNamed.duplicated('Sett_ID', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed.drop_duplicates(subset=['Sett_ID'], inplace=True, keep='first')\n",
    "SettlementsNamed.info() # Range of entries should be the same as original Settlements file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a16201",
   "metadata": {},
   "source": [
    "### 6.3 Reduce to single name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which source has a name geometrically closest to the settlement.\n",
    "# Since we switched NaN values to -1 earlier, we also resolved what happens in the event of a tie, \n",
    "# i.e. when more than one source is 0.0 meters from the settlement. It will take the value from the first column.\n",
    "SettlementsNamed['SettName'] = \"UNK\"\n",
    "SettlementsNamed['closest'] = SettlementsNamed[['distAF', 'distGN', 'distUC']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6551664",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single name column where non-named settlements are \"UNK\" but all others use one of the three name sources.\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distAF\", \n",
    "    'SettName'] = SettlementsNamed['Afpl_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distUC\", \n",
    "    'SettName'] = SettlementsNamed['UCDB_Name']\n",
    "\n",
    "SettlementsNamed.loc[\n",
    "    SettlementsNamed['closest'] == \"distGN\", \n",
    "    'SettName'] = SettlementsNamed['GeoName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c57cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SettlementsNamed[SettlementsNamed['SettName'] != 'UNK'].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c748e23",
   "metadata": {},
   "source": [
    "### 6.4 Make sure place name is unique by stripping smaller localities of duplicated names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dupes = SettlementsNamed[ \n",
    "    (SettlementsNamed['SettName'] != 'UNK') & \n",
    "    (SettlementsNamed.duplicated('SettName', keep=False)) ] # keep=False is necessary to retain *all* duplicates, not just first or last in each group.\n",
    "\n",
    "print(\"Number of named settlements: %s\" % SettlementsNamed['SettName'].str.contains('UNK').value_counts()[False])\n",
    "print(\"Number of named settlements where name is duplicated at least once: %s\" % len(Dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Largest = Dupes.loc[Dupes.groupby([\"SettName\"])[\"AREA2015\"].idxmax()]\n",
    "print(Largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to settlements which have a duplicated name and are not the largest of those with that name, then replace with UNK.\n",
    "SettlementsNamed.loc[(~SettlementsNamed.Sett_ID.isin(Largest.Sett_ID)) \n",
    "                     & (SettlementsNamed.Sett_ID.isin(Dupes.Sett_ID)), \n",
    "                     'SettName'] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bddc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second number should now be zero.\n",
    "\n",
    "print(\"Number of named settlements: %s\" % SettlementsNamed['SettName'].str.contains('UNK').value_counts()[False])\n",
    "print(\"Number of named settlements where name is duplicated at least once: %s\" % len(SettlementsNamed[ \n",
    "    (SettlementsNamed['SettName'] != 'UNK') & \n",
    "    (SettlementsNamed.duplicated('SettName', keep=False)) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dd231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(SettlementsNamed.info(), SettlementsNamed[SettlementsNamed['SettName'] != \"UNK\"].sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d202603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns and save to file.\n",
    "SettlementsNamed = SettlementsNamed[['Sett_ID', 'SettName']]\n",
    "SettlementsNamed.to_csv(r'Results/PlaceNames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SettlementsNamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c3d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d7b6d",
   "metadata": {},
   "source": [
    "## 7. CREATE FRAGMENTATION INDEX\n",
    "We are determining what percentage of a settlement's area lies outside of its administrative zone each year.\n",
    "The index is a range of 0 to 100, i.e. the percent of the settlement area which is fragmented.\n",
    "\n",
    "For each Sett_ID:\n",
    "((Area of Boundless settlement - Area of largest Bounded settlement feature) / Area of Boundless settlement) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7537f",
   "metadata": {},
   "source": [
    "### 7.1 Load boundless and bounded cumulative settlements and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bdf01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BoundlessAreas = pd.read_csv(os.path.join(Results, 'Area.csv'))\n",
    "print('Loaded Boundless dataset, whose settlements will be used as the index of the Fragmentation Index dataset. %s' \n",
    "      % time.ctime())\n",
    "print(BoundlessAreas.info())\n",
    "\n",
    "BoundedAreas = pd.read_csv(os.path.join(Results, 'Area_Bounded.csv'))\n",
    "print('Loaded Bounded dataset, which will factor into the fragmentation calculation. %s' % time.ctime())\n",
    "print(BoundedAreas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BoundlessAreas = BoundlessAreas.loc[:, ~BoundlessAreas.columns.str.contains('Unnamed')]\n",
    "BoundedAreas = BoundedAreas.loc[:, ~BoundedAreas.columns.str.contains('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261495a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LargestFragments = BoundedAreas.loc[BoundedAreas.groupby([\"Sett_ID\"])[\"AREA2015\"].idxmax()] \n",
    "print(LargestFragments.info())\n",
    "print(\"Filtered the Bounded dataset to only rows where latest year's area is largest for each Sett_ID. %s\" % time.ctime())\n",
    "LargestFragments.columns = LargestFragments.columns.str.replace('AREA', 'Largest')\n",
    "LargestFragments = LargestFragments.drop(columns=['year', 'ADM_ID'])\n",
    "print(\"Renamed columns to avoid duplication during merge, and dropped unnecessary columns. %s\" % time.ctime())\n",
    "FragIndices = BoundlessAreas.merge(LargestFragments, how='left', on='Sett_ID')\n",
    "print(FragIndices.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a828bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del BoundlessAreas, BoundedAreas, LargestFragments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d4c7b",
   "metadata": {},
   "source": [
    "### 7.2 Merge and run fragmentation calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bcaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in AllStudyYears:\n",
    "    YY = str(item) # 4-digit year\n",
    "    AreaYY = ''.join([\"AREA\", YY]) # The Boundless area variable name\n",
    "    LargestYY = ''.join(['Largest', YY]) # The Bounded largest area variable name\n",
    "    FragYY = ''.join([\"Frag\", YY]) # Name for the fragmentation index variable\n",
    "    print(\"Created names for Year %s's variables and temporary objects. %s\" % (item, time.ctime()))\n",
    "    \n",
    "    FragIndices[FragYY] = ((FragIndices[AreaYY] - FragIndices[LargestYY]) / FragIndices[AreaYY]) * 100\n",
    "    FragIndices[FragYY] = (FragIndices[FragYY].fillna(0).replace([np.inf, -np.inf], 0)).astype('int')\n",
    "    print(\"Calculated fragmentation index for year %s. %s\" % (item, time.ctime()))\n",
    "\n",
    "# Remove unnecessary columns.\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('Largest')]\n",
    "FragIndices = FragIndices.loc[:, ~FragIndices.columns.str.startswith('AREA')]\n",
    "\n",
    "print('Completed fragmentation index calculations for all years. %s' % time.ctime())\n",
    "print(FragIndices.info())\n",
    "print(FragIndices.sort_values(by='Frag2015', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FragIndices = FragIndices.drop(columns=['year', 'ADM_ID'])\n",
    "FragIndices.to_csv(os.path.join(Results, 'FragIndex.csv'))\n",
    "print('Saved to file. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del FragIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36f039",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db18c",
   "metadata": {},
   "source": [
    "## 8. PREPARE YEARLY DATASETS: POPULATION\n",
    "Can use this as a template for other annualized rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd172",
   "metadata": {},
   "source": [
    "### 8.1 Reproject and reclassify with settlement buffer mask.\n",
    "Reclassify so that we only need to work with cells within X distance of settlements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae7f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PopList = glob.glob(os.path.join(Source, 'Population', \"*.tif\"))\n",
    "Settlements = gpd.read_file(os.path.join(Results, 'SETTLEMENTS.gpkg'), layer='SETTLEMENTS')\n",
    "PopList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BatchZonal(RasterFileList= PopList, Zones = Settlements, \n",
    "           KeepFields = ['Sett_ID'], \n",
    "           RasterDirectory = os.path.join(Source, 'Population'), \n",
    "           OutPath = os.path.join(Results, 'Population.csv'), \n",
    "           Statistics=['sum'], \n",
    "           NoDataVal = -99999, \n",
    "           Prefix = 'POP',\n",
    "           Suffix = '', \n",
    "           DropStatName = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a37640",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49e4cf",
   "metadata": {},
   "source": [
    "## 9. PREPARE YEARLY DATASETS: NIGHTTIME LIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667dd97",
   "metadata": {},
   "source": [
    "### 9.1 Reclassify with settlement buffer mask.\n",
    "Reclassify so that we only need to work with cells within X distance of settlements. The two NTL sources have already been reprojected in a separate script, and cropped to Central & Western Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fc3d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_avg = glob.glob(os.path.join(Source, 'NTL', \"*D*avg.tif\"))\n",
    "V_avg = glob.glob(os.path.join(Source, 'NTL', \"*V*avg.tif\"))\n",
    "D_cfc = glob.glob(os.path.join(Source, 'NTL', \"*D*cfc.tif\"))\n",
    "V_cfc = glob.glob(os.path.join(Source, 'NTL', \"*V*cfc.tif\"))\n",
    "Settlements = gpd.read_file(os.path.join(Results, 'SETTLEMENTS.gpkg'), layer='SETTLEMENTS')\n",
    "\n",
    "print(D_avg, V_avg, D_cfc, V_cfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b57e5e",
   "metadata": {},
   "source": [
    "#### Nighttime lights from two different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3aa57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BatchZonal(RasterFileList= D_avg, Zones = Settlements, \n",
    "           KeepFields = ['Sett_ID'], \n",
    "           RasterDirectory = os.path.join(Source, 'NTL'), \n",
    "           OutPath = os.path.join(Results, 'NTL_DMSP.csv'), \n",
    "           Statistics=['sum', 'max', 'mean', 'median', 'std'], \n",
    "           NoDataVal = -99999, # For NTL, the actual NoData is a \"soft NaN\": 3.40282e+38. Just using pop's here.\n",
    "           Prefix = 'NTL_D',\n",
    "           Suffix = '', \n",
    "           DropStatName = False)\n",
    "\n",
    "BatchZonal(RasterFileList= V_avg, Zones = Settlements, \n",
    "           KeepFields = ['Sett_ID'], \n",
    "           RasterDirectory = os.path.join(Source, 'NTL'), \n",
    "           OutPath = os.path.join(Results, 'NTL_VIIRS.csv'), \n",
    "           Statistics=['sum', 'max', 'mean', 'median', 'std'], \n",
    "           NoDataVal = -99999,\n",
    "           Prefix = 'NTL_V',\n",
    "           Suffix = '', \n",
    "           DropStatName = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1a2b5",
   "metadata": {},
   "source": [
    "#### Cloud-free coverage (confidence metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchZonal(RasterFileList= D_cfc, Zones = Settlements, \n",
    "           KeepFields = ['Sett_ID'], \n",
    "           RasterDirectory = os.path.join(Source, 'NTL'), \n",
    "           OutPath = os.path.join(Results, 'NTL_DMSPcfc.csv'), \n",
    "           Statistics=['sum', 'count', 'mean', 'median', 'std'], \n",
    "           NoDataVal = -99999, \n",
    "           Prefix = 'NTL_D',\n",
    "           Suffix = '_cfc', \n",
    "           DropStatName = False)\n",
    "\n",
    "BatchZonal(RasterFileList= V_cfc, Zones = Settlements, \n",
    "           KeepFields = ['Sett_ID'], \n",
    "           RasterDirectory = os.path.join(Source, 'NTL'), \n",
    "           OutPath = os.path.join(Results, 'NTL_VIIRScfc.csv'), \n",
    "           Statistics=['sum', 'count', 'mean', 'median', 'std'], \n",
    "           NoDataVal = -99999,\n",
    "           Prefix = 'NTL_V',\n",
    "           Suffix = '_cfc', \n",
    "           DropStatName = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab1eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3dbfb",
   "metadata": {},
   "source": [
    "## 10. FLOOD EXPOSURE BY RETURN PERIOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb0569",
   "metadata": {},
   "source": [
    "### 10.1 Calculate Expected Annual Depth (EAD) using exceedance probabilities of every flood return period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ccc6d",
   "metadata": {},
   "source": [
    "##### Flood layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15030b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FloodFolder = os.path.join(Source, 'Flood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "InRasters = os.listdir(FloodFolder)\n",
    "InRasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86401134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Exceedances = []\n",
    "    \n",
    "for Raster in InRasters:\n",
    "    InPath = os.path.join(SourceFolder, Raster)\n",
    "    RP = re.sub('\\D', '', Raster)[1:] # Get the return period\n",
    "    NewFileName = Raster.replace('.tif', '_EXC.tif')\n",
    "    OutPath = os.path.join(FloodFolder, NewFileName)\n",
    "    \n",
    "    Calc = \"(1/\" + RP + \")*A\"\n",
    "\n",
    "    calcShell(A=InPath, OutFile=OutPath, Calculation = Calc)\n",
    "    Exceedances = Exceedances + [NewFileName]\n",
    "    \n",
    "print('Done with list. New flood set: %s' % Exceedances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdal_calc doesn't always take well to adding together a large number of files, so we'll do it in 2 batches.\n",
    "\n",
    "Calc = 'A+B+C+D+E'\n",
    "OutName = 'Batch1.tif'\n",
    "\n",
    "A = os.path.join(FloodFolder, Exceedances[0])\n",
    "B = os.path.join(FloodFolder, Exceedances[1])\n",
    "C = os.path.join(FloodFolder, Exceedances[2])\n",
    "D = os.path.join(FloodFolder, Exceedances[3])\n",
    "E = os.path.join(FloodFolder, Exceedances[4])\n",
    "\n",
    "calcShell(A=A, B=B, C=C, D=D, E=E,\n",
    "          OutFile = os.path.join(FloodFolder, OutName), \n",
    "          Calculation = Calc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5302d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calc = 'A+B+C+D+E+F'\n",
    "OutName = 'FU_ExpectedAnnualDepth.tif'\n",
    "\n",
    "A = os.path.join(FloodFolder, 'Batch1.tif')\n",
    "B = os.path.join(FloodFolder, Exceedances[5])\n",
    "C = os.path.join(FloodFolder, Exceedances[6])\n",
    "D = os.path.join(FloodFolder, Exceedances[7])\n",
    "E = os.path.join(FloodFolder, Exceedances[8])\n",
    "F = os.path.join(FloodFolder, Exceedances[9])\n",
    "\n",
    "calcShell(A=A, B=B, C=C, D=D, E=E, F=F,\n",
    "          OutFile = os.path.join(FloodFolder, OutName), \n",
    "          Calculation = Calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0692b6",
   "metadata": {},
   "source": [
    "### 10.2 Reclassify and resample flood data and buildup data in preparation for the impact calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2c398",
   "metadata": {},
   "source": [
    "##### Reclassify flood as a binary: flooded / not-flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "InPath = os.path.join(FloodFolder, OutName)\n",
    "OutPath = os.path.join(FloodFolder, 'FU_EAD_reclassed.tif')\n",
    "\n",
    "[xsize, ysize, geotransform, geoproj, Z] = readRaster(InPath)\n",
    "\n",
    "Z[Z<0.15] = 0 # Not-flooded category. This includes no data cells.\n",
    "Z[Z>=0.15] = 1 # Flooded category. This includes permanent water bodies.\n",
    "\n",
    "writeRaster(OutPath,geotransform,geoproj,Z)\n",
    "InPath = OutPath = None\n",
    "\n",
    "print('Finished reclassifying. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04af5e4",
   "metadata": {},
   "source": [
    "##### Buildup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFE = 'WSFE_equalarea.tif'\n",
    "WSFEPath = os.path.join(Workspace, 'Buildup', WSFE)\n",
    "OutPath = os.path.join(FloodFolder, WSFE.replace('equalarea.tif', 'simplified.tif'))\n",
    "\n",
    "[xsize, ysize, geotransform, geoproj, Z] = readRaster(WSFEPath)\n",
    "\n",
    "np.putmask(Z, Z>0, Z-1984) # All years now converted to at most 2 digits: 1-31. (All non-buildup = 0)\n",
    "\n",
    "writeRaster(OutPath,geotransform,geoproj,Z)\n",
    "\n",
    "print('\\nSimplified buildup file: %s' % OutPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374df178",
   "metadata": {},
   "source": [
    "##### Resample flood to match buildup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSFEPath = os.path.join(FloodFolder, 'WSFE_simplified.tif') \n",
    "\n",
    "RasterPath = os.path.join(FloodFolder, 'FU_EAD_reclassed.tif')\n",
    "OutPath = os.path.join(FloodFolder, 'FU_EAD_resampled.tif')\n",
    "resampleRaster(RasterPath, WSFEPath, OutPath)\n",
    "    \n",
    "print('Resampled to match WSFE. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad4e00",
   "metadata": {},
   "source": [
    "### 10.3 Mask out built areas that were not flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026a3ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WSFEPath = os.path.join(FloodFolder, 'WSFE_simplified.tif') \n",
    "    \n",
    "InPath = os.path.join(FloodFolder, 'FU_EAD_resampled.tif')\n",
    "OutPath = os.path.join(FloodFolder, 'FU_EAD_WSFEimpact.tif')\n",
    "\n",
    "calcShell(A=WSFEPath, B=InPath, OutFile=OutPath, Calculation=\"A*B\", OutType=\" --type=Byte\")\n",
    "    \n",
    "print('Done. Only built-up cells that have been flooded remain.. %s' % time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4250f",
   "metadata": {},
   "source": [
    "### 10.4 Join with Settlements via concatenation\n",
    "Using the serial method, combine settlement IDs with 1) WSFE year cells and 2) the flooded-only WSFE year cells under each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da10ec8",
   "metadata": {},
   "source": [
    "##### Rasterize the settlements we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSFEPath = os.path.join(FloodFolder, 'WSFE_simplified.tif') \n",
    "OutSett = os.path.join(Results, 'Settlements2015_rasterized.tif')\n",
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS_equalarea')[['Sett_ID', 'geometry']]\n",
    "\n",
    "len_WSFE = 2 # We already know that the WSFE years are reclassified to 1-31, i.e. a max of 2 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e175146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShapeToRaster(Shapefile=Settlements, ValueVar=\"Sett_ID\", MetaRasterPath=WSFEPath, OutFilePath=OutSett, NewDType = 'uint32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468957a0",
   "metadata": {},
   "source": [
    "##### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calc = \"(A*\" + str(10**len_WSFE) + \")+B\" \n",
    "\n",
    "FloodImpactPath = os.path.join(FloodFolder, 'FU_EAD_WSFEimpact.tif')\n",
    "FloodSerialPath = os.path.join(FloodFolder, 'FU_Settlements_serial.tif')\n",
    "\n",
    "#WSFEPath = os.path.join(FloodFolder, 'WSFE_simplified.tif')\n",
    "WSFESerialPath = os.path.join(FloodFolder, 'WSFE_Settlements_serial.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "calcShell(A=OutSett, B=FloodImpactPath, OutFile=FloodSerialPath, Calculation=Calc)\n",
    "calcShell(A=OutSett, B=WSFEPath, OutFile=WSFESerialPath, Calculation=Calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465cf86",
   "metadata": {},
   "source": [
    "### 10.5 Vector math to split raster strings into Settlement and WSFE year assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f7306",
   "metadata": {},
   "source": [
    "##### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981aa267",
   "metadata": {},
   "outputs": [],
   "source": [
    "FloodVec = 'FloodedBuildup.shp' # Was having write issues when putting both in the same gpkg, so we're settling for .shp.\n",
    "FloodVecPath = os.path.join(FloodFolder, FloodVec)\n",
    "BuildVec = 'AllBuildup.shp' \n",
    "BuildVecPath = os.path.join(FloodFolder, BuildVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3160f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RasterToShapefile(InRasterPath=FloodSerialPath, OutFilePath=FloodVecPath, \n",
    "                  OutName='', VariableName='gridcode')\n",
    "RasterToShapefile(InRasterPath=WSFESerialPath, OutFilePath=BuildVecPath, \n",
    "                  OutName='', VariableName='gridcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df522b41",
   "metadata": {},
   "source": [
    "##### Split string into separate fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132451f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sett_rio = rasterio.open(os.path.join(Results, 'Settlements2015_rasterized.tif')).read(1)\n",
    "len_Sett = len(str(Sett_rio.max()))\n",
    "Sett_rio = None\n",
    "\n",
    "Fill = len_Sett + 2 # Add the digits stored as len_WSFE # or just write +2 since we already know the length of reclassed WSFE.\n",
    "\n",
    "OutPackage = os.path.join(FloodFolder, 'FloodedSettlements.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472a100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load newly created vectorized datasets.\n",
    "for File in [FloodVec, BuildVec]:\n",
    "    InObject = gpd.read_file(os.path.join(FloodFolder, File)).to_crs(\"ESRI:102022\")\n",
    "    print(InObject.info(), '\\n\\n', InObject.sample(10), '\\n\\n', InObject.crs, '\\n\\n', InObject['gridcode'].max())\n",
    "    \n",
    "    InObject['gridstring'] = InObject['gridcode'].astype(str).str.zfill(Fill)\n",
    "\n",
    "    InObject['Sett_ID'] = InObject['gridstring'].str[:-2].astype(int) # Remove the last digits to get the Sett ID portion.\n",
    "    InObject['year'] = InObject['gridstring'].str[-2:].astype(int) # Keep only the last digits to get the year portion.\n",
    "    InObject['year'] = np.where(InObject['year'] > 0, InObject['year'] + 1984, InObject['year']) # Reclass back to year value.\n",
    "    \n",
    "    print('%s Serial split by year of buildup and Sett ID.\\n\\n' % time.ctime(), InObject.sample(10))\n",
    "    \n",
    "    # Remove features where year or settlement = 0.\n",
    "    print(\"%s Before: %s\\n\" % (File, InObject.shape))\n",
    "    InObject = InObject.loc[(InObject[\"year\"] >1984) & (InObject[\"year\"] < 2016) & (InObject[\"Sett_ID\"] != 0)] \n",
    "    print(\"%s After: %s\\n\" % (File, InObject.shape))\n",
    "\n",
    "    # Save intermediate file.\n",
    "    InObject.to_file(driver='GPKG', filename=OutPackage, layer=File.replace('.shp', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758a744",
   "metadata": {},
   "source": [
    "### 10.6 Group by settlement and count cells for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d39571",
   "metadata": {},
   "source": [
    "##### Flooded buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61edaeb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS_equalarea')\n",
    "AllSummaries = pd.DataFrame(Settlements).drop(columns='geometry')[['Sett_ID']]\n",
    "Settlements = None\n",
    "\n",
    "ValObject = pd.DataFrame(gpd.read_file(OutPackage, layer='FloodedBuildup'))[['Sett_ID', 'year']]\n",
    "\n",
    "print(AllSummaries.info(), '\\n', AllSummaries.sample(10), '\\n', ValObject.info(), '\\n', ValObject.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62294e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for BuiltYear in AllStudyYears:\n",
    "    GroupedVals = ValObject[\n",
    "        ValObject['year']<=BuiltYear].groupby(\n",
    "        'Sett_ID', as_index=False)\n",
    "    \n",
    "    VariableName = ''.join(['FLDct_', str(BuiltYear)])\n",
    "    \n",
    "    AllSummaries = AllSummaries.merge(GroupedVals.count().rename(columns={'year': VariableName}), how = 'left', on='Sett_ID')\n",
    "\n",
    "    print('\\nDesired aggregation methods applied to settlement level, year %s. %s \\n' % (BuiltYear, time.ctime()))\n",
    "\n",
    "    # Save in-progress results\n",
    "    AllSummaries.to_csv(os.path.join(FloodFolder, 'FloodedCellCount1999to2015.csv'))\n",
    "    print(AllSummaries.sort_values(by=AllSummaries.columns[1], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8379ac1",
   "metadata": {},
   "source": [
    "##### All buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settlements = gpd.read_file(r'Results/SETTLEMENTS.gpkg', layer='SETTLEMENTS_equalarea')\n",
    "AllSummaries = pd.DataFrame(Settlements).drop(columns='geometry')[['Sett_ID']]\n",
    "Settlements = None\n",
    "AllSummaries\n",
    "\n",
    "ValObject = pd.DataFrame(gpd.read_file(OutPackage, layer='AllBuildup'))[['Sett_ID', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for BuiltYear in AllStudyYears:\n",
    "    GroupedVals = ValObject[\n",
    "        ValObject['year']<=BuiltYear].groupby(\n",
    "        'Sett_ID', as_index=False)\n",
    "    \n",
    "    VariableName = ''.join(['BLDct_', str(BuiltYear)])\n",
    "    \n",
    "    AllSummaries = AllSummaries.merge(GroupedVals.count().rename(columns={'year': VariableName}), how = 'left', on='Sett_ID')\n",
    "\n",
    "    print('\\nDesired aggregation methods applied to settlement level, year %s. %s \\n' % (BuiltYear, time.ctime()))\n",
    "\n",
    "    # Save in-progress results\n",
    "    AllSummaries.to_csv(os.path.join(FloodFolder, 'BuiltCellCount1999to2015.csv'))\n",
    "    print(AllSummaries.sort_values(by=AllSummaries.columns[1], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f81df",
   "metadata": {},
   "source": [
    "### 10.7 Calculate area and percent flooded and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf63d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BuiltArea = pd.read_csv(os.path.join(FloodFolder, 'BuiltCellCount1999to2015.csv'))\n",
    "Flood = pd.read_csv(os.path.join(FloodFolder, 'FloodedCellCount1999to2015.csv'))\n",
    "Areas = pd.read_csv(os.path.join(Results, 'Areas1999to2015.csv'))\n",
    "\n",
    "for Dataset in [BuiltArea, Flood, Areas]:\n",
    "    if 'Unnamed: 0' in Dataset.columns:\n",
    "        Dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "    print(Dataset.info())\n",
    "\n",
    "Stats = reduce(lambda  left,right: pd.merge(left,right,on=['Sett_ID'],\n",
    "                                            how='outer'), [BuiltArea, Flood, Areas])\n",
    "print(Stats.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick spot-checking. Number of flood cells should always be less than or equal to number of built area cells.\n",
    "Check1 = (Stats['FLDct_2007'] > Stats['BLDct_2007']).sum()\n",
    "Check2 = (Stats['FLDct_2000'] > Stats['BLDct_2000']).sum()\n",
    "Check3 = (Stats['FLDct_2011'] > Stats['BLDct_2011']).sum()\n",
    "print(Check1, Check2, Check3) # All should be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bea3a9",
   "metadata": {},
   "source": [
    "##### Percent flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e622c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['FLDct_', str(year)])\n",
    "    DenomVar = ''.join(['BLDct_', str(year)])\n",
    "    NewVar = ''.join(['FLDpc', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (DenomVar in Stats.columns)):\n",
    "        Stats[NewVar] = Stats[RawVar] / Stats[DenomVar]\n",
    "    else:\n",
    "        pass\n",
    "Stats.sort_values(by='FLDpc2005', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6311fea",
   "metadata": {},
   "source": [
    "##### Area flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['FLDpc', str(year)])\n",
    "    DenomVar = ''.join(['AREA', str(year)])\n",
    "    NewVar = ''.join(['FLDarea', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (DenomVar in Stats.columns)):\n",
    "        Stats[NewVar] = Stats[RawVar] * Stats[DenomVar]\n",
    "    else:\n",
    "        pass\n",
    "Stats.sort_values(by='FLDarea2005', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|FLDpc|FLDarea')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "Stats.to_csv(os.path.join(Results, 'Flood1999to2015.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429f6cd",
   "metadata": {},
   "source": [
    "## 11. GROWTH STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179e1f8",
   "metadata": {},
   "source": [
    "### 11.1 Load and prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45388",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStudyYears = ListFromRange(1999, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cedfe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PlaceNames = pd.read_csv(os.path.join(Results, 'PlaceNames.csv'))\n",
    "Areas = pd.read_csv(os.path.join(Results, 'Area.csv'))\n",
    "Population = pd.read_csv(os.path.join(Results, 'Population.csv'))\n",
    "DMSP = pd.read_csv(os.path.join(Results, 'NTL_DMSP.csv'))\n",
    "VNL = pd.read_csv(os.path.join(Results, 'NTL_VIIRS.csv'))\n",
    "#Flood = pd.read_csv(os.path.join(Results, 'Flood.csv'))\n",
    "\n",
    "RawValues = [PlaceNames, Areas, Population, DMSP, VNL]\n",
    "\n",
    "for Dataset in RawValues:\n",
    "    if 'Unnamed: 0' in Dataset.columns:\n",
    "        Dataset.drop(columns='Unnamed: 0', inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "    if 'year' in Dataset.columns:\n",
    "        Dataset.drop(columns='year', inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "    print(Dataset.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07909650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AllStats = reduce(lambda  left,right: pd.merge(left,right,on=['Sett_ID'],\n",
    "                                            how='outer'), RawValues)\n",
    "AllStats.to_csv(os.path.join(Results, 'AllStats.csv'))\n",
    "\n",
    "AllStats[AllStats.SettName!='UNK'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224573a1",
   "metadata": {},
   "source": [
    "### 11.2 Change over time of raw variables\n",
    "pch = percent change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39152aff",
   "metadata": {},
   "source": [
    "#### Population change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(Population, how = 'outer', on='Sett_ID')\n",
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['POP', str(year)])\n",
    "    LagVar = ''.join(['POP', str(year-1)])\n",
    "    NewVar = ''.join(['POPpch', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "        Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|pch')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'PopChange.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87709ba2",
   "metadata": {},
   "source": [
    "#### Area change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb53d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(Areas, how = 'outer', on='Sett_ID')\n",
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['AREA', str(year)])\n",
    "    LagVar = ''.join(['AREA', str(year-1)])\n",
    "    NewVar = ''.join(['AREApch', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "        Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|pch')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'AreaChange.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb6568",
   "metadata": {},
   "source": [
    "#### NTL change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff131334",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(NTL, how = 'outer', on='Sett_ID')\n",
    "Sensors = ['D_', 'V_']\n",
    "Methods = ['sum', 'avg', 'max']\n",
    "\n",
    "for year in AllStudyYears:\n",
    "    for Sensor in Sensors:\n",
    "        for agg in Methods:\n",
    "            RawVar = ''.join(['NTL', agg, Sensor, str(year)])\n",
    "            LagVar = ''.join(['NTL', agg, Sensor, str(year-1)])\n",
    "            NewVar = ''.join(['NTL', agg, '_pch', Sensor, str(year)])\n",
    "            if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "                Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|pch')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'NTLChange.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd27f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafa4cd",
   "metadata": {},
   "source": [
    "#### Flood change: change in area and change in percent of total area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbea939",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(Flood, how = 'outer', on='Sett_ID')\n",
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['FLDarea', str(year)])\n",
    "    LagVar = ''.join(['FLDarea', str(year-1)])\n",
    "    NewVar = ''.join(['FLDareapch', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "        Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ed0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['FLDpc', str(year)])\n",
    "    LagVar = ''.join(['FLDpc', str(year-1)])\n",
    "    NewVar = ''.join(['FLDpcpch', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "        Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|FLDareapch|FLDpcpch')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'FloodChange.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165c1e6",
   "metadata": {},
   "source": [
    "#### Update parent spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStats.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStats.to_csv(os.path.join(Results, 'AllStats.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d074141",
   "metadata": {},
   "source": [
    "### 11.3 Densities\n",
    "POPden = people per square kilometer\n",
    "<br>NTL...den = nighttime light luminosity per square kilometer\n",
    "<br>NTL...pop = nighttime light luminosity per capita"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08da5d",
   "metadata": {},
   "source": [
    "#### Population Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(Population, how = 'outer', on='Sett_ID')\n",
    "Stats = Stats.merge(Areas, how='left', on='Sett_ID')\n",
    "\n",
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['POPsum', str(year)])\n",
    "    DenomVar = ''.join(['AREA', str(year)])\n",
    "    NewVar = ''.join(['POPden', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (DenomVar in Stats.columns)):\n",
    "        Stats[NewVar] = Stats[RawVar] / Stats[DenomVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20848ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in density\n",
    "for year in AllStudyYears:\n",
    "    RawVar = ''.join(['POPden', str(year)])\n",
    "    LagVar = ''.join(['POPden', str(year-1)])\n",
    "    NewVar = ''.join(['POPdenpch', str(year)])\n",
    "    if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "        Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, ~Stats.columns.str.contains('AREA|sum|ct|year|ADM')]\n",
    "Stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b74d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'PopDensity.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9e3a3",
   "metadata": {},
   "source": [
    "#### Nighttime Lights Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef877b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(NTL, how = 'outer', on='Sett_ID')\n",
    "Stats = Stats.merge(Areas, how='left', on='Sett_ID')\n",
    "Sensors = ['D_', 'V_']\n",
    "Methods = ['sum', 'avg', 'max']\n",
    "\n",
    "for year in AllStudyYears:\n",
    "    for Sensor in Sensors:\n",
    "        for agg in Methods:\n",
    "            RawVar = ''.join(['NTL', agg, Sensor, str(year)])\n",
    "            DenomVar = ''.join(['AREA', str(year)])\n",
    "            NewVar = ''.join(['NTL', agg, '_den', Sensor, str(year)])\n",
    "            if ((RawVar in Stats.columns) and (DenomVar in Stats.columns)):\n",
    "                Stats[NewVar] = Stats[RawVar] / Stats[DenomVar]\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d6807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change in density\n",
    "for year in AllStudyYears:\n",
    "    for Sensor in Sensors:\n",
    "        for agg in Methods:\n",
    "            RawVar = ''.join(['NTL', agg, '_den', Sensor, str(year)])\n",
    "            LagVar = ''.join(['NTL', agg, '_den', Sensor, str(year-1)])\n",
    "            NewVar = ''.join(['NTL', agg, '_denpch', Sensor, str(year)])\n",
    "            if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "                Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|den')]\n",
    "list(Stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'NTLDensity.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af7bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e01e37",
   "metadata": {},
   "source": [
    "#### Nighttime Lights per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74429c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = PlaceNames.copy().merge(NTL, how = 'outer', on='Sett_ID')\n",
    "Stats = Stats.merge(Population, how='left', on='Sett_ID')\n",
    "Sensors = ['D_', 'V_']\n",
    "Methods = ['sum', 'avg', 'max']\n",
    "\n",
    "for year in AllStudyYears:\n",
    "    for Sensor in Sensors:\n",
    "        for agg in Methods:\n",
    "            RawVar = ''.join(['NTL', agg, Sensor, str(year)])\n",
    "            DenomVar = ''.join(['POPsum', str(year)])\n",
    "            NewVar = ''.join(['NTL', agg, '_pop', Sensor, str(year)])\n",
    "            if ((RawVar in Stats.columns) and (DenomVar in Stats.columns)):\n",
    "                Stats[NewVar] = Stats[RawVar] / Stats[DenomVar]\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c950974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in density\n",
    "for year in AllStudyYears:\n",
    "    for Sensor in Sensors:\n",
    "        for agg in Methods:\n",
    "            RawVar = ''.join(['NTL', agg, '_pop', Sensor, str(year)])\n",
    "            LagVar = ''.join(['NTL', agg, '_pop', Sensor, str(year-1)])\n",
    "            NewVar = ''.join(['NTL', agg, '_poppch', Sensor, str(year)])\n",
    "            if ((RawVar in Stats.columns) and (LagVar in Stats.columns)):\n",
    "                Stats[NewVar] = (Stats[RawVar] - Stats[LagVar]) / Stats[LagVar]\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc4c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop original variables.\n",
    "Stats = Stats.loc[:, Stats.columns.str.contains('Sett|_pop')]\n",
    "list(Stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'NTLperCapita.csv'))\n",
    "Stats.drop(columns='SettName', inplace=True)\n",
    "AllStats = AllStats.merge(Stats, how='left', on='Sett_ID')\n",
    "AllStats[AllStats.SettName!='UNK'].sort_values(by=AllStats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3e9c7",
   "metadata": {},
   "source": [
    "#### Update parent spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStats.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c78bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStats.to_csv(os.path.join(Results, 'AllStats.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe10b2",
   "metadata": {},
   "source": [
    "### 11.4 Urban Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in AllStudyYears:\n",
    "    PopVar = ''.join(['POPsum', str(year)])\n",
    "    DenVar = ''.join(['POPden', str(year)])\n",
    "    NewVar = ''.join(['UrbType', str(year)])\n",
    "    if ((PopVar in AllStats.columns) and (DenVar in AllStats.columns)):\n",
    "        AllStats[NewVar] = 'LD'\n",
    "        AllStats.loc[(AllStats[PopVar] >= 5000) & (AllStats[DenVar] >= 300), NewVar] = 'SDurban'\n",
    "        AllStats.loc[(AllStats[PopVar] >= 50000) & (AllStats[DenVar] >= 1500), NewVar] = 'HDurban'\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c694c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStats.to_csv(os.path.join(Results, 'AllStats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ef1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = AllStats.loc[:, AllStats.columns.str.contains('Sett|UrbType')]\n",
    "list(Stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c48bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats[Stats.SettName!='UNK'].sort_values(by=Stats.columns[5], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats.to_csv(os.path.join(Results, 'UrbanType.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
